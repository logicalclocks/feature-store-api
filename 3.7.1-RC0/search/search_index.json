{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Feature Store # HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository. Usage # Usage data is collected for improving quality of the library. It is turned on by default if the backend is \"c.app.hopsworks.ai\". To turn it off, use one of the following way: # use environment variable import os os . environ [ \"ENABLE_HOPSWORKS_USAGE\" ] = \"false\" # use `disable_usage_logging` import hsfs hsfs . disable_usage_logging () The source code can be found in python/hsfs/usage.py. Documentation # Documentation is available at Hopsworks Feature Store Documentation . Issues # For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Hopsworks Feature Store"},{"location":"#hopsworks-feature-store","text":"HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Feature Store"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository.","title":"Getting Started On Hopsworks"},{"location":"#usage","text":"Usage data is collected for improving quality of the library. It is turned on by default if the backend is \"c.app.hopsworks.ai\". To turn it off, use one of the following way: # use environment variable import os os . environ [ \"ENABLE_HOPSWORKS_USAGE\" ] = \"false\" # use `disable_usage_logging` import hsfs hsfs . disable_usage_logging () The source code can be found in python/hsfs/usage.py.","title":"Usage"},{"location":"#documentation","text":"Documentation is available at Hopsworks Feature Store Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[python,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ python,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Option 1: Build only current version of docs # Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve Option 2 (Preferred): Build multi-version doc with mike # Versioning on docs.hopsworks.ai # On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 . Build Instructions # For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ] Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[python,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ python,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","text":"Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve","title":"Option 1: Build only current version of docs"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","text":"","title":"Option 2 (Preferred): Build multi-version doc with mike"},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","text":"On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 .","title":"Versioning on docs.hopsworks.ai"},{"location":"CONTRIBUTING/#build-instructions","text":"For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ]","title":"Build Instructions"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"generated/api/connection_api/","text":"Connection # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # Hostname of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store # or import hopsworks project = hopsworks . login () fs = project . get_feature_store () Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides. Arguments host Optional[str] : The hostname of the Hopsworks instance in the form of [UUID].cloud.hopsworks.ai , defaults to None . Do not use the url including https:// when connecting programatically. port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] cert_folder # [source] host # [source] hostname_verification # [source] port # [source] project # [source] region_name # [source] secrets_store # [source] trust_store_path # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. Example import hsfs conn = hsfs . connection () conn . close () [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. How to get feature store instance import hsfs conn = hsfs . connection () fs = conn . get_feature_store () # or import hopsworks project = hopsworks . login () fs = project . get_feature_store () Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"Connection"},{"location":"generated/api/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/api/connection_api/#connection_1","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # Hostname of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store # or import hopsworks project = hopsworks . login () fs = project . get_feature_store () Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides. Arguments host Optional[str] : The hostname of the Hopsworks instance in the form of [UUID].cloud.hopsworks.ai , defaults to None . Do not use the url including https:// when connecting programatically. port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/api/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/api/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/api/connection_api/#cert_folder","text":"[source]","title":"cert_folder"},{"location":"generated/api/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/api/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/api/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/api/connection_api/#project","text":"[source]","title":"project"},{"location":"generated/api/connection_api/#region_name","text":"[source]","title":"region_name"},{"location":"generated/api/connection_api/#secrets_store","text":"[source]","title":"secrets_store"},{"location":"generated/api/connection_api/#trust_store_path","text":"","title":"trust_store_path"},{"location":"generated/api/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. Example import hsfs conn = hsfs . connection () conn . close () [source]","title":"close"},{"location":"generated/api/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/api/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source]","title":"connection"},{"location":"generated/api/connection_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. How to get feature store instance import hsfs conn = hsfs . connection () fs = conn . get_feature_store () # or import hopsworks project = hopsworks . login () fs = project . get_feature_store () Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/api/embedding_index_api/","text":"EmbeddingIndex # [source] EmbeddingIndex # hsfs . embedding . EmbeddingIndex ( index_name = None , features = None , col_prefix = None ) Represents an index for managing embeddings with associated features. Arguments index_name Optional[str] : The name of the embedding index. features Optional[List[hsfs.embedding.EmbeddingFeature]] : A list of EmbeddingFeature objects for the features that contain embeddings that should be indexed for similarity search. col_prefix Optional[str] : The prefix to be added to column names. Example embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=256) embeddings = embedding_index.get_embeddings() Methods # [source] add_embedding # EmbeddingIndex . add_embedding ( name , dimension , similarity_function_type = \"l2_norm\" ) Adds a new embedding feature to the index. Example: embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=256) Arguments name str : The name of the embedding feature. dimension int : The dimensionality of the embedding feature. similarity_function_type Optional[hsfs.embedding.SimilarityFunctionType] : The type of similarity function to be used. [source] from_json_response # EmbeddingIndex . from_json_response ( json_dict ) [source] get_embeddings # EmbeddingIndex . get_embeddings () Returns the list of hsfs.embedding.EmbeddingFeature objects associated with the index. Returns A list of hsfs.embedding.EmbeddingFeature objects [source] json # EmbeddingIndex . json () [source] to_dict # EmbeddingIndex . to_dict () {{embedding_feature}}","title":"EmbeddingIndex"},{"location":"generated/api/embedding_index_api/#embeddingindex","text":"[source]","title":"EmbeddingIndex"},{"location":"generated/api/embedding_index_api/#embeddingindex_1","text":"hsfs . embedding . EmbeddingIndex ( index_name = None , features = None , col_prefix = None ) Represents an index for managing embeddings with associated features. Arguments index_name Optional[str] : The name of the embedding index. features Optional[List[hsfs.embedding.EmbeddingFeature]] : A list of EmbeddingFeature objects for the features that contain embeddings that should be indexed for similarity search. col_prefix Optional[str] : The prefix to be added to column names. Example embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=256) embeddings = embedding_index.get_embeddings()","title":"EmbeddingIndex"},{"location":"generated/api/embedding_index_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/embedding_index_api/#add_embedding","text":"EmbeddingIndex . add_embedding ( name , dimension , similarity_function_type = \"l2_norm\" ) Adds a new embedding feature to the index. Example: embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=256) Arguments name str : The name of the embedding feature. dimension int : The dimensionality of the embedding feature. similarity_function_type Optional[hsfs.embedding.SimilarityFunctionType] : The type of similarity function to be used. [source]","title":"add_embedding"},{"location":"generated/api/embedding_index_api/#from_json_response","text":"EmbeddingIndex . from_json_response ( json_dict ) [source]","title":"from_json_response"},{"location":"generated/api/embedding_index_api/#get_embeddings","text":"EmbeddingIndex . get_embeddings () Returns the list of hsfs.embedding.EmbeddingFeature objects associated with the index. Returns A list of hsfs.embedding.EmbeddingFeature objects [source]","title":"get_embeddings"},{"location":"generated/api/embedding_index_api/#json","text":"EmbeddingIndex . json () [source]","title":"json"},{"location":"generated/api/embedding_index_api/#to_dict","text":"EmbeddingIndex . to_dict () {{embedding_feature}}","title":"to_dict"},{"location":"generated/api/expectation_api/","text":"Expectation # {{expectation}} Properties # {{expectation_properties}} Methods # {{expectation_methods}} Creation # {{expectation_create}} Retrieval # {{expectation_getall}} {{expectation_get}}","title":"Expectation"},{"location":"generated/api/expectation_api/#expectation","text":"{{expectation}}","title":"Expectation"},{"location":"generated/api/expectation_api/#properties","text":"{{expectation_properties}}","title":"Properties"},{"location":"generated/api/expectation_api/#methods","text":"{{expectation_methods}}","title":"Methods"},{"location":"generated/api/expectation_api/#creation","text":"{{expectation_create}}","title":"Creation"},{"location":"generated/api/expectation_api/#retrieval","text":"{{expectation_getall}} {{expectation_get}}","title":"Retrieval"},{"location":"generated/api/expectation_suite_api/","text":"Expectation Suite # [source] ExpectationSuite # hsfs . expectation_suite . ExpectationSuite ( expectation_suite_name , expectations , meta , id = None , data_asset_type = None , ge_cloud_id = None , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , feature_store_id = None , feature_group_id = None , href = None , expand = None , items = None , count = None , type = None , created = None , ** kwargs ) Metadata object representing an feature validation expectation in the Feature Store. Creation with Great Expectations # import great_expectations as ge expectation_suite = ge . core . ExpectationSuite ( \"new_expectation_suite\" , expectations = [ ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_max_to_be_between\" , kwargs = { \"column\" : \"feature\" , \"min_value\" : - 1 , \"max_value\" : 1 } ) ] ) Attach to Feature Group # [source] save_expectation_suite # FeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , overwrite = False ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . save_expectation_suite ( expectation_suite , run_validation = True ) Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the Feature Group. overwrite bool : If an Expectation Suite is already attached, overwrite it. The new suite will have its own validation history, but former reports are preserved. run_validation bool : Set whether the expectation_suite will run on ingestion validation_ingestion_policy str : Set the policy for ingestion to the Feature Group. \"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group. \"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result. Raises hsfs.client.exceptions.RestAPIError . Single Expectation API # An API to edit the expectation list based on Great Expectations API. [source] add_expectation # ExpectationSuite . add_expectation ( expectation , ge_type = True ) Append an expectation to the local suite or in the backend if attached to a Feature Group. Example # check if the minimum value of specific column is within a range of 0 and 1 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_min_to_be_between\" , kwargs = { \"column\" : \"foo_id\" , \"min_value\" : 0 , \"max_value\" : 1 } ) ) # check if the length of specific column value is within a range of 3 and 10 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_value_lengths_to_be_between\" , kwargs = { \"column\" : \"bar_name\" , \"min_value\" : 3 , \"max_value\" : 10 } ) ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The new expectation object. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The new expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source] replace_expectation # ExpectationSuite . replace_expectation ( expectation , ge_type = True ) Update an expectation from the suite locally or from the backend if attached to a Feature Group. Example updated_expectation = expectation_suite . replace_expectation ( new_expectation_object ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The updated expectation object. The meta field should contain an expectationId field. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The updated expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source] remove_expectation # ExpectationSuite . remove_expectation ( expectation_id = None ) Remove an expectation from the suite locally and from the backend if attached to a Feature Group. Example expectation_suite . remove_expectation ( expectation_id = 123 ) Arguments expectation_id Optional[int] : Id of the expectation to remove. The expectation will be deleted both locally and from the backend. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException Properties # [source] data_asset_type # Data asset type of the expectation suite, not used by backend. [source] expectation_suite_name # Name of the expectation suite. [source] expectations # List of expectations to run at validation. [source] ge_cloud_id # ge_cloud_id of the expectation suite, not used by backend. [source] ge_cloud_id # ge_cloud_id of the expectation suite, not used by backend. [source] id # Id of the expectation suite, set by backend. [source] meta # Meta field of the expectation suite to store additional informations. [source] run_validation # Boolean to determine whether or not the expectation suite shoudl run on ingestion. [source] validation_ingestion_policy # Whether to ingest a df based on the validation result. \"STRICT\" : ingest df only if all expectations succeed, \"ALWAYS\" : always ingest df, even if one or more expectations fail Methods # [source] add_expectation # ExpectationSuite . add_expectation ( expectation , ge_type = True ) Append an expectation to the local suite or in the backend if attached to a Feature Group. Example # check if the minimum value of specific column is within a range of 0 and 1 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_min_to_be_between\" , kwargs = { \"column\" : \"foo_id\" , \"min_value\" : 0 , \"max_value\" : 1 } ) ) # check if the length of specific column value is within a range of 3 and 10 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_value_lengths_to_be_between\" , kwargs = { \"column\" : \"bar_name\" , \"min_value\" : 3 , \"max_value\" : 10 } ) ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The new expectation object. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The new expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source] from_ge_type # ExpectationSuite . from_ge_type ( ge_expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , id = None , feature_store_id = None , feature_group_id = None , ) Used to create a Hopsworks Expectation Suite instance from a great_expectations instance. Arguments ge_expectation_suite great_expectations.core.expectation_suite.ExpectationSuite : great_expectations.ExpectationSuite The great_expectations ExpectationSuite instance to convert to a Hopsworks ExpectationSuite. run_validation bool : bool Whether to run validation on inserts when the expectation suite is attached. validation_ingestion_policy str : str The validation ingestion policy to use when the expectation suite is attached. Defaults to \"ALWAYS\". Options are \"STRICT\" or \"ALWAYS\". id Optional[int] : int The id of the expectation suite in Hopsworks. If not provided, a new expectation suite will be created. feature_store_id Optional[int] : int The id of the feature store of the feature group to which the expectation suite belongs. feature_group_id Optional[int] : int The id of the feature group to which the expectation suite belongs. Returns Hopsworks Expectation Suite instance. [source] from_response_json # ExpectationSuite . from_response_json ( json_dict ) [source] get_expectation # ExpectationSuite . get_expectation ( expectation_id , ge_type = True ) Fetch expectation with expectation_id from the backend. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) expectation_suite = fg . get_expectation_suite () selected_expectation = expectation_suite . get_expectation ( expectation_id = 123 ) Arguments expectation_id int : Id of the expectation to fetch from the backend. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The expectation with expectation_id registered in the backend. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source] json # ExpectationSuite . json () [source] remove_expectation # ExpectationSuite . remove_expectation ( expectation_id = None ) Remove an expectation from the suite locally and from the backend if attached to a Feature Group. Example expectation_suite . remove_expectation ( expectation_id = 123 ) Arguments expectation_id Optional[int] : Id of the expectation to remove. The expectation will be deleted both locally and from the backend. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source] replace_expectation # ExpectationSuite . replace_expectation ( expectation , ge_type = True ) Update an expectation from the suite locally or from the backend if attached to a Feature Group. Example updated_expectation = expectation_suite . replace_expectation ( new_expectation_object ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The updated expectation object. The meta field should contain an expectationId field. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The updated expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source] to_dict # ExpectationSuite . to_dict () [source] to_ge_type # ExpectationSuite . to_ge_type () [source] to_json_dict # ExpectationSuite . to_json_dict ( decamelize = False )","title":"ExpectationSuite"},{"location":"generated/api/expectation_suite_api/#expectation-suite","text":"[source]","title":"Expectation Suite"},{"location":"generated/api/expectation_suite_api/#expectationsuite","text":"hsfs . expectation_suite . ExpectationSuite ( expectation_suite_name , expectations , meta , id = None , data_asset_type = None , ge_cloud_id = None , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , feature_store_id = None , feature_group_id = None , href = None , expand = None , items = None , count = None , type = None , created = None , ** kwargs ) Metadata object representing an feature validation expectation in the Feature Store.","title":"ExpectationSuite"},{"location":"generated/api/expectation_suite_api/#creation-with-great-expectations","text":"import great_expectations as ge expectation_suite = ge . core . ExpectationSuite ( \"new_expectation_suite\" , expectations = [ ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_max_to_be_between\" , kwargs = { \"column\" : \"feature\" , \"min_value\" : - 1 , \"max_value\" : 1 } ) ] )","title":"Creation with Great Expectations"},{"location":"generated/api/expectation_suite_api/#attach-to-feature-group","text":"[source]","title":"Attach to Feature Group"},{"location":"generated/api/expectation_suite_api/#save_expectation_suite","text":"FeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , overwrite = False ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . save_expectation_suite ( expectation_suite , run_validation = True ) Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the Feature Group. overwrite bool : If an Expectation Suite is already attached, overwrite it. The new suite will have its own validation history, but former reports are preserved. run_validation bool : Set whether the expectation_suite will run on ingestion validation_ingestion_policy str : Set the policy for ingestion to the Feature Group. \"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group. \"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result. Raises hsfs.client.exceptions.RestAPIError .","title":"save_expectation_suite"},{"location":"generated/api/expectation_suite_api/#single-expectation-api","text":"An API to edit the expectation list based on Great Expectations API. [source]","title":"Single Expectation API"},{"location":"generated/api/expectation_suite_api/#add_expectation","text":"ExpectationSuite . add_expectation ( expectation , ge_type = True ) Append an expectation to the local suite or in the backend if attached to a Feature Group. Example # check if the minimum value of specific column is within a range of 0 and 1 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_min_to_be_between\" , kwargs = { \"column\" : \"foo_id\" , \"min_value\" : 0 , \"max_value\" : 1 } ) ) # check if the length of specific column value is within a range of 3 and 10 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_value_lengths_to_be_between\" , kwargs = { \"column\" : \"bar_name\" , \"min_value\" : 3 , \"max_value\" : 10 } ) ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The new expectation object. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The new expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source]","title":"add_expectation"},{"location":"generated/api/expectation_suite_api/#replace_expectation","text":"ExpectationSuite . replace_expectation ( expectation , ge_type = True ) Update an expectation from the suite locally or from the backend if attached to a Feature Group. Example updated_expectation = expectation_suite . replace_expectation ( new_expectation_object ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The updated expectation object. The meta field should contain an expectationId field. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The updated expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source]","title":"replace_expectation"},{"location":"generated/api/expectation_suite_api/#remove_expectation","text":"ExpectationSuite . remove_expectation ( expectation_id = None ) Remove an expectation from the suite locally and from the backend if attached to a Feature Group. Example expectation_suite . remove_expectation ( expectation_id = 123 ) Arguments expectation_id Optional[int] : Id of the expectation to remove. The expectation will be deleted both locally and from the backend. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException","title":"remove_expectation"},{"location":"generated/api/expectation_suite_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/expectation_suite_api/#data_asset_type","text":"Data asset type of the expectation suite, not used by backend. [source]","title":"data_asset_type"},{"location":"generated/api/expectation_suite_api/#expectation_suite_name","text":"Name of the expectation suite. [source]","title":"expectation_suite_name"},{"location":"generated/api/expectation_suite_api/#expectations","text":"List of expectations to run at validation. [source]","title":"expectations"},{"location":"generated/api/expectation_suite_api/#ge_cloud_id","text":"ge_cloud_id of the expectation suite, not used by backend. [source]","title":"ge_cloud_id"},{"location":"generated/api/expectation_suite_api/#ge_cloud_id_1","text":"ge_cloud_id of the expectation suite, not used by backend. [source]","title":"ge_cloud_id"},{"location":"generated/api/expectation_suite_api/#id","text":"Id of the expectation suite, set by backend. [source]","title":"id"},{"location":"generated/api/expectation_suite_api/#meta","text":"Meta field of the expectation suite to store additional informations. [source]","title":"meta"},{"location":"generated/api/expectation_suite_api/#run_validation","text":"Boolean to determine whether or not the expectation suite shoudl run on ingestion. [source]","title":"run_validation"},{"location":"generated/api/expectation_suite_api/#validation_ingestion_policy","text":"Whether to ingest a df based on the validation result. \"STRICT\" : ingest df only if all expectations succeed, \"ALWAYS\" : always ingest df, even if one or more expectations fail","title":"validation_ingestion_policy"},{"location":"generated/api/expectation_suite_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/expectation_suite_api/#add_expectation_1","text":"ExpectationSuite . add_expectation ( expectation , ge_type = True ) Append an expectation to the local suite or in the backend if attached to a Feature Group. Example # check if the minimum value of specific column is within a range of 0 and 1 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_min_to_be_between\" , kwargs = { \"column\" : \"foo_id\" , \"min_value\" : 0 , \"max_value\" : 1 } ) ) # check if the length of specific column value is within a range of 3 and 10 expectation_suite . add_expectation ( ge . core . ExpectationConfiguration ( expectation_type = \"expect_column_value_lengths_to_be_between\" , kwargs = { \"column\" : \"bar_name\" , \"min_value\" : 3 , \"max_value\" : 10 } ) ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The new expectation object. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The new expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source]","title":"add_expectation"},{"location":"generated/api/expectation_suite_api/#from_ge_type","text":"ExpectationSuite . from_ge_type ( ge_expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , id = None , feature_store_id = None , feature_group_id = None , ) Used to create a Hopsworks Expectation Suite instance from a great_expectations instance. Arguments ge_expectation_suite great_expectations.core.expectation_suite.ExpectationSuite : great_expectations.ExpectationSuite The great_expectations ExpectationSuite instance to convert to a Hopsworks ExpectationSuite. run_validation bool : bool Whether to run validation on inserts when the expectation suite is attached. validation_ingestion_policy str : str The validation ingestion policy to use when the expectation suite is attached. Defaults to \"ALWAYS\". Options are \"STRICT\" or \"ALWAYS\". id Optional[int] : int The id of the expectation suite in Hopsworks. If not provided, a new expectation suite will be created. feature_store_id Optional[int] : int The id of the feature store of the feature group to which the expectation suite belongs. feature_group_id Optional[int] : int The id of the feature group to which the expectation suite belongs. Returns Hopsworks Expectation Suite instance. [source]","title":"from_ge_type"},{"location":"generated/api/expectation_suite_api/#from_response_json","text":"ExpectationSuite . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/expectation_suite_api/#get_expectation","text":"ExpectationSuite . get_expectation ( expectation_id , ge_type = True ) Fetch expectation with expectation_id from the backend. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) expectation_suite = fg . get_expectation_suite () selected_expectation = expectation_suite . get_expectation ( expectation_id = 123 ) Arguments expectation_id int : Id of the expectation to fetch from the backend. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The expectation with expectation_id registered in the backend. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source]","title":"get_expectation"},{"location":"generated/api/expectation_suite_api/#json","text":"ExpectationSuite . json () [source]","title":"json"},{"location":"generated/api/expectation_suite_api/#remove_expectation_1","text":"ExpectationSuite . remove_expectation ( expectation_id = None ) Remove an expectation from the suite locally and from the backend if attached to a Feature Group. Example expectation_suite . remove_expectation ( expectation_id = 123 ) Arguments expectation_id Optional[int] : Id of the expectation to remove. The expectation will be deleted both locally and from the backend. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source]","title":"remove_expectation"},{"location":"generated/api/expectation_suite_api/#replace_expectation_1","text":"ExpectationSuite . replace_expectation ( expectation , ge_type = True ) Update an expectation from the suite locally or from the backend if attached to a Feature Group. Example updated_expectation = expectation_suite . replace_expectation ( new_expectation_object ) Arguments expectation Union[hsfs.ge_expectation.GeExpectation, great_expectations.core.expectation_configuration.ExpectationConfiguration] : The updated expectation object. The meta field should contain an expectationId field. ge_type bool : Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True. Returns The updated expectation attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException [source]","title":"replace_expectation"},{"location":"generated/api/expectation_suite_api/#to_dict","text":"ExpectationSuite . to_dict () [source]","title":"to_dict"},{"location":"generated/api/expectation_suite_api/#to_ge_type","text":"ExpectationSuite . to_ge_type () [source]","title":"to_ge_type"},{"location":"generated/api/expectation_suite_api/#to_json_dict","text":"ExpectationSuite . to_json_dict ( decamelize = False )","title":"to_json_dict"},{"location":"generated/api/external_feature_group_api/","text":"ExternalFeatureGroup # [source] ExternalFeatureGroup # hsfs . feature_group . ExternalFeatureGroup ( storage_connector , query = None , data_format = None , path = None , options = {}, name = None , version = None , description = None , primary_key = None , featurestore_id = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , href = None , online_topic_name = None , topic_name = None , notification_topic_name = None , spine = False , deprecated = False , ** kwargs ) Creation # [source] create_external_feature_group # FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , topic_name = None , notification_topic_name = None , ) Create a external feature group metadata object. Example # connect to the Feature Store fs = ... external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually: external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , online_enabled = True ) external_fg . save () # read from external storage and filter data to sync to online df = external_fg . read () . filter ( external_fg . customer_status == \"active\" ) # insert to online storage external_fg . insert ( df ) Arguments name str : Name of the external feature group to create. storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group options Optional[Dict[str, str]] : Additional options to be used by the engine when reading data from the specified storage connector. For example, {\"header\": True} when reading CSV files with column names in the first row. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . expectation_suite : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . online_enabled Optional[bool] : Define whether it should be possible to sync the feature group to the online feature store for low latency access, defaults to False . topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns ExternalFeatureGroup . The external feature group metadata object. Retrieval # [source] get_external_feature_group # FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... external_fg = fs . get_external_feature_group ( \"external_fg_test\" ) Arguments name str : Name of the external feature group to get. version int : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # [source] creator # [source] data_format # [source] deprecated # Setting if the feature group is deprecated. [source] description # [source] embedding_index # [source] event_time # Event time feature in the feature group. [source] expectation_suite # Expectation Suite configuration object defining the settings for data validation of the feature group. [source] feature_store # [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Feature Group schema (alias) [source] id # [source] location # [source] name # Name of the feature group. [source] notification_topic_name # The topic used for feature group notifications. [source] online_enabled # Setting if the feature group is available in online storage. [source] options # [source] path # [source] primary_key # List of features building the primary key. [source] query # [source] schema # Feature Group schema [source] statistics # Get the latest computed statistics for the whole feature group. Raises hsfs.client.exceptions.FeatureStoreException . [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. Raises hsfs.client.exceptions.FeatureStoreException . [source] storage_connector # [source] subject # Subject of the feature group. [source] topic_name # The topic used for feature group data ingestion. [source] version # Version number of the feature group. Methods # [source] add_tag # ExternalFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . add_tag ( name = \"example_tag\" , value = \"42\" ) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source] append_features # ExternalFeatureGroup . append_features ( features ) Append features to the schema of the feature group. Example # connect to the Feature Store fs = ... # define features to be inserted in the feature group features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . append_features ( features ) Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] check_deprecated # ExternalFeatureGroup . check_deprecated () [source] compute_statistics # ExternalFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) statistics_metadata = fg . compute_statistics () Returns Statistics . The statistics metadata object. Raises hsfs.client.exceptions.RestAPIError . Unable to persist the statistics. hsfs.client.exceptions.FeatureStoreException . [source] create_feature_monitoring # ExternalFeatureGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_statistics_monitoring # ExternalFeatureGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] delete # ExternalFeatureGroup . delete () Drop the entire feature group along with its feature data. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , version = 1 ) # delete the feature group fg . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises hsfs.client.exceptions.RestAPIError . [source] delete_expectation_suite # ExternalFeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_expectation_suite () Raises hsfs.client.exceptions.RestAPIError . [source] delete_tag # ExternalFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_tag ( \"example_tag\" ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source] filter # ExternalFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: Example fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: Example fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # ExternalFeatureGroup . from_response_json ( json_dict ) [source] get_all_statistics # ExternalFeatureGroup . get_all_statistics ( computation_time = None , feature_names = None ) Returns all the statistics metadata computed before a specific time for the current feature group. If computation_time is None , all the statistics metadata are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source] get_all_validation_reports # ExternalFeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) val_reports = fg . get_all_validation_reports () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns Union[List[ ValidationReport ], ValidationReport ]. All validation reports attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source] get_complex_features # ExternalFeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. Example complex_dtype_features = fg . get_complex_features () [source] get_expectation_suite # ExternalFeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) exp_suite = fg . get_expectation_suite () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . [source] get_feature # ExternalFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get Feature instanse fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Arguments: name: The name of the feature to retrieve Returns: Feature: The feature object Raises hsfs.client.exceptions.FeatureStoreException . [source] get_feature_monitoring_configs # ExternalFeatureGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source] get_feature_monitoring_history # ExternalFeatureGroup . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fg . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # fetch feature monitoring history for a given feature monitoring config id fm_history = fg . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The start date of the feature monitoring history to fetch. Defaults to None. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source] get_fg_name # ExternalFeatureGroup . get_fg_name () [source] get_generated_feature_groups # ExternalFeatureGroup . get_generated_feature_groups () Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_generated_feature_views # ExternalFeatureGroup . get_generated_feature_views () Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_latest_validation_report # ExternalFeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the Feature Group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) latest_val_report = fg . get_latest_validation_report () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError . [source] get_parent_feature_groups # ExternalFeatureGroup . get_parent_feature_groups () Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_statistics # ExternalFeatureGroup . get_statistics ( computation_time = None , feature_names = None ) Returns the statistics computed at a specific time for the current feature group. If computation_time is None , the most recent statistics are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source] get_tag # ExternalFeatureGroup . get_tag ( name ) Get the tags of a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_tag_value = fg . get_tag ( \"example_tag\" ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # ExternalFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source] get_validation_history # ExternalFeatureGroup . get_validation_history ( expectation_id , start_validation_time = None , end_validation_time = None , filter_by = [], ge_type = True ) Fetch validation history of an Expectation specified by its id. Example validation_history = fg . get_validation_history ( expectation_id = 1 , filter_by = [ \"REJECTED\" , \"UNKNOWN\" ], start_validation_time = \"2022-01-01 00:00:00\" , end_validation_time = datetime . datetime . now (), ge_type = False ) Arguments expectation_id int : id of the Expectation for which to fetch the validation history filter_by List[str] : list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\". start_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. end_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. Raises hsfs.client.exceptions.RestAPIError . Return Union[List[ ValidationResult ], List[ ExpectationValidationResult ]] A list of validation result connected to the expectation_id [source] insert # ExternalFeatureGroup . insert ( features , write_options = {}, validation_options = {}, save_code = True , wait = False ) Insert the dataframe feature values ONLY in the online feature store. External Feature Groups contains metadata about feature data in an external storage system. External storage system are usually offline, meaning feature values cannot be retrieved in real-time. In order to use the feature values for real-time use-cases, you can insert them in Hopsoworks Online Feature Store via this method. The Online Feature Store has a single-entry per primary key value, meaining that providing a new value with for a given primary key will overwrite the existing value. No record of the previous value is kept. Example # connect to the Feature Store fs = ... # get the External Feature Group instance fg = fs . get_feature_group ( name = \"external_sales_records\" , version = 1 ) # get the feature values, e.g reading from csv files in a S3 bucket feature_values = ... # insert the feature values in the online feature store fg . insert ( feature_values ) Note Data Validation via Great Expectation is supported if you have attached an expectation suite to your External Feature Group. However, as opposed to regular Feature Groups, this can lead to discrepancies between the data in the external storage system and the online feature store. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default True , to control whether the expectation suite of the feature group should be fetched before every insert. save_code Optional[bool] : When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create the feature group or used to insert data to it. When calling the insert method repeatedly with small batches of data, this can slow down the writes. Use this option to turn off saving code. Defaults to True . Returns Tuple( Job , ValidationReport ) The validation report if validation is enabled. Raises hsfs.client.exceptions.RestAPIError . e.g fail to create feature group, dataframe schema does not match existing feature group schema, etc. hsfs.client.exceptions.DataValidationException . If data validation fails and the expectation suite validation_ingestion_policy is set to STRICT . Data is NOT ingested. [source] json # ExternalFeatureGroup . json () [source] read # ExternalFeatureGroup . read ( dataframe_type = \"default\" , online = False ) Get the feature group as a DataFrame. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) df = fg . read () Engine Support Spark only Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises hsfs.client.exceptions.RestAPIError . [source] save # ExternalFeatureGroup . save () Persist the metadata for this external feature group. Without calling this method, your feature group will only exist in your Python Kernel, but not in Hopsworks. query = \"SELECT * FROM sales\" fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) fg . save () ---- < span style = \"float:right;\" > [[ source ]]( https : // github . com / logicalclocks / feature - store - api / blob / master / python / hsfs / feature_group . py #L843)</span> ### save_expectation_suite ``` python ExternalFeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , overwrite = False ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . save_expectation_suite ( expectation_suite , run_validation = True ) Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the Feature Group. overwrite bool : If an Expectation Suite is already attached, overwrite it. The new suite will have its own validation history, but former reports are preserved. run_validation bool : Set whether the expectation_suite will run on ingestion validation_ingestion_policy str : Set the policy for ingestion to the Feature Group. \"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group. \"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result. Raises hsfs.client.exceptions.RestAPIError . [source] save_validation_report # ExternalFeatureGroup . save_validation_report ( validation_report , ingestion_result = \"UNKNOWN\" , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... , expectation_suite = expectation_suite ) validation_report = great_expectations . from_pandas ( my_experimental_features_df , fg . get_expectation_suite ()) . validate () fg . save_validation_report ( validation_report , ingestion_result = \"EXPERIMENT\" ) Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the Feature Group. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises hsfs.client.exceptions.RestAPIError . [source] select # ExternalFeatureGroup . select ( features ) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select ([ \"id\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features List[Union[str, hsfs.feature.Feature]] : A list of Feature objects or feature names as strings to be selected. Returns Query : A query object with the selected features of the feature group. [source] select_all # ExternalFeatureGroup . select_all ( include_primary_key = True , include_event_time = True ) Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view. Example # connect to the Feature Store fs = ... # get the Feature Group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # show first 5 rows query . show ( 5 ) # select all features exclude primary key and event time from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) query = fg . select_all () query . features # [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)] query = fg . select_all ( include_primary_key = False , include_event_time = False ) query . features # [Feature('f1', ...), Feature('f2', ...)] Arguments include_primary_key Optional[bool] : If True, include primary key of the feature group to the feature list. Defaults to True. include_event_time Optional[bool] : If True, include event time of the feature group to the feature list. Defaults to True. Returns Query . A query object with all features of the feature group. [source] select_except # ExternalFeatureGroup . select_except ( features = []) Select all features including primary key and event time feature of the feature group except provided features and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select_except ([ \"ts\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features Optional[List[Union[str, hsfs.feature.Feature]]] : A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # ExternalFeatureGroup . show ( n ) Show the first n rows of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . show ( 5 ) [source] to_dict # ExternalFeatureGroup . to_dict () [source] update_deprecated # ExternalFeatureGroup . update_deprecated ( deprecate = True ) Deprecate the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_deprecated ( deprecate = True ) Safe update This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged. Arguments deprecate bool : Boolean value identifying if the feature group should be deprecated. Defaults to True. Returns FeatureGroup . The updated feature group object. [source] update_description # ExternalFeatureGroup . update_description ( description ) Update the description of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_description ( description = \"Much better description.\" ) Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # ExternalFeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_feature_description ( feature_name = \"min_temp\" , description = \"Much better feature description.\" ) Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # ExternalFeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # ExternalFeatureGroup . update_from_response_json ( json_dict ) [source] update_notification_topic_name # ExternalFeatureGroup . update_notification_topic_name ( notification_topic_name ) Update the notification topic name of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_notification_topic_name ( notification_topic_name = \"notification_topic_name\" ) Safe update This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name. Arguments notification_topic_name str : Name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If set to None no notifications are sent. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # ExternalFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_statistics_config () Returns FeatureGroup . The updated metadata object of the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source] validate # ExternalFeatureGroup . validate ( dataframe = None , expectation_suite = None , save_report = False , validation_options = {}, ingestion_result = \"UNKNOWN\" , ge_type = True , ) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Example # connect to the Feature Store fs = ... # get feature group instance fg = fs . get_or_create_feature_group ( ... ) ge_report = fg . validate ( df , save_report = False ) Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The dataframe to run the data validation expectations against. expectation_suite Optional[hsfs.expectation_suite.ExpectationSuite] : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. save_report Optional[bool] : Whether to save the report to the backend. This is only possible if the Expectation suite is initialised and attached to the Feature Group. Defaults to False. ge_type bool : Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True. Returns A Validation Report produced by Great Expectations.","title":"ExternalFeatureGroup"},{"location":"generated/api/external_feature_group_api/#externalfeaturegroup","text":"[source]","title":"ExternalFeatureGroup"},{"location":"generated/api/external_feature_group_api/#externalfeaturegroup_1","text":"hsfs . feature_group . ExternalFeatureGroup ( storage_connector , query = None , data_format = None , path = None , options = {}, name = None , version = None , description = None , primary_key = None , featurestore_id = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , href = None , online_topic_name = None , topic_name = None , notification_topic_name = None , spine = False , deprecated = False , ** kwargs )","title":"ExternalFeatureGroup"},{"location":"generated/api/external_feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/external_feature_group_api/#create_external_feature_group","text":"FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , topic_name = None , notification_topic_name = None , ) Create a external feature group metadata object. Example # connect to the Feature Store fs = ... external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually: external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , online_enabled = True ) external_fg . save () # read from external storage and filter data to sync to online df = external_fg . read () . filter ( external_fg . customer_status == \"active\" ) # insert to online storage external_fg . insert ( df ) Arguments name str : Name of the external feature group to create. storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group options Optional[Dict[str, str]] : Additional options to be used by the engine when reading data from the specified storage connector. For example, {\"header\": True} when reading CSV files with column names in the first row. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . expectation_suite : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . online_enabled Optional[bool] : Define whether it should be possible to sync the feature group to the online feature store for low latency access, defaults to False . topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns ExternalFeatureGroup . The external feature group metadata object.","title":"create_external_feature_group"},{"location":"generated/api/external_feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/external_feature_group_api/#get_external_feature_group","text":"FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... external_fg = fs . get_external_feature_group ( \"external_fg_test\" ) Arguments name str : Name of the external feature group to get. version int : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_external_feature_group"},{"location":"generated/api/external_feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/external_feature_group_api/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/api/external_feature_group_api/#created","text":"[source]","title":"created"},{"location":"generated/api/external_feature_group_api/#creator","text":"[source]","title":"creator"},{"location":"generated/api/external_feature_group_api/#data_format","text":"[source]","title":"data_format"},{"location":"generated/api/external_feature_group_api/#deprecated","text":"Setting if the feature group is deprecated. [source]","title":"deprecated"},{"location":"generated/api/external_feature_group_api/#description","text":"[source]","title":"description"},{"location":"generated/api/external_feature_group_api/#embedding_index","text":"[source]","title":"embedding_index"},{"location":"generated/api/external_feature_group_api/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/api/external_feature_group_api/#expectation_suite","text":"Expectation Suite configuration object defining the settings for data validation of the feature group. [source]","title":"expectation_suite"},{"location":"generated/api/external_feature_group_api/#feature_store","text":"[source]","title":"feature_store"},{"location":"generated/api/external_feature_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/external_feature_group_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/external_feature_group_api/#features","text":"Feature Group schema (alias) [source]","title":"features"},{"location":"generated/api/external_feature_group_api/#id","text":"[source]","title":"id"},{"location":"generated/api/external_feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/external_feature_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/external_feature_group_api/#notification_topic_name","text":"The topic used for feature group notifications. [source]","title":"notification_topic_name"},{"location":"generated/api/external_feature_group_api/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/api/external_feature_group_api/#options","text":"[source]","title":"options"},{"location":"generated/api/external_feature_group_api/#path","text":"[source]","title":"path"},{"location":"generated/api/external_feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/external_feature_group_api/#query","text":"[source]","title":"query"},{"location":"generated/api/external_feature_group_api/#schema","text":"Feature Group schema [source]","title":"schema"},{"location":"generated/api/external_feature_group_api/#statistics","text":"Get the latest computed statistics for the whole feature group. Raises hsfs.client.exceptions.FeatureStoreException . [source]","title":"statistics"},{"location":"generated/api/external_feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. Raises hsfs.client.exceptions.FeatureStoreException . [source]","title":"statistics_config"},{"location":"generated/api/external_feature_group_api/#storage_connector","text":"[source]","title":"storage_connector"},{"location":"generated/api/external_feature_group_api/#subject","text":"Subject of the feature group. [source]","title":"subject"},{"location":"generated/api/external_feature_group_api/#topic_name","text":"The topic used for feature group data ingestion. [source]","title":"topic_name"},{"location":"generated/api/external_feature_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/external_feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/external_feature_group_api/#add_tag","text":"ExternalFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . add_tag ( name = \"example_tag\" , value = \"42\" ) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/external_feature_group_api/#append_features","text":"ExternalFeatureGroup . append_features ( features ) Append features to the schema of the feature group. Example # connect to the Feature Store fs = ... # define features to be inserted in the feature group features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . append_features ( features ) Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/external_feature_group_api/#check_deprecated","text":"ExternalFeatureGroup . check_deprecated () [source]","title":"check_deprecated"},{"location":"generated/api/external_feature_group_api/#compute_statistics","text":"ExternalFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) statistics_metadata = fg . compute_statistics () Returns Statistics . The statistics metadata object. Raises hsfs.client.exceptions.RestAPIError . Unable to persist the statistics. hsfs.client.exceptions.FeatureStoreException . [source]","title":"compute_statistics"},{"location":"generated/api/external_feature_group_api/#create_feature_monitoring","text":"ExternalFeatureGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_feature_monitoring"},{"location":"generated/api/external_feature_group_api/#create_statistics_monitoring","text":"ExternalFeatureGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_statistics_monitoring"},{"location":"generated/api/external_feature_group_api/#delete","text":"ExternalFeatureGroup . delete () Drop the entire feature group along with its feature data. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , version = 1 ) # delete the feature group fg . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete"},{"location":"generated/api/external_feature_group_api/#delete_expectation_suite","text":"ExternalFeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_expectation_suite () Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete_expectation_suite"},{"location":"generated/api/external_feature_group_api/#delete_tag","text":"ExternalFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_tag ( \"example_tag\" ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/external_feature_group_api/#filter","text":"ExternalFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: Example fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: Example fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/external_feature_group_api/#from_response_json","text":"ExternalFeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/external_feature_group_api/#get_all_statistics","text":"ExternalFeatureGroup . get_all_statistics ( computation_time = None , feature_names = None ) Returns all the statistics metadata computed before a specific time for the current feature group. If computation_time is None , all the statistics metadata are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_all_statistics"},{"location":"generated/api/external_feature_group_api/#get_all_validation_reports","text":"ExternalFeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) val_reports = fg . get_all_validation_reports () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns Union[List[ ValidationReport ], ValidationReport ]. All validation reports attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_all_validation_reports"},{"location":"generated/api/external_feature_group_api/#get_complex_features","text":"ExternalFeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. Example complex_dtype_features = fg . get_complex_features () [source]","title":"get_complex_features"},{"location":"generated/api/external_feature_group_api/#get_expectation_suite","text":"ExternalFeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) exp_suite = fg . get_expectation_suite () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_expectation_suite"},{"location":"generated/api/external_feature_group_api/#get_feature","text":"ExternalFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get Feature instanse fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Arguments: name: The name of the feature to retrieve Returns: Feature: The feature object Raises hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_feature"},{"location":"generated/api/external_feature_group_api/#get_feature_monitoring_configs","text":"ExternalFeatureGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source]","title":"get_feature_monitoring_configs"},{"location":"generated/api/external_feature_group_api/#get_feature_monitoring_history","text":"ExternalFeatureGroup . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fg . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # fetch feature monitoring history for a given feature monitoring config id fm_history = fg . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The start date of the feature monitoring history to fetch. Defaults to None. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source]","title":"get_feature_monitoring_history"},{"location":"generated/api/external_feature_group_api/#get_fg_name","text":"ExternalFeatureGroup . get_fg_name () [source]","title":"get_fg_name"},{"location":"generated/api/external_feature_group_api/#get_generated_feature_groups","text":"ExternalFeatureGroup . get_generated_feature_groups () Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_generated_feature_groups"},{"location":"generated/api/external_feature_group_api/#get_generated_feature_views","text":"ExternalFeatureGroup . get_generated_feature_views () Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_generated_feature_views"},{"location":"generated/api/external_feature_group_api/#get_latest_validation_report","text":"ExternalFeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the Feature Group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) latest_val_report = fg . get_latest_validation_report () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_latest_validation_report"},{"location":"generated/api/external_feature_group_api/#get_parent_feature_groups","text":"ExternalFeatureGroup . get_parent_feature_groups () Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_parent_feature_groups"},{"location":"generated/api/external_feature_group_api/#get_statistics","text":"ExternalFeatureGroup . get_statistics ( computation_time = None , feature_names = None ) Returns the statistics computed at a specific time for the current feature group. If computation_time is None , the most recent statistics are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_statistics"},{"location":"generated/api/external_feature_group_api/#get_tag","text":"ExternalFeatureGroup . get_tag ( name ) Get the tags of a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_tag_value = fg . get_tag ( \"example_tag\" ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/external_feature_group_api/#get_tags","text":"ExternalFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/external_feature_group_api/#get_validation_history","text":"ExternalFeatureGroup . get_validation_history ( expectation_id , start_validation_time = None , end_validation_time = None , filter_by = [], ge_type = True ) Fetch validation history of an Expectation specified by its id. Example validation_history = fg . get_validation_history ( expectation_id = 1 , filter_by = [ \"REJECTED\" , \"UNKNOWN\" ], start_validation_time = \"2022-01-01 00:00:00\" , end_validation_time = datetime . datetime . now (), ge_type = False ) Arguments expectation_id int : id of the Expectation for which to fetch the validation history filter_by List[str] : list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\". start_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. end_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. Raises hsfs.client.exceptions.RestAPIError . Return Union[List[ ValidationResult ], List[ ExpectationValidationResult ]] A list of validation result connected to the expectation_id [source]","title":"get_validation_history"},{"location":"generated/api/external_feature_group_api/#insert","text":"ExternalFeatureGroup . insert ( features , write_options = {}, validation_options = {}, save_code = True , wait = False ) Insert the dataframe feature values ONLY in the online feature store. External Feature Groups contains metadata about feature data in an external storage system. External storage system are usually offline, meaning feature values cannot be retrieved in real-time. In order to use the feature values for real-time use-cases, you can insert them in Hopsoworks Online Feature Store via this method. The Online Feature Store has a single-entry per primary key value, meaining that providing a new value with for a given primary key will overwrite the existing value. No record of the previous value is kept. Example # connect to the Feature Store fs = ... # get the External Feature Group instance fg = fs . get_feature_group ( name = \"external_sales_records\" , version = 1 ) # get the feature values, e.g reading from csv files in a S3 bucket feature_values = ... # insert the feature values in the online feature store fg . insert ( feature_values ) Note Data Validation via Great Expectation is supported if you have attached an expectation suite to your External Feature Group. However, as opposed to regular Feature Groups, this can lead to discrepancies between the data in the external storage system and the online feature store. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default True , to control whether the expectation suite of the feature group should be fetched before every insert. save_code Optional[bool] : When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create the feature group or used to insert data to it. When calling the insert method repeatedly with small batches of data, this can slow down the writes. Use this option to turn off saving code. Defaults to True . Returns Tuple( Job , ValidationReport ) The validation report if validation is enabled. Raises hsfs.client.exceptions.RestAPIError . e.g fail to create feature group, dataframe schema does not match existing feature group schema, etc. hsfs.client.exceptions.DataValidationException . If data validation fails and the expectation suite validation_ingestion_policy is set to STRICT . Data is NOT ingested. [source]","title":"insert"},{"location":"generated/api/external_feature_group_api/#json","text":"ExternalFeatureGroup . json () [source]","title":"json"},{"location":"generated/api/external_feature_group_api/#read","text":"ExternalFeatureGroup . read ( dataframe_type = \"default\" , online = False ) Get the feature group as a DataFrame. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) df = fg . read () Engine Support Spark only Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"read"},{"location":"generated/api/external_feature_group_api/#save","text":"ExternalFeatureGroup . save () Persist the metadata for this external feature group. Without calling this method, your feature group will only exist in your Python Kernel, but not in Hopsworks. query = \"SELECT * FROM sales\" fg = feature_store . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) fg . save () ---- < span style = \"float:right;\" > [[ source ]]( https : // github . com / logicalclocks / feature - store - api / blob / master / python / hsfs / feature_group . py #L843)</span> ### save_expectation_suite ``` python ExternalFeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , overwrite = False ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . save_expectation_suite ( expectation_suite , run_validation = True ) Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the Feature Group. overwrite bool : If an Expectation Suite is already attached, overwrite it. The new suite will have its own validation history, but former reports are preserved. run_validation bool : Set whether the expectation_suite will run on ingestion validation_ingestion_policy str : Set the policy for ingestion to the Feature Group. \"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group. \"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"save"},{"location":"generated/api/external_feature_group_api/#save_validation_report","text":"ExternalFeatureGroup . save_validation_report ( validation_report , ingestion_result = \"UNKNOWN\" , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... , expectation_suite = expectation_suite ) validation_report = great_expectations . from_pandas ( my_experimental_features_df , fg . get_expectation_suite ()) . validate () fg . save_validation_report ( validation_report , ingestion_result = \"EXPERIMENT\" ) Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the Feature Group. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises hsfs.client.exceptions.RestAPIError . [source]","title":"save_validation_report"},{"location":"generated/api/external_feature_group_api/#select","text":"ExternalFeatureGroup . select ( features ) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select ([ \"id\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features List[Union[str, hsfs.feature.Feature]] : A list of Feature objects or feature names as strings to be selected. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/external_feature_group_api/#select_all","text":"ExternalFeatureGroup . select_all ( include_primary_key = True , include_event_time = True ) Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view. Example # connect to the Feature Store fs = ... # get the Feature Group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # show first 5 rows query . show ( 5 ) # select all features exclude primary key and event time from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) query = fg . select_all () query . features # [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)] query = fg . select_all ( include_primary_key = False , include_event_time = False ) query . features # [Feature('f1', ...), Feature('f2', ...)] Arguments include_primary_key Optional[bool] : If True, include primary key of the feature group to the feature list. Defaults to True. include_event_time Optional[bool] : If True, include event time of the feature group to the feature list. Defaults to True. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/external_feature_group_api/#select_except","text":"ExternalFeatureGroup . select_except ( features = []) Select all features including primary key and event time feature of the feature group except provided features and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select_except ([ \"ts\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features Optional[List[Union[str, hsfs.feature.Feature]]] : A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/external_feature_group_api/#show","text":"ExternalFeatureGroup . show ( n ) Show the first n rows of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . show ( 5 ) [source]","title":"show"},{"location":"generated/api/external_feature_group_api/#to_dict","text":"ExternalFeatureGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/external_feature_group_api/#update_deprecated","text":"ExternalFeatureGroup . update_deprecated ( deprecate = True ) Deprecate the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_deprecated ( deprecate = True ) Safe update This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged. Arguments deprecate bool : Boolean value identifying if the feature group should be deprecated. Defaults to True. Returns FeatureGroup . The updated feature group object. [source]","title":"update_deprecated"},{"location":"generated/api/external_feature_group_api/#update_description","text":"ExternalFeatureGroup . update_description ( description ) Update the description of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_description ( description = \"Much better description.\" ) Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/external_feature_group_api/#update_feature_description","text":"ExternalFeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_feature_description ( feature_name = \"min_temp\" , description = \"Much better feature description.\" ) Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/api/external_feature_group_api/#update_features","text":"ExternalFeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/api/external_feature_group_api/#update_from_response_json","text":"ExternalFeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/external_feature_group_api/#update_notification_topic_name","text":"ExternalFeatureGroup . update_notification_topic_name ( notification_topic_name ) Update the notification topic name of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_notification_topic_name ( notification_topic_name = \"notification_topic_name\" ) Safe update This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name. Arguments notification_topic_name str : Name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If set to None no notifications are sent. Returns FeatureGroup . The updated feature group object. [source]","title":"update_notification_topic_name"},{"location":"generated/api/external_feature_group_api/#update_statistics_config","text":"ExternalFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_statistics_config () Returns FeatureGroup . The updated metadata object of the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source]","title":"update_statistics_config"},{"location":"generated/api/external_feature_group_api/#validate","text":"ExternalFeatureGroup . validate ( dataframe = None , expectation_suite = None , save_report = False , validation_options = {}, ingestion_result = \"UNKNOWN\" , ge_type = True , ) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Example # connect to the Feature Store fs = ... # get feature group instance fg = fs . get_or_create_feature_group ( ... ) ge_report = fg . validate ( df , save_report = False ) Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The dataframe to run the data validation expectations against. expectation_suite Optional[hsfs.expectation_suite.ExpectationSuite] : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. save_report Optional[bool] : Whether to save the report to the backend. This is only possible if the Expectation suite is initialised and attached to the Feature Group. Defaults to False. ge_type bool : Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True. Returns A Validation Report produced by Great Expectations.","title":"validate"},{"location":"generated/api/feature_api/","text":"Feature # [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ** kwargs ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] description # Description of the feature. [source] feature_group_id # [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] contains # Feature . contains ( other ) Deprecated contains method is deprecated. Use isin instead. [source] from_response_json # Feature . from_response_json ( json_dict ) [source] is_complex # Feature . is_complex () Returns true if the feature has a complex type. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) selected_feature = fg . get_feature ( \"min_temp\" ) selected_feature . is_complex () [source] isin # Feature . isin ( other ) [source] json # Feature . json () [source] like # Feature . like ( other ) [source] to_dict # Feature . to_dict () Get structured info about specific Feature in python dictionary format. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) selected_feature = fg . get_feature ( \"min_temp\" ) selected_feature . to_dict ()","title":"Feature"},{"location":"generated/api/feature_api/#feature","text":"[source]","title":"Feature"},{"location":"generated/api/feature_api/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ** kwargs ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/api/feature_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_api/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/api/feature_api/#description","text":"Description of the feature. [source]","title":"description"},{"location":"generated/api/feature_api/#feature_group_id","text":"[source]","title":"feature_group_id"},{"location":"generated/api/feature_api/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_api/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/api/feature_api/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/api/feature_api/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/api/feature_api/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/api/feature_api/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/api/feature_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_api/#contains","text":"Feature . contains ( other ) Deprecated contains method is deprecated. Use isin instead. [source]","title":"contains"},{"location":"generated/api/feature_api/#from_response_json","text":"Feature . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_api/#is_complex","text":"Feature . is_complex () Returns true if the feature has a complex type. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) selected_feature = fg . get_feature ( \"min_temp\" ) selected_feature . is_complex () [source]","title":"is_complex"},{"location":"generated/api/feature_api/#isin","text":"Feature . isin ( other ) [source]","title":"isin"},{"location":"generated/api/feature_api/#json","text":"Feature . json () [source]","title":"json"},{"location":"generated/api/feature_api/#like","text":"Feature . like ( other ) [source]","title":"like"},{"location":"generated/api/feature_api/#to_dict","text":"Feature . to_dict () Get structured info about specific Feature in python dictionary format. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) selected_feature = fg . get_feature ( \"min_temp\" ) selected_feature . to_dict ()","title":"to_dict"},{"location":"generated/api/feature_descriptive_statistics_api/","text":"Feature Descriptive Statistics # [source] FeatureDescriptiveStatistics # hsfs . core . feature_descriptive_statistics . FeatureDescriptiveStatistics ( feature_name , feature_type = None , count = None , completeness = None , num_non_null_values = None , num_null_values = None , approx_num_distinct_values = None , min = None , max = None , sum = None , mean = None , stddev = None , percentiles = None , distinctness = None , entropy = None , uniqueness = None , exact_num_distinct_values = None , extended_statistics = None , id = None , ** kwargs ) Properties # [source] approx_num_distinct_values # Approximate number of distinct values. [source] completeness # Fraction of non-null values in a column. [source] count # Number of values. [source] distinctness # Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once. Example [a, a, b] [a, a, b] contains two distinct values a a and b b , so distinctness is 2/3 2/3 . [source] entropy # Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values). Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count). Example [a, b, b, c, c] [a, b, b, c, c] has three distinct values with counts [1, 2, 2] [1, 2, 2] . Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055 (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055 . [source] exact_num_distinct_values # Exact number of distinct values. [source] extended_statistics # Additional statistics computed on the feature values such as histograms and correlations. [source] feature_name # Name of the feature. [source] feature_type # Data type of the feature. It can be one of Boolean, Fractional, Integral, or String. [source] id # [source] max # Maximum value. [source] mean # Mean value. [source] min # Minimum value. [source] num_non_null_values # Number of non-null values. [source] num_null_values # Number of null values. [source] percentiles # Percentiles. [source] stddev # Standard deviation of the feature values. [source] sum # Sum of all feature values. [source] uniqueness # Fraction of unique values over the number of all values of a column. Unique values occur exactly once. Example [a, a, b] [a, a, b] contains one unique value b b , so uniqueness is 1/3 1/3 .","title":"Feature descriptive statistics"},{"location":"generated/api/feature_descriptive_statistics_api/#feature-descriptive-statistics","text":"[source]","title":"Feature Descriptive Statistics"},{"location":"generated/api/feature_descriptive_statistics_api/#featuredescriptivestatistics","text":"hsfs . core . feature_descriptive_statistics . FeatureDescriptiveStatistics ( feature_name , feature_type = None , count = None , completeness = None , num_non_null_values = None , num_null_values = None , approx_num_distinct_values = None , min = None , max = None , sum = None , mean = None , stddev = None , percentiles = None , distinctness = None , entropy = None , uniqueness = None , exact_num_distinct_values = None , extended_statistics = None , id = None , ** kwargs )","title":"FeatureDescriptiveStatistics"},{"location":"generated/api/feature_descriptive_statistics_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_descriptive_statistics_api/#approx_num_distinct_values","text":"Approximate number of distinct values. [source]","title":"approx_num_distinct_values"},{"location":"generated/api/feature_descriptive_statistics_api/#completeness","text":"Fraction of non-null values in a column. [source]","title":"completeness"},{"location":"generated/api/feature_descriptive_statistics_api/#count","text":"Number of values. [source]","title":"count"},{"location":"generated/api/feature_descriptive_statistics_api/#distinctness","text":"Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once. Example [a, a, b] [a, a, b] contains two distinct values a a and b b , so distinctness is 2/3 2/3 . [source]","title":"distinctness"},{"location":"generated/api/feature_descriptive_statistics_api/#entropy","text":"Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values). Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count). Example [a, b, b, c, c] [a, b, b, c, c] has three distinct values with counts [1, 2, 2] [1, 2, 2] . Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055 (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055 . [source]","title":"entropy"},{"location":"generated/api/feature_descriptive_statistics_api/#exact_num_distinct_values","text":"Exact number of distinct values. [source]","title":"exact_num_distinct_values"},{"location":"generated/api/feature_descriptive_statistics_api/#extended_statistics","text":"Additional statistics computed on the feature values such as histograms and correlations. [source]","title":"extended_statistics"},{"location":"generated/api/feature_descriptive_statistics_api/#feature_name","text":"Name of the feature. [source]","title":"feature_name"},{"location":"generated/api/feature_descriptive_statistics_api/#feature_type","text":"Data type of the feature. It can be one of Boolean, Fractional, Integral, or String. [source]","title":"feature_type"},{"location":"generated/api/feature_descriptive_statistics_api/#id","text":"[source]","title":"id"},{"location":"generated/api/feature_descriptive_statistics_api/#max","text":"Maximum value. [source]","title":"max"},{"location":"generated/api/feature_descriptive_statistics_api/#mean","text":"Mean value. [source]","title":"mean"},{"location":"generated/api/feature_descriptive_statistics_api/#min","text":"Minimum value. [source]","title":"min"},{"location":"generated/api/feature_descriptive_statistics_api/#num_non_null_values","text":"Number of non-null values. [source]","title":"num_non_null_values"},{"location":"generated/api/feature_descriptive_statistics_api/#num_null_values","text":"Number of null values. [source]","title":"num_null_values"},{"location":"generated/api/feature_descriptive_statistics_api/#percentiles","text":"Percentiles. [source]","title":"percentiles"},{"location":"generated/api/feature_descriptive_statistics_api/#stddev","text":"Standard deviation of the feature values. [source]","title":"stddev"},{"location":"generated/api/feature_descriptive_statistics_api/#sum","text":"Sum of all feature values. [source]","title":"sum"},{"location":"generated/api/feature_descriptive_statistics_api/#uniqueness","text":"Fraction of unique values over the number of all values of a column. Unique values occur exactly once. Example [a, a, b] [a, a, b] contains one unique value b b , so uniqueness is 1/3 1/3 .","title":"uniqueness"},{"location":"generated/api/feature_group_api/","text":"FeatureGroup # [source] FeatureGroup # hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , embedding_index = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , online_topic_name = None , topic_name = None , notification_topic_name = None , event_time = None , stream = False , expectation_suite = None , parents = None , href = None , delta_streamer_job_conf = None , deprecated = False , ** kwargs ) Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , parents = [], topic_name = None , notification_topic_name = None , ) Create a feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . create_feature_group ( name = 'air_quality' , description = 'Air Quality characteristics of each day' , version = 1 , primary_key = [ 'city' , 'date' ], online_enabled = True , event_time = 'date' ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, vector database is used as online feature store. This enables similarity search by using find_neighbors . default to None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. [source] get_or_create_feature_group # FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , parents = [], topic_name = None , notification_topic_name = None , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = \"electricity_prices\" , version = 1 , description = \"Electricity prices from NORD POOL\" , primary_key = [ \"day\" , \"area\" ], online_enabled = True , event_time = \"timestamp\" , ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, the vector database is used as online feature store. This enables similarity search by using find_neighbors . default is None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... fg = fs . get_feature_group ( name = \"electricity_prices\" , version = 1 , ) Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] deprecated # Setting if the feature group is deprecated. [source] description # Description of the feature group contents. [source] embedding_index # [source] event_time # Event time feature in the feature group. [source] expectation_suite # Expectation Suite configuration object defining the settings for data validation of the feature group. [source] feature_store # [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Feature Group schema (alias) [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] materialization_job # Get the Job object reference for the materialization job for this Feature Group. [source] name # Name of the feature group. [source] notification_topic_name # The topic used for feature group notifications. [source] online_enabled # Setting if the feature group is available in online storage. [source] parents # Parent feature groups as origin of the data in the current feature group. This is part of explicit provenance [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] schema # Feature Group schema [source] statistics # Get the latest computed statistics for the whole feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. Raises hsfs.client.exceptions.FeatureStoreException . [source] stream # Whether to enable real time stream writing capabilities. [source] subject # Subject of the feature group. [source] time_travel_format # Setting of the feature group time travel format. [source] topic_name # The topic used for feature group data ingestion. [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . add_tag ( name = \"example_tag\" , value = \"42\" ) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Example # connect to the Feature Store fs = ... # define features to be inserted in the feature group features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . append_features ( features ) Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] as_of # FeatureGroup . as_of ( wallclock_time = None , exclude_until = None ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get data at a specific point in time and show it fg . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fg . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fg . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time: fg . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) fg1 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" ) . join ( fg2 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: Example fg1 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" ) # as_of is not applied . join ( fg2 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-15\" )) # as_of is not applied . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" ) Warning This function only works for feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Read data as of this point in time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . exclude_until Optional[Union[str, int, datetime.datetime, datetime.date]] : Exclude commits until this point in time. String should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . Returns Query . The query object with the applied time travel condition. [source] check_deprecated # FeatureGroup . check_deprecated () [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on feature groups stored as HUDI or DELTA. Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises hsfs.client.exceptions.RestAPIError . [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) commit_details = fg . commit_details () Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Commit details as of specific point in time. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . If the feature group does not have HUDI time travel format [source] compute_statistics # FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Returns Statistics . The statistics metadata object. Raises hsfs.client.exceptions.RestAPIError . Unable to persist the statistics. [source] create_feature_monitoring # FeatureGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_statistics_monitoring # FeatureGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , version = 1 ) # delete the feature group fg . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises hsfs.client.exceptions.RestAPIError . [source] delete_expectation_suite # FeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_expectation_suite () Raises hsfs.client.exceptions.RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_tag ( \"example_tag\" ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: Example fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: Example fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] finalize_multi_part_insert # FeatureGroup . finalize_multi_part_insert () Finalizes and exits the multi part insert context opened by multi_part_insert in a blocking fashion once all rows have been transmitted. Multi part insert with manual context management Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. feature_group = fs . get_or_create_feature_group ( \"fg_name\" , version = 1 ) while loop : small_batch_df = ... feature_group . multi_part_insert ( small_batch_df ) # IMPORTANT: finalize the multi part insert to make sure all rows # have been transmitted feature_group . finalize_multi_part_insert () Note that the first call to multi_part_insert initiates the context and be sure to finalize it. The finalize_multi_part_insert is a blocking call that returns once all rows have been transmitted. [source] find_neighbors # FeatureGroup . find_neighbors ( embedding , col = None , k = 10 , filter = None , min_score = 0 ) Finds the nearest neighbors for a given embedding in the vector database. Arguments embedding List[Union[int, float]] : The target embedding for which neighbors are to be found. col Optional[str] : The column name used to compute similarity score. Required only if there are multiple embeddings (optional). k Optional[int] : The number of nearest neighbors to retrieve (default is 10). filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : A filter expression to restrict the search space (optional). min_score Optional[float] : The minimum similarity score for neighbors to be considered (default is 0). Returns A list of tuples representing the nearest neighbors. Each tuple contains: (The similarity score, A list of feature values) Example embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=3) fg = fs.create_feature_group( name='air_quality', embedding_index = embedding_index, version=1, primary_key=['id1'], online_enabled=True, ) fg.insert(data) fg.find_neighbors( [0.1, 0.2, 0.3], k=5, ) # apply filter fg.find_neighbors( [0.1, 0.2, 0.3], k=5, filter=(fg.id1 > 10) & (fg.id1 < 30) ) [source] from_response_json # FeatureGroup . from_response_json ( json_dict ) [source] get_all_statistics # FeatureGroup . get_all_statistics ( computation_time = None , feature_names = None ) Returns all the statistics metadata computed before a specific time for the current feature group. If computation_time is None , all the statistics metadata are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source] get_all_validation_reports # FeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) val_reports = fg . get_all_validation_reports () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns Union[List[ ValidationReport ], ValidationReport ]. All validation reports attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source] get_complex_features # FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. Example complex_dtype_features = fg . get_complex_features () [source] get_expectation_suite # FeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) exp_suite = fg . get_expectation_suite () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get Feature instanse fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Arguments: name: The name of the feature to retrieve Returns: Feature: The feature object Raises hsfs.client.exceptions.FeatureStoreException . [source] get_feature_monitoring_configs # FeatureGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source] get_feature_monitoring_history # FeatureGroup . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fg . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # fetch feature monitoring history for a given feature monitoring config id fm_history = fg . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The start date of the feature monitoring history to fetch. Defaults to None. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source] get_fg_name # FeatureGroup . get_fg_name () [source] get_generated_feature_groups # FeatureGroup . get_generated_feature_groups () Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_generated_feature_views # FeatureGroup . get_generated_feature_views () Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_latest_validation_report # FeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the Feature Group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) latest_val_report = fg . get_latest_validation_report () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError . [source] get_parent_feature_groups # FeatureGroup . get_parent_feature_groups () Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_statistics # FeatureGroup . get_statistics ( computation_time = None , feature_names = None ) Returns the statistics computed at a specific time for the current feature group. If computation_time is None , the most recent statistics are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source] get_statistics_by_commit_window # FeatureGroup . get_statistics_by_commit_window ( from_commit_time = None , to_commit_time = None , feature_names = None ) Returns the statistics computed on a specific commit window for this feature group. If time travel is not enabled, it raises an exception. If from_commit_time is None , the commit window starts from the first commit. If to_commit_time is None , the commit window ends at the last commit. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics_by_commit_window ( from_commit_time = None , to_commit_time = None ) Arguments to_commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Date and time of the last commit of the window. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . from_commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Date and time of the first commit of the window. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name ) Get the tags of a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_tag_value = fg . get_tag ( \"example_tag\" ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source] get_validation_history # FeatureGroup . get_validation_history ( expectation_id , start_validation_time = None , end_validation_time = None , filter_by = [], ge_type = True ) Fetch validation history of an Expectation specified by its id. Example validation_history = fg . get_validation_history ( expectation_id = 1 , filter_by = [ \"REJECTED\" , \"UNKNOWN\" ], start_validation_time = \"2022-01-01 00:00:00\" , end_validation_time = datetime . datetime . now (), ge_type = False ) Arguments expectation_id int : id of the Expectation for which to fetch the validation history filter_by List[str] : list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\". start_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. end_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. Raises hsfs.client.exceptions.RestAPIError . Return Union[List[ ValidationResult ], List[ ExpectationValidationResult ]] A list of validation result connected to the expectation_id [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, save_code = True , wait = False , ) Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified features dataframe as feature group to the online/offline feature store. Changed in 3.3.0 insert and save methods are now async by default in non-spark clients. To achieve the old behaviour, set wait argument to True . Upsert new feature data with time travel format HUDI # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , description = 'Bitcoin price aggregated for days' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_bitcoin_processed ) Async insert # connect to the Feature Store fs = ... fg1 = fs . get_or_create_feature_group ( name = 'feature_group_name1' , description = 'Description of the first FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) # async insertion in order not to wait till finish of the job fg . insert ( df_for_fg1 , write_options = { \"wait_for_job\" : False }) fg2 = fs . get_or_create_feature_group ( name = 'feature_group_name2' , description = 'Description of the second FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_for_fg2 ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None (If the streaming APIs are enabled, specifying the storage option is not supported). write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job gets started immediately. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default True , to control whether the expectation suite of the feature group should be fetched before every insert. save_code Optional[bool] : When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create the feature group or used to insert data to it. When calling the insert method repeatedly with small batches of data, this can slow down the writes. Use this option to turn off saving code. Defaults to True . wait bool : Wait for job to finish before returning, defaults to False . Shortcut for read_options {\"wait_for_job\": False} . Returns ( Job , ValidationReport ) A tuple with job information if python engine is used and the validation report if validation is enabled. Raises hsfs.client.exceptions.RestAPIError . e.g fail to create feature group, dataframe schema does not match existing feature group schema, etc. hsfs.client.exceptions.DataValidationException . If data validation fails and the expectation suite validation_ingestion_policy is set to STRICT . Data is NOT ingested. [source] insert_stream # FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Engine Support Spark only Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Data Validation Support insert_stream does not perform any data validation using Great Expectations even when a expectation suite is attached. Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options: Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source] json # FeatureGroup . json () Get specific Feature Group metadata in json format. Example fg . json () [source] multi_part_insert # FeatureGroup . multi_part_insert ( features = None , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, ) Get FeatureGroupWriter for optimized multi part inserts or call this method to start manual multi part optimized inserts. In use cases where very small batches (1 to 1000) rows per Dataframe need to be written to the feature store repeatedly, it might be inefficient to use the standard feature_group.insert() method as it performs some background actions to update the metadata of the feature group object first. For these cases, the feature group provides the multi_part_insert API, which is optimized for writing many small Dataframes after another. There are two ways to use this API: Python Context Manager Using the Python with syntax you can acquire a FeatureGroupWriter object that implements the same multi_part_insert API. feature_group = fs . get_or_create_feature_group ( \"fg_name\" , version = 1 ) with feature_group . multi_part_insert () as writer : # run inserts in a loop: while loop : small_batch_df = ... writer . insert ( small_batch_df ) The writer batches the small Dataframes and transmits them to Hopsworks efficiently. When exiting the context, the feature group writer is sure to exit only once all the rows have been transmitted. Multi part insert with manual context management Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. feature_group = fs . get_or_create_feature_group ( \"fg_name\" , version = 1 ) while loop : small_batch_df = ... feature_group . multi_part_insert ( small_batch_df ) # IMPORTANT: finalize the multi part insert to make sure all rows # have been transmitted feature_group . finalize_multi_part_insert () Note that the first call to multi_part_insert initiates the context and be sure to finalize it. The finalize_multi_part_insert is a blocking call that returns once all rows have been transmitted. Once you are done with the multi part insert, it is good practice to start the materialization job in order to write the data to the offline storage: feature_group . materialization_job . run ( await_termination = True ) Arguments features Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job does not get started automatically for multi part inserts. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default False for multi part inserts, to control whether the expectation suite of the feature group should be fetched before every insert. Returns ( Job , ValidationReport ) A tuple with job information if python engine is used and the validation report if validation is enabled. FeatureGroupWriter When used as a context manager with Python with statement. [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . read () Read feature group as of specific point in time: fg = fs . get_or_create_feature_group ( ... ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : If specified will retrieve feature group as of specific point in time. Defaults to None . If not specified, will return as of most recent time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read feature group with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key \"pandas_types\" and value True to retrieve columns as Pandas nullable types rather than numpy/object(string) types (experimental). Defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises hsfs.client.exceptions.RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. Deprecated `read_changes` method is deprecated . Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)` instead . This function only works on feature groups with HUDI time travel format. Arguments start_wallclock_time Union[str, int, datetime.datetime, datetime.date] : Start time of the time travel query. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . end_wallclock_time Union[str, int, datetime.datetime, datetime.date] : End time of the time travel query. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . read_options Optional[dict] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises hsfs.client.exceptions.RestAPIError . No data is available for feature group with this commit date. hsfs.client.exceptions.FeatureStoreException . If the feature group does not have HUDI time travel format [source] save # FeatureGroup . save ( features = None , write_options = {}, validation_options = {}, wait = False ) Persist the metadata and materialize the feature group to the feature store. Changed in 3.3.0 insert and save methods are now async by default in non-spark clients. To achieve the old behaviour, set wait argument to True . Calling save creates the metadata for the feature group in the feature store. If a DataFrame, RDD or Ndarray is provided, the data is written to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[hsfs.feature.Feature]]] : DataFrame, RDD, Ndarray or a list of features. Features to be saved. This argument is optional if the feature list is provided in the create_feature_group or in the get_or_create_feature_group method invokation. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it does not wait. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job gets started immediately. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection, consider changing the producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. wait bool : Wait for job to finish before returning, defaults to False . Shortcut for read_options {\"wait_for_job\": False} . Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises hsfs.client.exceptions.RestAPIError . Unable to create feature group. [source] save_expectation_suite # FeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , overwrite = False ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . save_expectation_suite ( expectation_suite , run_validation = True ) Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the Feature Group. overwrite bool : If an Expectation Suite is already attached, overwrite it. The new suite will have its own validation history, but former reports are preserved. run_validation bool : Set whether the expectation_suite will run on ingestion validation_ingestion_policy str : Set the policy for ingestion to the Feature Group. \"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group. \"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result. Raises hsfs.client.exceptions.RestAPIError . [source] save_validation_report # FeatureGroup . save_validation_report ( validation_report , ingestion_result = \"UNKNOWN\" , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... , expectation_suite = expectation_suite ) validation_report = great_expectations . from_pandas ( my_experimental_features_df , fg . get_expectation_suite ()) . validate () fg . save_validation_report ( validation_report , ingestion_result = \"EXPERIMENT\" ) Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the Feature Group. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises hsfs.client.exceptions.RestAPIError . [source] select # FeatureGroup . select ( features ) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select ([ \"id\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features List[Union[str, hsfs.feature.Feature]] : A list of Feature objects or feature names as strings to be selected. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all ( include_primary_key = True , include_event_time = True ) Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view. Example # connect to the Feature Store fs = ... # get the Feature Group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # show first 5 rows query . show ( 5 ) # select all features exclude primary key and event time from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) query = fg . select_all () query . features # [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)] query = fg . select_all ( include_primary_key = False , include_event_time = False ) query . features # [Feature('f1', ...), Feature('f2', ...)] Arguments include_primary_key Optional[bool] : If True, include primary key of the feature group to the feature list. Defaults to True. include_event_time Optional[bool] : If True, include event time of the feature group to the feature list. Defaults to True. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features including primary key and event time feature of the feature group except provided features and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select_except ([ \"ts\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features Optional[List[Union[str, hsfs.feature.Feature]]] : A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # make a query and show top 5 rows fg . select ([ 'date' , 'weekly_sales' , 'is_holiday' ]) . show ( 5 ) Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] to_dict # FeatureGroup . to_dict () Get structured info about specific Feature Group in python dictionary format. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . to_dict () [source] update_deprecated # FeatureGroup . update_deprecated ( deprecate = True ) Deprecate the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_deprecated ( deprecate = True ) Safe update This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged. Arguments deprecate bool : Boolean value identifying if the feature group should be deprecated. Defaults to True. Returns FeatureGroup . The updated feature group object. [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_description ( description = \"Much better description.\" ) Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_feature_description ( feature_name = \"min_temp\" , description = \"Much better feature description.\" ) Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # FeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # FeatureGroup . update_from_response_json ( json_dict ) [source] update_notification_topic_name # FeatureGroup . update_notification_topic_name ( notification_topic_name ) Update the notification topic name of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_notification_topic_name ( notification_topic_name = \"notification_topic_name\" ) Safe update This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name. Arguments notification_topic_name str : Name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If set to None no notifications are sent. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_statistics_config () Returns FeatureGroup . The updated metadata object of the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source] validate # FeatureGroup . validate ( dataframe = None , expectation_suite = None , save_report = False , validation_options = {}, ingestion_result = \"UNKNOWN\" , ge_type = True , ) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Example # connect to the Feature Store fs = ... # get feature group instance fg = fs . get_or_create_feature_group ( ... ) ge_report = fg . validate ( df , save_report = False ) Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The dataframe to run the data validation expectations against. expectation_suite Optional[hsfs.expectation_suite.ExpectationSuite] : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. save_report Optional[bool] : Whether to save the report to the backend. This is only possible if the Expectation suite is initialised and attached to the Feature Group. Defaults to False. ge_type bool : Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True. Returns A Validation Report produced by Great Expectations.","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup","text":"[source]","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup_1","text":"hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , embedding_index = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , online_topic_name = None , topic_name = None , notification_topic_name = None , event_time = None , stream = False , expectation_suite = None , parents = None , href = None , delta_streamer_job_conf = None , deprecated = False , ** kwargs )","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_group_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , parents = [], topic_name = None , notification_topic_name = None , ) Create a feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . create_feature_group ( name = 'air_quality' , description = 'Air Quality characteristics of each day' , version = 1 , primary_key = [ 'city' , 'date' ], online_enabled = True , event_time = 'date' ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, vector database is used as online feature store. This enables similarity search by using find_neighbors . default to None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_group_api/#get_or_create_feature_group","text":"FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , parents = [], topic_name = None , notification_topic_name = None , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = \"electricity_prices\" , version = 1 , description = \"Electricity prices from NORD POOL\" , primary_key = [ \"day\" , \"area\" ], online_enabled = True , event_time = \"timestamp\" , ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, the vector database is used as online feature store. This enables similarity search by using find_neighbors . default is None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object.","title":"get_or_create_feature_group"},{"location":"generated/api/feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_group_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... fg = fs . get_feature_group ( name = \"electricity_prices\" , version = 1 , ) Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/api/feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_group_api/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/api/feature_group_api/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/api/feature_group_api/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/api/feature_group_api/#deprecated","text":"Setting if the feature group is deprecated. [source]","title":"deprecated"},{"location":"generated/api/feature_group_api/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/api/feature_group_api/#embedding_index","text":"[source]","title":"embedding_index"},{"location":"generated/api/feature_group_api/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/api/feature_group_api/#expectation_suite","text":"Expectation Suite configuration object defining the settings for data validation of the feature group. [source]","title":"expectation_suite"},{"location":"generated/api/feature_group_api/#feature_store","text":"[source]","title":"feature_store"},{"location":"generated/api/feature_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/feature_group_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/feature_group_api/#features","text":"Feature Group schema (alias) [source]","title":"features"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_group_api/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/api/feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/feature_group_api/#materialization_job","text":"Get the Job object reference for the materialization job for this Feature Group. [source]","title":"materialization_job"},{"location":"generated/api/feature_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/feature_group_api/#notification_topic_name","text":"The topic used for feature group notifications. [source]","title":"notification_topic_name"},{"location":"generated/api/feature_group_api/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/api/feature_group_api/#parents","text":"Parent feature groups as origin of the data in the current feature group. This is part of explicit provenance [source]","title":"parents"},{"location":"generated/api/feature_group_api/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/api/feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/feature_group_api/#schema","text":"Feature Group schema [source]","title":"schema"},{"location":"generated/api/feature_group_api/#statistics","text":"Get the latest computed statistics for the whole feature group. [source]","title":"statistics"},{"location":"generated/api/feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. Raises hsfs.client.exceptions.FeatureStoreException . [source]","title":"statistics_config"},{"location":"generated/api/feature_group_api/#stream","text":"Whether to enable real time stream writing capabilities. [source]","title":"stream"},{"location":"generated/api/feature_group_api/#subject","text":"Subject of the feature group. [source]","title":"subject"},{"location":"generated/api/feature_group_api/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/api/feature_group_api/#topic_name","text":"The topic used for feature group data ingestion. [source]","title":"topic_name"},{"location":"generated/api/feature_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_group_api/#add_tag","text":"FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . add_tag ( name = \"example_tag\" , value = \"42\" ) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/feature_group_api/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Example # connect to the Feature Store fs = ... # define features to be inserted in the feature group features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . append_features ( features ) Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/feature_group_api/#as_of","text":"FeatureGroup . as_of ( wallclock_time = None , exclude_until = None ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get data at a specific point in time and show it fg . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fg . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fg . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time: fg . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) fg1 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" ) . join ( fg2 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: Example fg1 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" ) # as_of is not applied . join ( fg2 . select_all () . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-15\" )) # as_of is not applied . as_of ( \"2020-10-20\" , exclude_until = \"2020-10-19\" ) Warning This function only works for feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Read data as of this point in time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . exclude_until Optional[Union[str, int, datetime.datetime, datetime.date]] : Exclude commits until this point in time. String should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/api/feature_group_api/#check_deprecated","text":"FeatureGroup . check_deprecated () [source]","title":"check_deprecated"},{"location":"generated/api/feature_group_api/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on feature groups stored as HUDI or DELTA. Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises hsfs.client.exceptions.RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/api/feature_group_api/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) commit_details = fg . commit_details () Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Commit details as of specific point in time. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"commit_details"},{"location":"generated/api/feature_group_api/#compute_statistics","text":"FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Returns Statistics . The statistics metadata object. Raises hsfs.client.exceptions.RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/api/feature_group_api/#create_feature_monitoring","text":"FeatureGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_feature_monitoring"},{"location":"generated/api/feature_group_api/#create_statistics_monitoring","text":"FeatureGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_statistics_monitoring"},{"location":"generated/api/feature_group_api/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , version = 1 ) # delete the feature group fg . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_group_api/#delete_expectation_suite","text":"FeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_expectation_suite () Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete_expectation_suite"},{"location":"generated/api/feature_group_api/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_tag ( \"example_tag\" ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/feature_group_api/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: Example fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: Example fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/feature_group_api/#finalize_multi_part_insert","text":"FeatureGroup . finalize_multi_part_insert () Finalizes and exits the multi part insert context opened by multi_part_insert in a blocking fashion once all rows have been transmitted. Multi part insert with manual context management Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. feature_group = fs . get_or_create_feature_group ( \"fg_name\" , version = 1 ) while loop : small_batch_df = ... feature_group . multi_part_insert ( small_batch_df ) # IMPORTANT: finalize the multi part insert to make sure all rows # have been transmitted feature_group . finalize_multi_part_insert () Note that the first call to multi_part_insert initiates the context and be sure to finalize it. The finalize_multi_part_insert is a blocking call that returns once all rows have been transmitted. [source]","title":"finalize_multi_part_insert"},{"location":"generated/api/feature_group_api/#find_neighbors","text":"FeatureGroup . find_neighbors ( embedding , col = None , k = 10 , filter = None , min_score = 0 ) Finds the nearest neighbors for a given embedding in the vector database. Arguments embedding List[Union[int, float]] : The target embedding for which neighbors are to be found. col Optional[str] : The column name used to compute similarity score. Required only if there are multiple embeddings (optional). k Optional[int] : The number of nearest neighbors to retrieve (default is 10). filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : A filter expression to restrict the search space (optional). min_score Optional[float] : The minimum similarity score for neighbors to be considered (default is 0). Returns A list of tuples representing the nearest neighbors. Each tuple contains: (The similarity score, A list of feature values) Example embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=3) fg = fs.create_feature_group( name='air_quality', embedding_index = embedding_index, version=1, primary_key=['id1'], online_enabled=True, ) fg.insert(data) fg.find_neighbors( [0.1, 0.2, 0.3], k=5, ) # apply filter fg.find_neighbors( [0.1, 0.2, 0.3], k=5, filter=(fg.id1 > 10) & (fg.id1 < 30) ) [source]","title":"find_neighbors"},{"location":"generated/api/feature_group_api/#from_response_json","text":"FeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_group_api/#get_all_statistics","text":"FeatureGroup . get_all_statistics ( computation_time = None , feature_names = None ) Returns all the statistics metadata computed before a specific time for the current feature group. If computation_time is None , all the statistics metadata are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_all_statistics"},{"location":"generated/api/feature_group_api/#get_all_validation_reports","text":"FeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) val_reports = fg . get_all_validation_reports () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns Union[List[ ValidationReport ], ValidationReport ]. All validation reports attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_all_validation_reports"},{"location":"generated/api/feature_group_api/#get_complex_features","text":"FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. Example complex_dtype_features = fg . get_complex_features () [source]","title":"get_complex_features"},{"location":"generated/api/feature_group_api/#get_expectation_suite","text":"FeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) exp_suite = fg . get_expectation_suite () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_expectation_suite"},{"location":"generated/api/feature_group_api/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get Feature instanse fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Arguments: name: The name of the feature to retrieve Returns: Feature: The feature object Raises hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_feature"},{"location":"generated/api/feature_group_api/#get_feature_monitoring_configs","text":"FeatureGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source]","title":"get_feature_monitoring_configs"},{"location":"generated/api/feature_group_api/#get_feature_monitoring_history","text":"FeatureGroup . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fg . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # fetch feature monitoring history for a given feature monitoring config id fm_history = fg . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The start date of the feature monitoring history to fetch. Defaults to None. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source]","title":"get_feature_monitoring_history"},{"location":"generated/api/feature_group_api/#get_fg_name","text":"FeatureGroup . get_fg_name () [source]","title":"get_fg_name"},{"location":"generated/api/feature_group_api/#get_generated_feature_groups","text":"FeatureGroup . get_generated_feature_groups () Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_generated_feature_groups"},{"location":"generated/api/feature_group_api/#get_generated_feature_views","text":"FeatureGroup . get_generated_feature_views () Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_generated_feature_views"},{"location":"generated/api/feature_group_api/#get_latest_validation_report","text":"FeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the Feature Group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) latest_val_report = fg . get_latest_validation_report () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_latest_validation_report"},{"location":"generated/api/feature_group_api/#get_parent_feature_groups","text":"FeatureGroup . get_parent_feature_groups () Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_parent_feature_groups"},{"location":"generated/api/feature_group_api/#get_statistics","text":"FeatureGroup . get_statistics ( computation_time = None , feature_names = None ) Returns the statistics computed at a specific time for the current feature group. If computation_time is None , the most recent statistics are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_statistics"},{"location":"generated/api/feature_group_api/#get_statistics_by_commit_window","text":"FeatureGroup . get_statistics_by_commit_window ( from_commit_time = None , to_commit_time = None , feature_names = None ) Returns the statistics computed on a specific commit window for this feature group. If time travel is not enabled, it raises an exception. If from_commit_time is None , the commit window starts from the first commit. If to_commit_time is None , the commit window ends at the last commit. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics_by_commit_window ( from_commit_time = None , to_commit_time = None ) Arguments to_commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Date and time of the last commit of the window. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . from_commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Date and time of the first commit of the window. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_statistics_by_commit_window"},{"location":"generated/api/feature_group_api/#get_tag","text":"FeatureGroup . get_tag ( name ) Get the tags of a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_tag_value = fg . get_tag ( \"example_tag\" ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/feature_group_api/#get_tags","text":"FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/feature_group_api/#get_validation_history","text":"FeatureGroup . get_validation_history ( expectation_id , start_validation_time = None , end_validation_time = None , filter_by = [], ge_type = True ) Fetch validation history of an Expectation specified by its id. Example validation_history = fg . get_validation_history ( expectation_id = 1 , filter_by = [ \"REJECTED\" , \"UNKNOWN\" ], start_validation_time = \"2022-01-01 00:00:00\" , end_validation_time = datetime . datetime . now (), ge_type = False ) Arguments expectation_id int : id of the Expectation for which to fetch the validation history filter_by List[str] : list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\". start_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. end_validation_time Optional[Union[str, int, datetime.datetime, datetime.date]] : fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above. Raises hsfs.client.exceptions.RestAPIError . Return Union[List[ ValidationResult ], List[ ExpectationValidationResult ]] A list of validation result connected to the expectation_id [source]","title":"get_validation_history"},{"location":"generated/api/feature_group_api/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, save_code = True , wait = False , ) Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified features dataframe as feature group to the online/offline feature store. Changed in 3.3.0 insert and save methods are now async by default in non-spark clients. To achieve the old behaviour, set wait argument to True . Upsert new feature data with time travel format HUDI # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , description = 'Bitcoin price aggregated for days' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_bitcoin_processed ) Async insert # connect to the Feature Store fs = ... fg1 = fs . get_or_create_feature_group ( name = 'feature_group_name1' , description = 'Description of the first FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) # async insertion in order not to wait till finish of the job fg . insert ( df_for_fg1 , write_options = { \"wait_for_job\" : False }) fg2 = fs . get_or_create_feature_group ( name = 'feature_group_name2' , description = 'Description of the second FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_for_fg2 ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None (If the streaming APIs are enabled, specifying the storage option is not supported). write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job gets started immediately. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default True , to control whether the expectation suite of the feature group should be fetched before every insert. save_code Optional[bool] : When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create the feature group or used to insert data to it. When calling the insert method repeatedly with small batches of data, this can slow down the writes. Use this option to turn off saving code. Defaults to True . wait bool : Wait for job to finish before returning, defaults to False . Shortcut for read_options {\"wait_for_job\": False} . Returns ( Job , ValidationReport ) A tuple with job information if python engine is used and the validation report if validation is enabled. Raises hsfs.client.exceptions.RestAPIError . e.g fail to create feature group, dataframe schema does not match existing feature group schema, etc. hsfs.client.exceptions.DataValidationException . If data validation fails and the expectation suite validation_ingestion_policy is set to STRICT . Data is NOT ingested. [source]","title":"insert"},{"location":"generated/api/feature_group_api/#insert_stream","text":"FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Engine Support Spark only Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Data Validation Support insert_stream does not perform any data validation using Great Expectations even when a expectation suite is attached. Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options: Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source]","title":"insert_stream"},{"location":"generated/api/feature_group_api/#json","text":"FeatureGroup . json () Get specific Feature Group metadata in json format. Example fg . json () [source]","title":"json"},{"location":"generated/api/feature_group_api/#multi_part_insert","text":"FeatureGroup . multi_part_insert ( features = None , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, ) Get FeatureGroupWriter for optimized multi part inserts or call this method to start manual multi part optimized inserts. In use cases where very small batches (1 to 1000) rows per Dataframe need to be written to the feature store repeatedly, it might be inefficient to use the standard feature_group.insert() method as it performs some background actions to update the metadata of the feature group object first. For these cases, the feature group provides the multi_part_insert API, which is optimized for writing many small Dataframes after another. There are two ways to use this API: Python Context Manager Using the Python with syntax you can acquire a FeatureGroupWriter object that implements the same multi_part_insert API. feature_group = fs . get_or_create_feature_group ( \"fg_name\" , version = 1 ) with feature_group . multi_part_insert () as writer : # run inserts in a loop: while loop : small_batch_df = ... writer . insert ( small_batch_df ) The writer batches the small Dataframes and transmits them to Hopsworks efficiently. When exiting the context, the feature group writer is sure to exit only once all the rows have been transmitted. Multi part insert with manual context management Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. feature_group = fs . get_or_create_feature_group ( \"fg_name\" , version = 1 ) while loop : small_batch_df = ... feature_group . multi_part_insert ( small_batch_df ) # IMPORTANT: finalize the multi part insert to make sure all rows # have been transmitted feature_group . finalize_multi_part_insert () Note that the first call to multi_part_insert initiates the context and be sure to finalize it. The finalize_multi_part_insert is a blocking call that returns once all rows have been transmitted. Once you are done with the multi part insert, it is good practice to start the materialization job in order to write the data to the offline storage: feature_group . materialization_job . run ( await_termination = True ) Arguments features Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job does not get started automatically for multi part inserts. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default False for multi part inserts, to control whether the expectation suite of the feature group should be fetched before every insert. Returns ( Job , ValidationReport ) A tuple with job information if python engine is used and the validation report if validation is enabled. FeatureGroupWriter When used as a context manager with Python with statement. [source]","title":"multi_part_insert"},{"location":"generated/api/feature_group_api/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . read () Read feature group as of specific point in time: fg = fs . get_or_create_feature_group ( ... ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : If specified will retrieve feature group as of specific point in time. Defaults to None . If not specified, will return as of most recent time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read feature group with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key \"pandas_types\" and value True to retrieve columns as Pandas nullable types rather than numpy/object(string) types (experimental). Defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises hsfs.client.exceptions.RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/api/feature_group_api/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. Deprecated `read_changes` method is deprecated . Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)` instead . This function only works on feature groups with HUDI time travel format. Arguments start_wallclock_time Union[str, int, datetime.datetime, datetime.date] : Start time of the time travel query. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . end_wallclock_time Union[str, int, datetime.datetime, datetime.date] : End time of the time travel query. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . read_options Optional[dict] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises hsfs.client.exceptions.RestAPIError . No data is available for feature group with this commit date. hsfs.client.exceptions.FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"read_changes"},{"location":"generated/api/feature_group_api/#save","text":"FeatureGroup . save ( features = None , write_options = {}, validation_options = {}, wait = False ) Persist the metadata and materialize the feature group to the feature store. Changed in 3.3.0 insert and save methods are now async by default in non-spark clients. To achieve the old behaviour, set wait argument to True . Calling save creates the metadata for the feature group in the feature store. If a DataFrame, RDD or Ndarray is provided, the data is written to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[hsfs.feature.Feature]]] : DataFrame, RDD, Ndarray or a list of features. Features to be saved. This argument is optional if the feature list is provided in the create_feature_group or in the get_or_create_feature_group method invokation. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it does not wait. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job gets started immediately. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection, consider changing the producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. wait bool : Wait for job to finish before returning, defaults to False . Shortcut for read_options {\"wait_for_job\": False} . Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises hsfs.client.exceptions.RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/api/feature_group_api/#save_expectation_suite","text":"FeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" , overwrite = False ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . save_expectation_suite ( expectation_suite , run_validation = True ) Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the Feature Group. overwrite bool : If an Expectation Suite is already attached, overwrite it. The new suite will have its own validation history, but former reports are preserved. run_validation bool : Set whether the expectation_suite will run on ingestion validation_ingestion_policy str : Set the policy for ingestion to the Feature Group. \"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group. \"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"save_expectation_suite"},{"location":"generated/api/feature_group_api/#save_validation_report","text":"FeatureGroup . save_validation_report ( validation_report , ingestion_result = \"UNKNOWN\" , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same Feature Group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... , expectation_suite = expectation_suite ) validation_report = great_expectations . from_pandas ( my_experimental_features_df , fg . get_expectation_suite ()) . validate () fg . save_validation_report ( validation_report , ingestion_result = \"EXPERIMENT\" ) Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the Feature Group. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises hsfs.client.exceptions.RestAPIError . [source]","title":"save_validation_report"},{"location":"generated/api/feature_group_api/#select","text":"FeatureGroup . select ( features ) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select ([ \"id\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features List[Union[str, hsfs.feature.Feature]] : A list of Feature objects or feature names as strings to be selected. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/feature_group_api/#select_all","text":"FeatureGroup . select_all ( include_primary_key = True , include_event_time = True ) Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view. Example # connect to the Feature Store fs = ... # get the Feature Group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # show first 5 rows query . show ( 5 ) # select all features exclude primary key and event time from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) query = fg . select_all () query . features # [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)] query = fg . select_all ( include_primary_key = False , include_event_time = False ) query . features # [Feature('f1', ...), Feature('f2', ...)] Arguments include_primary_key Optional[bool] : If True, include primary key of the feature group to the feature list. Defaults to True. include_event_time Optional[bool] : If True, include event time of the feature group to the feature list. Defaults to True. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/feature_group_api/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features including primary key and event time feature of the feature group except provided features and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select_except ([ \"ts\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features Optional[List[Union[str, hsfs.feature.Feature]]] : A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/feature_group_api/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # make a query and show top 5 rows fg . select ([ 'date' , 'weekly_sales' , 'is_holiday' ]) . show ( 5 ) Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/api/feature_group_api/#to_dict","text":"FeatureGroup . to_dict () Get structured info about specific Feature Group in python dictionary format. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_group_api/#update_deprecated","text":"FeatureGroup . update_deprecated ( deprecate = True ) Deprecate the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_deprecated ( deprecate = True ) Safe update This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged. Arguments deprecate bool : Boolean value identifying if the feature group should be deprecated. Defaults to True. Returns FeatureGroup . The updated feature group object. [source]","title":"update_deprecated"},{"location":"generated/api/feature_group_api/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_description ( description = \"Much better description.\" ) Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/feature_group_api/#update_feature_description","text":"FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_feature_description ( feature_name = \"min_temp\" , description = \"Much better feature description.\" ) Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/api/feature_group_api/#update_features","text":"FeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/api/feature_group_api/#update_from_response_json","text":"FeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/feature_group_api/#update_notification_topic_name","text":"FeatureGroup . update_notification_topic_name ( notification_topic_name ) Update the notification topic name of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_notification_topic_name ( notification_topic_name = \"notification_topic_name\" ) Safe update This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name. Arguments notification_topic_name str : Name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If set to None no notifications are sent. Returns FeatureGroup . The updated feature group object. [source]","title":"update_notification_topic_name"},{"location":"generated/api/feature_group_api/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_statistics_config () Returns FeatureGroup . The updated metadata object of the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . [source]","title":"update_statistics_config"},{"location":"generated/api/feature_group_api/#validate","text":"FeatureGroup . validate ( dataframe = None , expectation_suite = None , save_report = False , validation_options = {}, ingestion_result = \"UNKNOWN\" , ge_type = True , ) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Example # connect to the Feature Store fs = ... # get feature group instance fg = fs . get_or_create_feature_group ( ... ) ge_report = fg . validate ( df , save_report = False ) Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The dataframe to run the data validation expectations against. expectation_suite Optional[hsfs.expectation_suite.ExpectationSuite] : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. save_report Optional[bool] : Whether to save the report to the backend. This is only possible if the Expectation suite is initialised and attached to the Feature Group. Defaults to False. ge_type bool : Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True. Returns A Validation Report produced by Great Expectations.","title":"validate"},{"location":"generated/api/feature_monitoring_config_api/","text":"Feature Monitoring Configuration # [source] FeatureMonitoringConfig # hsfs . core . feature_monitoring_config . FeatureMonitoringConfig ( feature_store_id , name , feature_name = None , feature_monitoring_type = STATISTICS_COMPUTATION , job_name = None , detection_window_config = None , reference_window_config = None , statistics_comparison_config = None , job_schedule = None , description = None , id = None , feature_group_id = None , feature_view_name = None , feature_view_version = None , href = None , ** kwargs ) Creation from Feature Group # [source] create_statistics_monitoring # FeatureGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_feature_monitoring # FeatureGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. Creation from Feature View # [source] create_statistics_monitoring # FeatureView . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable statistics monitoring my_config = fv . _create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature view. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_feature_monitoring # FeatureView . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fg = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # compare to a given value specific_value = 0.5 , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. Retrieval from Feature Group # [source] get_feature_monitoring_configs # FeatureGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. Retrieval from Feature View # [source] get_feature_monitoring_configs # FeatureView . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # fetch all feature monitoring configs attached to the feature view fm_configs = fv . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fv . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fv . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a particular id fm_config = fv . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. Properties # [source] description # Description of the feature monitoring configuration. [source] detection_window_config # Configuration for the detection window. [source] enabled # Controls whether or not this config is spawning new feature monitoring jobs. This field belongs to the scheduler configuration but is made transparent to the user for convenience. [source] feature_group_id # Id of the Feature Group to which this feature monitoring configuration is attached. [source] feature_monitoring_type # The type of feature monitoring to perform. Used for internal validation. Options are: - STATISTICS_COMPUTATION if no reference window (and, therefore, comparison config) is provided - STATISTICS_COMPARISON if a reference window (and, therefore, comparison config) is provided. This property is read-only. [source] feature_name # The name of the feature to monitor. If not set, all features of the Feature Group or Feature View are monitored, only available for scheduled statistics. This property is read-only [source] feature_store_id # Id of the Feature Store. [source] feature_view_name # Name of the Feature View to which this feature monitoring configuration is attached. [source] feature_view_version # Version of the Feature View to which this feature monitoring configuration is attached. [source] id # Id of the feature monitoring configuration. [source] job_name # Name of the feature monitoring job. [source] job_schedule # Schedule of the feature monitoring job. This field belongs to the job configuration but is made transparent to the user for convenience. [source] name # The name of the feature monitoring config. A Feature Group or Feature View cannot have multiple feature monitoring configurations with the same name. The name of a feature monitoring configuration is limited to 63 characters. This property is read-only once the feature monitoring configuration has been saved. [source] reference_window_config # Configuration for the reference window. [source] statistics_comparison_config # Configuration for the comparison of detection and reference statistics. Methods # [source] compare_on # FeatureMonitoringConfig . compare_on ( metric , threshold , strict = False , relative = False ) Sets the statistics comparison criteria for feature monitoring with a reference window. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring, a detection window and a reference window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) . with_reference_window ( ... ) # Choose a metric and set a threshold for the difference # e.g compare the relative mean of detection and reference window my_monitoring_config . compare_on ( metric = \"mean\" , threshold = 1.0 , relative = True , ) . save () Note Detection window and reference window/value/training_dataset must be set prior to comparison configuration. Arguments metric Optional[str] : The metric to use for comparison. Different metric are available for different feature type. threshold Optional[float] : The threshold to apply to the difference to potentially trigger an alert. strict Optional[bool] : Whether to use a strict comparison (e.g. > or <) or a non-strict comparison (e.g. >= or <=). relative Optional[bool] : Whether to use a relative comparison (e.g. relative mean) or an absolute comparison (e.g. absolute mean). Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source] delete # FeatureMonitoringConfig . delete () Deletes the feature monitoring configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Delete the feature monitoring config my_monitoring_config . delete () Raises FeatureStoreException : If the feature monitoring config has not been saved. [source] disable # FeatureMonitoringConfig . disable () Disables the schedule of the feature monitoring job. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Disable the feature monitoring config my_monitoring_config . disable () Raises FeatureStoreException : If the feature monitoring config has not been saved. [source] enable # FeatureMonitoringConfig . enable () Enables the schedule of the feature monitoring job. The scheduler can be configured via the job_schedule property. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Enable the feature monitoring config my_monitoring_config . enable () Raises FeatureStoreException : If the feature monitoring config has not been saved. [source] get_history # FeatureMonitoringConfig . get_history ( start_time = None , end_time = None , with_statistics = True ) Fetch the history of the computed statistics and comparison results for this configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Fetch the history of the computed statistics for this configuration history = my_monitoring_config . get_history ( start_time = \"2021-01-01\" , end_time = \"2021-01-31\" , ) Args: start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results. Raises FeatureStoreException : If the feature monitoring config has not been saved. [source] get_job # FeatureMonitoringConfig . get_job () Get the feature monitoring job which computes and compares statistics on the detection and reference windows. Example # Fetch registered config by name via feature group or feature view my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Get the job which computes statistics on detection and reference window job = my_monitoring_config . get_job () # Print job history and ongoing executions job . executions Raises FeatureStoreException : If the feature monitoring config has not been saved. Returns Job . A handle for the job computing the statistics. [source] run_job # FeatureMonitoringConfig . run_job () Trigger the feature monitoring job which computes and compares statistics on the detection and reference windows. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Trigger the feature monitoring job once my_monitoring_config . run_job () Info The feature monitoring job will be triggered asynchronously and the method will return immediately. Calling this method does not affect the ongoing schedule. Raises FeatureStoreException : If the feature monitoring config has not been saved. Returns Job . A handle for the job computing the statistics. [source] save # FeatureMonitoringConfig . save () Saves the feature monitoring configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_statistics_monitoring ( name = \"my_monitoring_config\" , ) . save () Returns FeatureMonitoringConfig . The saved FeatureMonitoringConfig object. [source] update # FeatureMonitoringConfig . update () Updates allowed fields of the saved feature monitoring configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Update the percentage of rows to use when computing the statistics my_monitoring_config . detection_window . row_percentage = 10 my_monitoring_config . update () Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source] with_detection_window # FeatureMonitoringConfig . with_detection_window ( time_offset = None , window_length = None , row_percentage = None ) Sets the detection window of data to compute statistics on. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Compute statistics on a regular basis fg . create_statistics_monitoring ( name = \"regular_stats\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( time_offset = \"1d\" , window_length = \"1d\" , row_percentage = 0.1 , ) . save () # Compute and compare statistics fg . create_feature_monitoring ( name = \"regular_stats\" , feature_name = \"my_feature\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( time_offset = \"1d\" , window_length = \"1d\" , row_percentage = 0.1 , ) . with_reference_window ( ... ) . compare_on ( ... ) . save () Arguments time_offset Optional[str] : The time offset from the current time to the start of the time window. window_length Optional[str] : The length of the time window. row_percentage Optional[float] : The fraction of rows to use when computing the statistics [0, 1.0]. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source] with_reference_training_dataset # FeatureMonitoringConfig . with_reference_training_dataset ( training_dataset_version = None ) Sets the reference training dataset to compare statistics with. See also with_reference_value(...) and with_reference_window(...) for other reference options. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) # Only for feature views: Compare to the statistics computed for one of your training datasets # particularly useful if it has been used to train a model currently in production my_monitoring_config . with_reference_training_dataset ( training_dataset_version = 3 , ) . compare_on ( ... ) . save () Provide a comparison configuration You must provide a comparison configuration via compare_on() before saving the feature monitoring config. Arguments training_dataset_version Optional[int] : The version of the training dataset to use as reference. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source] with_reference_value # FeatureMonitoringConfig . with_reference_value ( value = None ) Sets the reference value to compare statistics with. See also with_reference_window(...) and with_reference_training_dataset(...) for other reference options. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) # Simplest reference window is a specific value my_monitoring_config . with_reference_value ( value = 0.0 , ) . compare_on ( ... ) . save () Provide a comparison configuration You must provide a comparison configuration via compare_on() before saving the feature monitoring config. Arguments value Optional[Union[int, float]] : A float value to use as reference. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source] with_reference_window # FeatureMonitoringConfig . with_reference_window ( time_offset = None , window_length = None , row_percentage = None ) Sets the reference window of data to compute statistics on. See also with_reference_value(...) and with_reference_training_dataset(...) for other reference options. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) # Statistics computed on a rolling time window, e.g. same day last week my_monitoring_config . with_reference_window ( time_offset = \"1w\" , window_length = \"1d\" , ) . compare_on ( ... ) . save () Provide a comparison configuration You must provide a comparison configuration via compare_on() before saving the feature monitoring config. Arguments time_offset Optional[str] : The time offset from the current time to the start of the time window. window_length Optional[str] : The length of the time window. row_percentage Optional[float] : The percentage of rows to use when computing the statistics. Defaults to 20%. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object.","title":"Configuration"},{"location":"generated/api/feature_monitoring_config_api/#feature-monitoring-configuration","text":"[source]","title":"Feature Monitoring Configuration"},{"location":"generated/api/feature_monitoring_config_api/#featuremonitoringconfig","text":"hsfs . core . feature_monitoring_config . FeatureMonitoringConfig ( feature_store_id , name , feature_name = None , feature_monitoring_type = STATISTICS_COMPUTATION , job_name = None , detection_window_config = None , reference_window_config = None , statistics_comparison_config = None , job_schedule = None , description = None , id = None , feature_group_id = None , feature_view_name = None , feature_view_version = None , href = None , ** kwargs )","title":"FeatureMonitoringConfig"},{"location":"generated/api/feature_monitoring_config_api/#creation-from-feature-group","text":"[source]","title":"Creation from Feature Group"},{"location":"generated/api/feature_monitoring_config_api/#create_statistics_monitoring","text":"FeatureGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_statistics_monitoring"},{"location":"generated/api/feature_monitoring_config_api/#create_feature_monitoring","text":"FeatureGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled.","title":"create_feature_monitoring"},{"location":"generated/api/feature_monitoring_config_api/#creation-from-feature-view","text":"[source]","title":"Creation from Feature View"},{"location":"generated/api/feature_monitoring_config_api/#create_statistics_monitoring_1","text":"FeatureView . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable statistics monitoring my_config = fv . _create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature view. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_statistics_monitoring"},{"location":"generated/api/feature_monitoring_config_api/#create_feature_monitoring_1","text":"FeatureView . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fg = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # compare to a given value specific_value = 0.5 , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled.","title":"create_feature_monitoring"},{"location":"generated/api/feature_monitoring_config_api/#retrieval-from-feature-group","text":"[source]","title":"Retrieval from Feature Group"},{"location":"generated/api/feature_monitoring_config_api/#get_feature_monitoring_configs","text":"FeatureGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found.","title":"get_feature_monitoring_configs"},{"location":"generated/api/feature_monitoring_config_api/#retrieval-from-feature-view","text":"[source]","title":"Retrieval from Feature View"},{"location":"generated/api/feature_monitoring_config_api/#get_feature_monitoring_configs_1","text":"FeatureView . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # fetch all feature monitoring configs attached to the feature view fm_configs = fv . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fv . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fv . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a particular id fm_config = fv . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found.","title":"get_feature_monitoring_configs"},{"location":"generated/api/feature_monitoring_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_monitoring_config_api/#description","text":"Description of the feature monitoring configuration. [source]","title":"description"},{"location":"generated/api/feature_monitoring_config_api/#detection_window_config","text":"Configuration for the detection window. [source]","title":"detection_window_config"},{"location":"generated/api/feature_monitoring_config_api/#enabled","text":"Controls whether or not this config is spawning new feature monitoring jobs. This field belongs to the scheduler configuration but is made transparent to the user for convenience. [source]","title":"enabled"},{"location":"generated/api/feature_monitoring_config_api/#feature_group_id","text":"Id of the Feature Group to which this feature monitoring configuration is attached. [source]","title":"feature_group_id"},{"location":"generated/api/feature_monitoring_config_api/#feature_monitoring_type","text":"The type of feature monitoring to perform. Used for internal validation. Options are: - STATISTICS_COMPUTATION if no reference window (and, therefore, comparison config) is provided - STATISTICS_COMPARISON if a reference window (and, therefore, comparison config) is provided. This property is read-only. [source]","title":"feature_monitoring_type"},{"location":"generated/api/feature_monitoring_config_api/#feature_name","text":"The name of the feature to monitor. If not set, all features of the Feature Group or Feature View are monitored, only available for scheduled statistics. This property is read-only [source]","title":"feature_name"},{"location":"generated/api/feature_monitoring_config_api/#feature_store_id","text":"Id of the Feature Store. [source]","title":"feature_store_id"},{"location":"generated/api/feature_monitoring_config_api/#feature_view_name","text":"Name of the Feature View to which this feature monitoring configuration is attached. [source]","title":"feature_view_name"},{"location":"generated/api/feature_monitoring_config_api/#feature_view_version","text":"Version of the Feature View to which this feature monitoring configuration is attached. [source]","title":"feature_view_version"},{"location":"generated/api/feature_monitoring_config_api/#id","text":"Id of the feature monitoring configuration. [source]","title":"id"},{"location":"generated/api/feature_monitoring_config_api/#job_name","text":"Name of the feature monitoring job. [source]","title":"job_name"},{"location":"generated/api/feature_monitoring_config_api/#job_schedule","text":"Schedule of the feature monitoring job. This field belongs to the job configuration but is made transparent to the user for convenience. [source]","title":"job_schedule"},{"location":"generated/api/feature_monitoring_config_api/#name","text":"The name of the feature monitoring config. A Feature Group or Feature View cannot have multiple feature monitoring configurations with the same name. The name of a feature monitoring configuration is limited to 63 characters. This property is read-only once the feature monitoring configuration has been saved. [source]","title":"name"},{"location":"generated/api/feature_monitoring_config_api/#reference_window_config","text":"Configuration for the reference window. [source]","title":"reference_window_config"},{"location":"generated/api/feature_monitoring_config_api/#statistics_comparison_config","text":"Configuration for the comparison of detection and reference statistics.","title":"statistics_comparison_config"},{"location":"generated/api/feature_monitoring_config_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_monitoring_config_api/#compare_on","text":"FeatureMonitoringConfig . compare_on ( metric , threshold , strict = False , relative = False ) Sets the statistics comparison criteria for feature monitoring with a reference window. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring, a detection window and a reference window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) . with_reference_window ( ... ) # Choose a metric and set a threshold for the difference # e.g compare the relative mean of detection and reference window my_monitoring_config . compare_on ( metric = \"mean\" , threshold = 1.0 , relative = True , ) . save () Note Detection window and reference window/value/training_dataset must be set prior to comparison configuration. Arguments metric Optional[str] : The metric to use for comparison. Different metric are available for different feature type. threshold Optional[float] : The threshold to apply to the difference to potentially trigger an alert. strict Optional[bool] : Whether to use a strict comparison (e.g. > or <) or a non-strict comparison (e.g. >= or <=). relative Optional[bool] : Whether to use a relative comparison (e.g. relative mean) or an absolute comparison (e.g. absolute mean). Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source]","title":"compare_on"},{"location":"generated/api/feature_monitoring_config_api/#delete","text":"FeatureMonitoringConfig . delete () Deletes the feature monitoring configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Delete the feature monitoring config my_monitoring_config . delete () Raises FeatureStoreException : If the feature monitoring config has not been saved. [source]","title":"delete"},{"location":"generated/api/feature_monitoring_config_api/#disable","text":"FeatureMonitoringConfig . disable () Disables the schedule of the feature monitoring job. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Disable the feature monitoring config my_monitoring_config . disable () Raises FeatureStoreException : If the feature monitoring config has not been saved. [source]","title":"disable"},{"location":"generated/api/feature_monitoring_config_api/#enable","text":"FeatureMonitoringConfig . enable () Enables the schedule of the feature monitoring job. The scheduler can be configured via the job_schedule property. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Enable the feature monitoring config my_monitoring_config . enable () Raises FeatureStoreException : If the feature monitoring config has not been saved. [source]","title":"enable"},{"location":"generated/api/feature_monitoring_config_api/#get_history","text":"FeatureMonitoringConfig . get_history ( start_time = None , end_time = None , with_statistics = True ) Fetch the history of the computed statistics and comparison results for this configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Fetch the history of the computed statistics for this configuration history = my_monitoring_config . get_history ( start_time = \"2021-01-01\" , end_time = \"2021-01-31\" , ) Args: start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results. Raises FeatureStoreException : If the feature monitoring config has not been saved. [source]","title":"get_history"},{"location":"generated/api/feature_monitoring_config_api/#get_job","text":"FeatureMonitoringConfig . get_job () Get the feature monitoring job which computes and compares statistics on the detection and reference windows. Example # Fetch registered config by name via feature group or feature view my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Get the job which computes statistics on detection and reference window job = my_monitoring_config . get_job () # Print job history and ongoing executions job . executions Raises FeatureStoreException : If the feature monitoring config has not been saved. Returns Job . A handle for the job computing the statistics. [source]","title":"get_job"},{"location":"generated/api/feature_monitoring_config_api/#run_job","text":"FeatureMonitoringConfig . run_job () Trigger the feature monitoring job which computes and compares statistics on the detection and reference windows. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Trigger the feature monitoring job once my_monitoring_config . run_job () Info The feature monitoring job will be triggered asynchronously and the method will return immediately. Calling this method does not affect the ongoing schedule. Raises FeatureStoreException : If the feature monitoring config has not been saved. Returns Job . A handle for the job computing the statistics. [source]","title":"run_job"},{"location":"generated/api/feature_monitoring_config_api/#save","text":"FeatureMonitoringConfig . save () Saves the feature monitoring configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_statistics_monitoring ( name = \"my_monitoring_config\" , ) . save () Returns FeatureMonitoringConfig . The saved FeatureMonitoringConfig object. [source]","title":"save"},{"location":"generated/api/feature_monitoring_config_api/#update","text":"FeatureMonitoringConfig . update () Updates allowed fields of the saved feature monitoring configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Update the percentage of rows to use when computing the statistics my_monitoring_config . detection_window . row_percentage = 10 my_monitoring_config . update () Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source]","title":"update"},{"location":"generated/api/feature_monitoring_config_api/#with_detection_window","text":"FeatureMonitoringConfig . with_detection_window ( time_offset = None , window_length = None , row_percentage = None ) Sets the detection window of data to compute statistics on. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Compute statistics on a regular basis fg . create_statistics_monitoring ( name = \"regular_stats\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( time_offset = \"1d\" , window_length = \"1d\" , row_percentage = 0.1 , ) . save () # Compute and compare statistics fg . create_feature_monitoring ( name = \"regular_stats\" , feature_name = \"my_feature\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( time_offset = \"1d\" , window_length = \"1d\" , row_percentage = 0.1 , ) . with_reference_window ( ... ) . compare_on ( ... ) . save () Arguments time_offset Optional[str] : The time offset from the current time to the start of the time window. window_length Optional[str] : The length of the time window. row_percentage Optional[float] : The fraction of rows to use when computing the statistics [0, 1.0]. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source]","title":"with_detection_window"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_training_dataset","text":"FeatureMonitoringConfig . with_reference_training_dataset ( training_dataset_version = None ) Sets the reference training dataset to compare statistics with. See also with_reference_value(...) and with_reference_window(...) for other reference options. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) # Only for feature views: Compare to the statistics computed for one of your training datasets # particularly useful if it has been used to train a model currently in production my_monitoring_config . with_reference_training_dataset ( training_dataset_version = 3 , ) . compare_on ( ... ) . save () Provide a comparison configuration You must provide a comparison configuration via compare_on() before saving the feature monitoring config. Arguments training_dataset_version Optional[int] : The version of the training dataset to use as reference. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source]","title":"with_reference_training_dataset"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_value","text":"FeatureMonitoringConfig . with_reference_value ( value = None ) Sets the reference value to compare statistics with. See also with_reference_window(...) and with_reference_training_dataset(...) for other reference options. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) # Simplest reference window is a specific value my_monitoring_config . with_reference_value ( value = 0.0 , ) . compare_on ( ... ) . save () Provide a comparison configuration You must provide a comparison configuration via compare_on() before saving the feature monitoring config. Arguments value Optional[Union[int, float]] : A float value to use as reference. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object. [source]","title":"with_reference_value"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_window","text":"FeatureMonitoringConfig . with_reference_window ( time_offset = None , window_length = None , row_percentage = None ) Sets the reference window of data to compute statistics on. See also with_reference_value(...) and with_reference_training_dataset(...) for other reference options. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Setup feature monitoring and a detection window my_monitoring_config = fg . create_feature_monitoring ( ... ) . with_detection_window ( ... ) # Statistics computed on a rolling time window, e.g. same day last week my_monitoring_config . with_reference_window ( time_offset = \"1w\" , window_length = \"1d\" , ) . compare_on ( ... ) . save () Provide a comparison configuration You must provide a comparison configuration via compare_on() before saving the feature monitoring config. Arguments time_offset Optional[str] : The time offset from the current time to the start of the time window. window_length Optional[str] : The length of the time window. row_percentage Optional[float] : The percentage of rows to use when computing the statistics. Defaults to 20%. Returns FeatureMonitoringConfig . The updated FeatureMonitoringConfig object.","title":"with_reference_window"},{"location":"generated/api/feature_monitoring_result_api/","text":"Feature Monitoring Result # [source] FeatureMonitoringResult # hsfs . core . feature_monitoring_result . FeatureMonitoringResult ( feature_store_id , execution_id , monitoring_time , config_id , feature_name , difference = None , shift_detected = False , detection_statistics_id = None , reference_statistics_id = None , empty_detection_window = False , empty_reference_window = False , specific_value = None , raised_exception = False , detection_statistics = None , reference_statistics = None , id = None , href = None , ** kwargs ) Retrieval # [source] get_history # FeatureMonitoringConfig . get_history ( start_time = None , end_time = None , with_statistics = True ) Fetch the history of the computed statistics and comparison results for this configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Fetch the history of the computed statistics for this configuration history = my_monitoring_config . get_history ( start_time = \"2021-01-01\" , end_time = \"2021-01-31\" , ) Args: start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results. Raises FeatureStoreException : If the feature monitoring config has not been saved. Properties # [source] config_id # Id of the feature monitoring configuration containing this result. [source] detection_statistics # Feature descriptive statistics computed on the detection window. [source] detection_statistics_id # Id of the feature descriptive statistics computed on the detection window. [source] difference # Difference between detection and reference values. It can be relative or absolute difference, depending on the statistics comparison configuration provided in relative parameter passed to compare_on() when enabling feature monitoring. [source] empty_detection_window # Whether or not the detection window was empty in this feature monitoring run. [source] empty_reference_window # Whether or not the reference window was empty in this feature monitoring run. [source] execution_id # Execution id of the feature monitoring job. [source] feature_name # Name of the feature being monitored. [source] feature_store_id # Id of the Feature Store. [source] id # Id of the feature monitoring result. [source] monitoring_time # Time at which this feature monitoring result was created. [source] reference_statistics # Feature descriptive statistics computed on the reference window. [source] reference_statistics_id # Id of the feature descriptive statistics computed on the reference window. [source] shift_detected # Whether or not shift was detected in the detection window based on the computed statistics and the threshold provided in compare_on() when enabling feature monitoring. [source] specific_value # Specific value used as reference in the statistics comparison.","title":"Result"},{"location":"generated/api/feature_monitoring_result_api/#feature-monitoring-result","text":"[source]","title":"Feature Monitoring Result"},{"location":"generated/api/feature_monitoring_result_api/#featuremonitoringresult","text":"hsfs . core . feature_monitoring_result . FeatureMonitoringResult ( feature_store_id , execution_id , monitoring_time , config_id , feature_name , difference = None , shift_detected = False , detection_statistics_id = None , reference_statistics_id = None , empty_detection_window = False , empty_reference_window = False , specific_value = None , raised_exception = False , detection_statistics = None , reference_statistics = None , id = None , href = None , ** kwargs )","title":"FeatureMonitoringResult"},{"location":"generated/api/feature_monitoring_result_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_monitoring_result_api/#get_history","text":"FeatureMonitoringConfig . get_history ( start_time = None , end_time = None , with_statistics = True ) Fetch the history of the computed statistics and comparison results for this configuration. Example # Fetch your feature group or feature view fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # Fetch registered config by name my_monitoring_config = fg . get_feature_monitoring_configs ( name = \"my_monitoring_config\" ) # Fetch the history of the computed statistics for this configuration history = my_monitoring_config . get_history ( start_time = \"2021-01-01\" , end_time = \"2021-01-31\" , ) Args: start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results. Raises FeatureStoreException : If the feature monitoring config has not been saved.","title":"get_history"},{"location":"generated/api/feature_monitoring_result_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_monitoring_result_api/#config_id","text":"Id of the feature monitoring configuration containing this result. [source]","title":"config_id"},{"location":"generated/api/feature_monitoring_result_api/#detection_statistics","text":"Feature descriptive statistics computed on the detection window. [source]","title":"detection_statistics"},{"location":"generated/api/feature_monitoring_result_api/#detection_statistics_id","text":"Id of the feature descriptive statistics computed on the detection window. [source]","title":"detection_statistics_id"},{"location":"generated/api/feature_monitoring_result_api/#difference","text":"Difference between detection and reference values. It can be relative or absolute difference, depending on the statistics comparison configuration provided in relative parameter passed to compare_on() when enabling feature monitoring. [source]","title":"difference"},{"location":"generated/api/feature_monitoring_result_api/#empty_detection_window","text":"Whether or not the detection window was empty in this feature monitoring run. [source]","title":"empty_detection_window"},{"location":"generated/api/feature_monitoring_result_api/#empty_reference_window","text":"Whether or not the reference window was empty in this feature monitoring run. [source]","title":"empty_reference_window"},{"location":"generated/api/feature_monitoring_result_api/#execution_id","text":"Execution id of the feature monitoring job. [source]","title":"execution_id"},{"location":"generated/api/feature_monitoring_result_api/#feature_name","text":"Name of the feature being monitored. [source]","title":"feature_name"},{"location":"generated/api/feature_monitoring_result_api/#feature_store_id","text":"Id of the Feature Store. [source]","title":"feature_store_id"},{"location":"generated/api/feature_monitoring_result_api/#id","text":"Id of the feature monitoring result. [source]","title":"id"},{"location":"generated/api/feature_monitoring_result_api/#monitoring_time","text":"Time at which this feature monitoring result was created. [source]","title":"monitoring_time"},{"location":"generated/api/feature_monitoring_result_api/#reference_statistics","text":"Feature descriptive statistics computed on the reference window. [source]","title":"reference_statistics"},{"location":"generated/api/feature_monitoring_result_api/#reference_statistics_id","text":"Id of the feature descriptive statistics computed on the reference window. [source]","title":"reference_statistics_id"},{"location":"generated/api/feature_monitoring_result_api/#shift_detected","text":"Whether or not shift was detected in the detection window based on the computed statistics and the threshold provided in compare_on() when enabling feature monitoring. [source]","title":"shift_detected"},{"location":"generated/api/feature_monitoring_result_api/#specific_value","text":"Specific value used as reference in the statistics comparison.","title":"specific_value"},{"location":"generated/api/feature_monitoring_window_config_api/","text":"Feature Monitoring Window Configuration # [source] MonitoringWindowConfig # hsfs . core . monitoring_window_config . MonitoringWindowConfig ( id = None , window_config_type = SPECIFIC_VALUE , time_offset = None , window_length = None , training_dataset_version = None , specific_value = None , row_percentage = None , ** kwargs ) Properties # [source] id # Id of the window configuration. [source] row_percentage # The percentage of rows to fetch and compute the statistics on. Only used for windows of type ROLLING_TIME and ALL_TIME . [source] specific_value # The specific value to use as reference. Only used for windows of type SPECIFIC_VALUE . [source] time_offset # The time offset from the current time to the start of the time window. Only used for windows of type ROLLING_TIME . [source] training_dataset_version # The version of the training dataset to use as reference. Only used for windows of type TRAINING_DATASET . [source] window_config_type # Type of the window. It can be one of ALL_TIME , ROLLING_TIME , TRAINING_DATASET or SPECIFIC_VALUE . [source] window_length # The length of the time window. Only used for windows of type ROLLING_TIME .","title":"Window"},{"location":"generated/api/feature_monitoring_window_config_api/#feature-monitoring-window-configuration","text":"[source]","title":"Feature Monitoring Window Configuration"},{"location":"generated/api/feature_monitoring_window_config_api/#monitoringwindowconfig","text":"hsfs . core . monitoring_window_config . MonitoringWindowConfig ( id = None , window_config_type = SPECIFIC_VALUE , time_offset = None , window_length = None , training_dataset_version = None , specific_value = None , row_percentage = None , ** kwargs )","title":"MonitoringWindowConfig"},{"location":"generated/api/feature_monitoring_window_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_monitoring_window_config_api/#id","text":"Id of the window configuration. [source]","title":"id"},{"location":"generated/api/feature_monitoring_window_config_api/#row_percentage","text":"The percentage of rows to fetch and compute the statistics on. Only used for windows of type ROLLING_TIME and ALL_TIME . [source]","title":"row_percentage"},{"location":"generated/api/feature_monitoring_window_config_api/#specific_value","text":"The specific value to use as reference. Only used for windows of type SPECIFIC_VALUE . [source]","title":"specific_value"},{"location":"generated/api/feature_monitoring_window_config_api/#time_offset","text":"The time offset from the current time to the start of the time window. Only used for windows of type ROLLING_TIME . [source]","title":"time_offset"},{"location":"generated/api/feature_monitoring_window_config_api/#training_dataset_version","text":"The version of the training dataset to use as reference. Only used for windows of type TRAINING_DATASET . [source]","title":"training_dataset_version"},{"location":"generated/api/feature_monitoring_window_config_api/#window_config_type","text":"Type of the window. It can be one of ALL_TIME , ROLLING_TIME , TRAINING_DATASET or SPECIFIC_VALUE . [source]","title":"window_config_type"},{"location":"generated/api/feature_monitoring_window_config_api/#window_length","text":"The length of the time window. Only used for windows of type ROLLING_TIME .","title":"window_length"},{"location":"generated/api/feature_store_api/","text":"Feature Store # [source] FeatureStore # hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , project_name , project_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , num_feature_views = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , ** kwargs ) Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. How to get feature store instance import hsfs conn = hsfs . connection () fs = conn . get_feature_store () # or import hopsworks project = hopsworks . login () fs = project . get_feature_store () Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_external_feature_group # FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , topic_name = None , notification_topic_name = None , ) Create a external feature group metadata object. Example # connect to the Feature Store fs = ... external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually: external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , online_enabled = True ) external_fg . save () # read from external storage and filter data to sync to online df = external_fg . read () . filter ( external_fg . customer_status == \"active\" ) # insert to online storage external_fg . insert ( df ) Arguments name str : Name of the external feature group to create. storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group options Optional[Dict[str, str]] : Additional options to be used by the engine when reading data from the specified storage connector. For example, {\"header\": True} when reading CSV files with column names in the first row. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . expectation_suite : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . online_enabled Optional[bool] : Define whether it should be possible to sync the feature group to the online feature store for low latency access, defaults to False . topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns ExternalFeatureGroup . The external feature group metadata object. [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , parents = [], topic_name = None , notification_topic_name = None , ) Create a feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . create_feature_group ( name = 'air_quality' , description = 'Air Quality characteristics of each day' , version = 1 , primary_key = [ 'city' , 'date' ], online_enabled = True , event_time = 'date' ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, vector database is used as online feature store. This enables similarity search by using find_neighbors . default to None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. [source] create_feature_view # FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, ) Create a feature view metadata object and saved it to hopsworks. Example # connect to the Feature Store fs = ... # get the feature group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # get the transformation functions standard_scaler = fs . get_transformation_function ( name = 'standard_scaler' ) # construct dictionary of \"feature - transformation function\" pairs transformation_functions = { col_name : standard_scaler for col_name in df . columns } feature_view = fs . create_feature_view ( name = 'air_quality_fv' , version = 1 , transformation_functions = transformation_functions , query = query ) Example # get feature store instance fs = ... # define query object query = ... # define dictionary with column names and transformation functions pairs mapping_transformers = ... # create feature view feature_view = fs . create_feature_view ( name = 'feature_view_name' , version = 1 , transformation_functions = mapping_transformers , query = query ) Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. inference_helper_columns Optional[List[str]] : A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the Query object. If inference helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to be prepended to the original column name when defining inference_helper_columns list. When replaying a Query during model inference, the inference helper columns optionally can be omitted during batch ( get_batch_data ) and will be omitted during online inference ( get_feature_vector(s) ). To get inference helper column(s) during online inference use get_inference_helper(s) method. Defaults to `[], no helper columns. training_helper_columns Optional[List[str]] : A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the Query object. If training helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to prepended to the original column name when defining training_helper_columns list. When replaying a Query during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to `[], no training helper columns. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , topic_name = None , notification_topic_name = None , ) Create a external feature group metadata object. Deprecated create_on_demand_feature_group method is deprecated. Use the create_external_feature_group method instead. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group options Optional[Dict[str, str]] : Additional options to be used by the engine when reading data from the specified storage connector. For example, {\"header\": True} when reading CSV files with column names in the first row. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . expectation_suite : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source] from_response_json # FeatureStore . from_response_json ( json_dict ) [source] get_external_feature_group # FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... external_fg = fs . get_external_feature_group ( \"external_fg_test\" ) Arguments name str : Name of the external feature group to get. version int : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_external_feature_groups # FeatureStore . get_external_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... external_fgs_list = fs . get_external_feature_groups ( \"external_fg_test\" ) Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... fg = fs . get_feature_group ( name = \"electricity_prices\" , version = 1 , ) Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... fgs_list = fs . get_feature_groups ( name = \"electricity_prices\" ) Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_view # FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( name = 'feature_view_name' , version = 1 ) Arguments name str : Name of the feature view to get. version int : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. [source] get_feature_views # FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get a list of all versions of a feature view feature_view = fs . get_feature_views ( name = 'feature_view_name' ) Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Deprecated get_on_demand_feature_group method is deprecated. Use the get_external_feature_group method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version int : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_groups # FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Deprecated get_on_demand_feature_groups method is deprecated. Use the get_external_feature_groups method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Example # connect to the Feature Store fs = ... online_storage_connector = fs . get_online_storage_connector () Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_or_create_feature_group # FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , parents = [], topic_name = None , notification_topic_name = None , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = \"electricity_prices\" , version = 1 , description = \"Electricity prices from NORD POOL\" , primary_key = [ \"day\" , \"area\" ], online_enabled = True , event_time = \"timestamp\" , ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, the vector database is used as online feature store. This enables similarity search by using find_neighbors . default is None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. [source] get_or_create_feature_view # FeatureStore . get_or_create_feature_view ( name , query , version , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, ) Get feature view metadata object or create a new one if it doesn't exist. This method doesn't update existing feature view metadata object. Example # connect to the Feature Store fs = ... feature_view = fs . get_or_create_feature_view ( name = 'bitcoin_feature_view' , version = 1 , transformation_functions = transformation_functions , query = query ) Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version int : Version of the feature view to create. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. inference_helper_columns Optional[List[str]] : A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the Query object. If inference helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to be prepended to the original column name when defining inference_helper_columns list. When replaying a Query during model inference, the inference helper columns optionally can be omitted during batch ( get_batch_data ) and will be omitted during online inference ( get_feature_vector(s) ). To get inference helper column(s) during online inference use get_inference_helper(s) method. Defaults to `[], no helper columns. training_helper_columns Optional[List[str]] : A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the Query object. If training helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to prepended to the original column name when defining training_helper_columns list. When replaying a Query during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to `[], no training helper columns. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source] get_or_create_spine_group # FeatureStore . get_or_create_spine_group ( name , version = None , description = \"\" , primary_key = [], event_time = None , features = [], dataframe = None ) Create a spine group metadata object. Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins. Example # connect to the Feature Store fs = ... spine_df = pd . Dataframe () spine_group = fs . get_or_create_spine_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , dataframe = spine_df ) Note that you can inspect the dataframe in the spine group, or replace the dataframe: spine_group . dataframe . show () spine_group . dataframe = new_df The spine can then be used to construct queries, with only one speciality: Note Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against. If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving. These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again. For example, to generate training data: X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = training_data_entities ) Or to get batches of fresh data for batch scoring: feature_view_spine . get_batch_data ( spine = scoring_entities_df ) . show () Here you have the chance to pass a different set of entities to generate the training dataset. Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column. feature_view . get_batch_data ( spine = spine_group ) Arguments name str : Name of the spine group to create. version Optional[int] : Version of the spine group to retrieve, defaults to None and will create the spine group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the spine group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the spine group won't have any primary key. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins. Defaults to None . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the spine group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . dataframe : DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Returns SpineGroup . The spine group metadata object. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Example # connect to the Feature Store fs = ... sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Get transformation function by name. This will default to version 1 # get feature store instance fs = ... # get transformation function metadata object plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) Get built-in transformation function min max scaler # get feature store instance fs = ... # get transformation function metadata object min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) Get transformation function by name and version # get feature store instance fs = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 2 ) You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s). Attach transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 1 ) # attach transformation functions feature_view = fs . create_feature_view ( name = 'feature_view_name' , query = query , labels = [ \"target_column\" ], transformation_functions = { \"column_to_transform\" : min_max_scaler } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Attach built-in transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # retrieve transformation functions min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) # attach built-in transformation functions while creating feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category_column\" : label_encoder , \"weight\" : robust_scaler , \"age\" : min_max_scaler , \"salary\" : standard_scaler } ) Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved. Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Get all transformation functions # get feature store instance fs = ... # get all transformation functions list_transformation_fns = fs . get_transformation_functions () Returns: List[TransformationFunction] . List of transformation function instances. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Example # connect to the Feature Store fs = ... # construct the query and show head rows query_res_head = fs . sql ( \"SELECT * FROM `fg_1`\" ) . head () Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type.","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#feature-store","text":"[source]","title":"Feature Store"},{"location":"generated/api/feature_store_api/#featurestore","text":"hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , project_name , project_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , num_feature_views = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , ** kwargs )","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_store_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. How to get feature store instance import hsfs conn = hsfs . connection () fs = conn . get_feature_store () # or import hopsworks project = hopsworks . login () fs = project . get_feature_store () Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/api/feature_store_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_store_api/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/api/feature_store_api/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/api/feature_store_api/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/api/feature_store_api/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/api/feature_store_api/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/api/feature_store_api/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/api/feature_store_api/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/api/feature_store_api/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/api/feature_store_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_store_api/#create_external_feature_group","text":"FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , topic_name = None , notification_topic_name = None , ) Create a external feature group metadata object. Example # connect to the Feature Store fs = ... external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually: external_fg = fs . create_external_feature_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , online_enabled = True ) external_fg . save () # read from external storage and filter data to sync to online df = external_fg . read () . filter ( external_fg . customer_status == \"active\" ) # insert to online storage external_fg . insert ( df ) Arguments name str : Name of the external feature group to create. storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group options Optional[Dict[str, str]] : Additional options to be used by the engine when reading data from the specified storage connector. For example, {\"header\": True} when reading CSV files with column names in the first row. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . expectation_suite : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . online_enabled Optional[bool] : Define whether it should be possible to sync the feature group to the online feature store for low latency access, defaults to False . topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns ExternalFeatureGroup . The external feature group metadata object. [source]","title":"create_external_feature_group"},{"location":"generated/api/feature_store_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , parents = [], topic_name = None , notification_topic_name = None , ) Create a feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . create_feature_group ( name = 'air_quality' , description = 'Air Quality characteristics of each day' , version = 1 , primary_key = [ 'city' , 'date' ], online_enabled = True , event_time = 'date' ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, vector database is used as online feature store. This enables similarity search by using find_neighbors . default to None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_store_api/#create_feature_view","text":"FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, ) Create a feature view metadata object and saved it to hopsworks. Example # connect to the Feature Store fs = ... # get the feature group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # get the transformation functions standard_scaler = fs . get_transformation_function ( name = 'standard_scaler' ) # construct dictionary of \"feature - transformation function\" pairs transformation_functions = { col_name : standard_scaler for col_name in df . columns } feature_view = fs . create_feature_view ( name = 'air_quality_fv' , version = 1 , transformation_functions = transformation_functions , query = query ) Example # get feature store instance fs = ... # define query object query = ... # define dictionary with column names and transformation functions pairs mapping_transformers = ... # create feature view feature_view = fs . create_feature_view ( name = 'feature_view_name' , version = 1 , transformation_functions = mapping_transformers , query = query ) Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. inference_helper_columns Optional[List[str]] : A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the Query object. If inference helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to be prepended to the original column name when defining inference_helper_columns list. When replaying a Query during model inference, the inference helper columns optionally can be omitted during batch ( get_batch_data ) and will be omitted during online inference ( get_feature_vector(s) ). To get inference helper column(s) during online inference use get_inference_helper(s) method. Defaults to `[], no helper columns. training_helper_columns Optional[List[str]] : A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the Query object. If training helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to prepended to the original column name when defining training_helper_columns list. When replaying a Query during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to `[], no training helper columns. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source]","title":"create_feature_view"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , topic_name = None , notification_topic_name = None , ) Create a external feature group metadata object. Deprecated create_on_demand_feature_group method is deprecated. Use the create_external_feature_group method instead. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group options Optional[Dict[str, str]] : Additional options to be used by the engine when reading data from the specified storage connector. For example, {\"header\": True} when reading CSV files with column names in the first row. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . expectation_suite : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/api/feature_store_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"create_transformation_function"},{"location":"generated/api/feature_store_api/#from_response_json","text":"FeatureStore . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_store_api/#get_external_feature_group","text":"FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... external_fg = fs . get_external_feature_group ( \"external_fg_test\" ) Arguments name str : Name of the external feature group to get. version int : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_external_feature_group"},{"location":"generated/api/feature_store_api/#get_external_feature_groups","text":"FeatureStore . get_external_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... external_fgs_list = fs . get_external_feature_groups ( \"external_fg_test\" ) Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_external_feature_groups"},{"location":"generated/api/feature_store_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... fg = fs . get_feature_group ( name = \"electricity_prices\" , version = 1 , ) Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/api/feature_store_api/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Example # connect to the Feature Store fs = ... fgs_list = fs . get_feature_groups ( name = \"electricity_prices\" ) Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/api/feature_store_api/#get_feature_view","text":"FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( name = 'feature_view_name' , version = 1 ) Arguments name str : Name of the feature view to get. version int : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_view"},{"location":"generated/api/feature_store_api/#get_feature_views","text":"FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get a list of all versions of a feature view feature_view = fs . get_feature_views ( name = 'feature_view_name' ) Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_views"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Deprecated get_on_demand_feature_group method is deprecated. Use the get_external_feature_group method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version int : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_groups","text":"FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Deprecated get_on_demand_feature_groups method is deprecated. Use the get_external_feature_groups method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_groups"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Example # connect to the Feature Store fs = ... online_storage_connector = fs . get_online_storage_connector () Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/api/feature_store_api/#get_or_create_feature_group","text":"FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], embedding_index = None , hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , parents = [], topic_name = None , notification_topic_name = None , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Example # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = \"electricity_prices\" , version = 1 , description = \"Electricity prices from NORD POOL\" , primary_key = [ \"day\" , \"area\" ], online_enabled = True , event_time = \"timestamp\" , ) Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. embedding_index Optional[hsfs.embedding.EmbeddingIndex] : EmbeddingIndex . If an embedding index is provided, the vector database is used as online feature store. This enables similarity search by using find_neighbors . default is None hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . stream : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. parents Optional[List[hsfs.feature_group.FeatureGroup]] : Optionally, Define the parents of this feature group as the origin where the data is coming from. topic_name Optional[str] : Optionally, define the name of the topic used for data ingestion. If left undefined it defaults to using project topic. notification_topic_name Optional[str] : Optionally, define the name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If left undefined no notifications are sent. Returns FeatureGroup . The feature group metadata object. [source]","title":"get_or_create_feature_group"},{"location":"generated/api/feature_store_api/#get_or_create_feature_view","text":"FeatureStore . get_or_create_feature_view ( name , query , version , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, ) Get feature view metadata object or create a new one if it doesn't exist. This method doesn't update existing feature view metadata object. Example # connect to the Feature Store fs = ... feature_view = fs . get_or_create_feature_view ( name = 'bitcoin_feature_view' , version = 1 , transformation_functions = transformation_functions , query = query ) Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version int : Version of the feature view to create. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. inference_helper_columns Optional[List[str]] : A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the Query object. If inference helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to be prepended to the original column name when defining inference_helper_columns list. When replaying a Query during model inference, the inference helper columns optionally can be omitted during batch ( get_batch_data ) and will be omitted during online inference ( get_feature_vector(s) ). To get inference helper column(s) during online inference use get_inference_helper(s) method. Defaults to `[], no helper columns. training_helper_columns Optional[List[str]] : A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the Query object. If training helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to prepended to the original column name when defining training_helper_columns list. When replaying a Query during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to `[], no training helper columns. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source]","title":"get_or_create_feature_view"},{"location":"generated/api/feature_store_api/#get_or_create_spine_group","text":"FeatureStore . get_or_create_spine_group ( name , version = None , description = \"\" , primary_key = [], event_time = None , features = [], dataframe = None ) Create a spine group metadata object. Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins. Example # connect to the Feature Store fs = ... spine_df = pd . Dataframe () spine_group = fs . get_or_create_spine_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , dataframe = spine_df ) Note that you can inspect the dataframe in the spine group, or replace the dataframe: spine_group . dataframe . show () spine_group . dataframe = new_df The spine can then be used to construct queries, with only one speciality: Note Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against. If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving. These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again. For example, to generate training data: X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = training_data_entities ) Or to get batches of fresh data for batch scoring: feature_view_spine . get_batch_data ( spine = scoring_entities_df ) . show () Here you have the chance to pass a different set of entities to generate the training dataset. Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column. feature_view . get_batch_data ( spine = spine_group ) Arguments name str : Name of the spine group to create. version Optional[int] : Version of the spine group to retrieve, defaults to None and will create the spine group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the spine group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the spine group won't have any primary key. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins. Defaults to None . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the spine group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . dataframe : DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Returns SpineGroup . The spine group metadata object. [source]","title":"get_or_create_spine_group"},{"location":"generated/api/feature_store_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Example # connect to the Feature Store fs = ... sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/feature_store_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/api/feature_store_api/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_datasets"},{"location":"generated/api/feature_store_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Get transformation function by name. This will default to version 1 # get feature store instance fs = ... # get transformation function metadata object plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) Get built-in transformation function min max scaler # get feature store instance fs = ... # get transformation function metadata object min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) Get transformation function by name and version # get feature store instance fs = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 2 ) You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s). Attach transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 1 ) # attach transformation functions feature_view = fs . create_feature_view ( name = 'feature_view_name' , query = query , labels = [ \"target_column\" ], transformation_functions = { \"column_to_transform\" : min_max_scaler } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Attach built-in transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # retrieve transformation functions min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) # attach built-in transformation functions while creating feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category_column\" : label_encoder , \"weight\" : robust_scaler , \"age\" : min_max_scaler , \"salary\" : standard_scaler } ) Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/feature_store_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Get all transformation functions # get feature store instance fs = ... # get all transformation functions list_transformation_fns = fs . get_transformation_functions () Returns: List[TransformationFunction] . List of transformation function instances. [source]","title":"get_transformation_functions"},{"location":"generated/api/feature_store_api/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Example # connect to the Feature Store fs = ... # construct the query and show head rows query_res_head = fs . sql ( \"SELECT * FROM `fg_1`\" ) . head () Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type.","title":"sql"},{"location":"generated/api/feature_view_api/","text":"Feature View # [source] FeatureView # hsfs . feature_view . FeatureView ( name , query , featurestore_id , id = None , version = None , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, featurestore_name = None , serving_keys = None , ** kwargs ) Creation # [source] create_feature_view # FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, ) Create a feature view metadata object and saved it to hopsworks. Example # connect to the Feature Store fs = ... # get the feature group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # get the transformation functions standard_scaler = fs . get_transformation_function ( name = 'standard_scaler' ) # construct dictionary of \"feature - transformation function\" pairs transformation_functions = { col_name : standard_scaler for col_name in df . columns } feature_view = fs . create_feature_view ( name = 'air_quality_fv' , version = 1 , transformation_functions = transformation_functions , query = query ) Example # get feature store instance fs = ... # define query object query = ... # define dictionary with column names and transformation functions pairs mapping_transformers = ... # create feature view feature_view = fs . create_feature_view ( name = 'feature_view_name' , version = 1 , transformation_functions = mapping_transformers , query = query ) Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. inference_helper_columns Optional[List[str]] : A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the Query object. If inference helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to be prepended to the original column name when defining inference_helper_columns list. When replaying a Query during model inference, the inference helper columns optionally can be omitted during batch ( get_batch_data ) and will be omitted during online inference ( get_feature_vector(s) ). To get inference helper column(s) during online inference use get_inference_helper(s) method. Defaults to `[], no helper columns. training_helper_columns Optional[List[str]] : A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the Query object. If training helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to prepended to the original column name when defining training_helper_columns list. When replaying a Query during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to `[], no training helper columns. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. Retrieval # [source] get_feature_view # FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( name = 'feature_view_name' , version = 1 ) Arguments name str : Name of the feature view to get. version int : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. [source] get_feature_views # FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get a list of all versions of a feature view feature_view = fs . get_feature_views ( name = 'feature_view_name' ) Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. Properties # [source] description # Description of the feature view. [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Feature view schema. (alias) [source] featurestore_id # Feature store id. [source] id # Feature view id. [source] inference_helper_columns # The helper column sof the feature view. Can be a composite of multiple features. [source] labels # The labels/prediction feature of the feature view. Can be a composite of multiple features. [source] name # Name of the feature view. [source] primary_keys # Set of primary key names that is required as keys in input dict object for get_feature_vector(s) method. When there are duplicated primary key names and prefix is not defined in the query, prefix is generated and prepended to the primary key name in this format \"fgId_{feature_group_id}_{join_index}\" where join_index is the order of the join. [source] query # Query of the feature view. [source] schema # Feature view schema. [source] serving_keys # All primary keys of the feature groups included in the query. [source] training_helper_columns # The helper column sof the feature view. Can be a composite of multiple features. [source] transformation_functions # Get transformation functions. [source] version # Version number of the feature view. Methods # [source] add_tag # FeatureView . add_tag ( name , value ) Attach a tag to a feature view. A tag consists of a name and value pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # attach a tag to a feature view feature_view . add_tag ( name = \"tag_schema\" , value = { \"key\" , \"value\" }) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source] add_training_dataset_tag # FeatureView . add_training_dataset_tag ( training_dataset_version , name , value ) Attach a tag to a training dataset. Example # get feature store instance fs = ... # get feature feature view instance feature_view = fs . get_feature_view ( ... ) # attach a tag to a training dataset feature_view . add_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" , value = { \"key\" , \"value\" } ) Arguments training_dataset_version int : training dataset version name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source] clean # FeatureView . clean ( feature_store_id , feature_view_name , feature_view_version ) Delete the feature view and all associated metadata and training data. This can delete corrupted feature view which cannot be retrieved due to a corrupted query for example. Example # delete a feature view and all associated metadata from hsfs.feature_view import FeatureView FeatureView . clean ( feature_store_id = 1 , feature_view_name = 'feature_view_name' , feature_view_version = 1 ) Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Arguments feature_store_id int : int. Id of feature store. feature_view_name str : str. Name of feature view. feature_view_version str : str. Version of feature view. Raises hsfs.client.exceptions.RestAPIError . [source] create_feature_monitoring # FeatureView . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fg = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # compare to a given value specific_value = 0.5 , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_statistics_monitoring # FeatureView . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable statistics monitoring my_config = fv . _create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature view. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_train_test_split # FeatureView . create_train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"parquet\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and save the corresponding training data into location . The training data is split into train and test set at random or according to time ranges. The training data can be retrieved by calling feature_view.get_train_test_split . Create random splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # create a train-test split dataset version , job = feature_view . create_train_test_split ( test_size = 0.2 , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Create time series splits by specifying date as string # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates train_start = \"2022-01-01 00:00:00\" train_end = \"2022-06-06 23:59:59\" test_start = \"2022-06-07 00:00:00\" test_end = \"2022-12-25 23:59:59\" # create a train-test split dataset version , job = feature_view . create_train_test_split ( train_start = train_start , train_end = train_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Create time series splits by specifying date as datetime object # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates from datetime import datetime date_format = \"%Y-%m- %d %H:%M:%S\" train_start = datetime . strptime ( \"2022-01-01 00:00:00\" , date_format ) train_end = datetime . strptime ( \"2022-06-06 23:59:59\" , date_format ) test_start = datetime . strptime ( \"2022-06-07 00:00:00\" , date_format ) test_end = datetime . strptime ( \"2022-12-25 23:59:59\" , date_format ) # create a train-test split dataset version , job = feature_view . create_train_test_split ( train_start = train_start , train_end = train_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Write training dataset to external storage # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get storage connector instance external_storage_connector = fs . get_storage_connector ( \"storage_connector_name\" ) # create a train-test split dataset version , job = feature_view . create_train_test_split ( train_start =... , train_end =... , test_start =... , test_end =... , storage_connector = external_storage_connector , description =... , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format =... ) Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Warning, the following code will fail because category column contains sparse values and training dataset may not have all values available in test split. import pandas as pd df = pd . DataFrame ({ 'category_col' :[ 'category_a' , 'category_b' , 'category_c' , 'category_d' ], 'numeric_col' : [ 40 , 10 , 60 , 40 ] }) feature_group = fs . get_or_create_feature_group ( name = 'feature_group_name' , version = 1 , primary_key = [ 'category_col' ] ) feature_group . insert ( df ) label_encoder = fs . get_transformation_function ( name = 'label_encoder' ) feature_view = fs . create_feature_view ( name = 'feature_view_name' , query = feature_group . select_all (), transformation_functions = { 'category_col' : label_encoder } ) feature_view . create_train_test_split ( test_size = 0.5 ) # Output: KeyError: 'category_c' Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following ormats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . data_format Optional[str] : The data format used to save the training dataset, defaults to \"parquet\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] create_train_validation_test_split # FeatureView . create_train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"parquet\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and save the corresponding training data into location . The training data is split into train, validation, and test set at random or according to time range. The training data can be retrieved by calling feature_view.get_train_validation_test_split . Create random splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 , description = 'Description of a dataset' , data_format = 'csv' ) Create time series splits by specifying date as string # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates train_start = \"2022-01-01 00:00:00\" train_end = \"2022-06-01 23:59:59\" validation_start = \"2022-06-02 00:00:00\" validation_end = \"2022-07-01 23:59:59\" test_start = \"2022-07-02 00:00:00\" test_end = \"2022-08-01 23:59:59\" # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( train_start = train_start , train_end = train_end , validation_start = validation_start , validation_end = validation_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Create time series splits by specifying date as datetime object # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates from datetime import datetime date_format = \"%Y-%m- %d %H:%M:%S\" train_start = datetime . strptime ( \"2022-01-01 00:00:00\" , date_format ) train_end = datetime . strptime ( \"2022-06-06 23:59:59\" , date_format ) validation_start = datetime . strptime ( \"2022-06-02 00:00:00\" , date_format ) validation_end = datetime . strptime ( \"2022-07-01 23:59:59\" , date_format ) test_start = datetime . strptime ( \"2022-06-07 00:00:00\" , date_format ) test_end = datetime . strptime ( \"2022-12-25 23:59:59\" , date_format ) # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( train_start = train_start , train_end = train_end , validation_start = validation_start , validation_end = validation_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Write training dataset to external storage # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get storage connector instance external_storage_connector = fs . get_storage_connector ( \"storage_connector_name\" ) # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( train_start =... , train_end =... , validation_start =... , validation_end =... , test_start =... , test_end =... , description =... , storage_connector = external_storage_connector , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format =... ) Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments validation_size Optional[float] : size of validation set. test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the validation split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the validation split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . data_format Optional[str] : The data format used to save the training dataset, defaults to \"parquet\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] create_training_data # FeatureView . create_training_data ( start_time = \"\" , end_time = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"parquet\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and save the corresponding training data into location . The training data can be retrieved by calling feature_view.get_training_data . Create training dataset # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # create a training dataset version , job = feature_view . create_training_data ( description = 'Description of a dataset' , data_format = 'csv' , # async creation in order not to wait till finish of the job write_options = { \"wait_for_job\" : False } ) Create training data specifying date range with dates as strings # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates start_time = \"2022-01-01 00:00:00\" end_time = \"2022-06-06 23:59:59\" # create a training dataset version , job = feature_view . create_training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) # When we want to read the training data, we need to supply the training data version returned by the create_training_data method: X_train , X_test , y_train , y_test = feature_view . get_training_data ( version ) Create training data specifying date range with dates as datetime objects # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates from datetime import datetime date_format = \"%Y-%m- %d %H:%M:%S\" start_time = datetime . strptime ( \"2022-01-01 00:00:00\" , date_format ) end_time = datetime . strptime ( \"2022-06-06 23:59:59\" , date_format ) # create a training dataset version , job = feature_view . create_training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Write training dataset to external storage # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get storage connector instance external_storage_connector = fs . get_storage_connector ( \"storage_connector_name\" ) # create a train-test split dataset version , job = feature_view . create_training_data ( start_time =... , end_time =... , storage_connector = external_storage_connector , description =... , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format =... ) Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the training dataset query, inclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the training dataset query, exclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . data_format Optional[str] : The data format used to save the training dataset, defaults to \"parquet\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] delete # FeatureView . delete () Delete current feature view, all associated metadata and training data. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete a feature view feature_view . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Raises hsfs.client.exceptions.RestAPIError . [source] delete_all_training_datasets # FeatureView . delete_all_training_datasets () Delete all training datasets. This will delete both metadata and training data. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete all training datasets feature_view . delete_all_training_datasets () Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training datasets. [source] delete_tag # FeatureView . delete_tag ( name ) Delete a tag attached to a feature view. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete a tag feature_view . delete_tag ( 'name_of_tag' ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source] delete_training_dataset # FeatureView . delete_training_dataset ( training_dataset_version ) Delete a training dataset. This will delete both metadata and training data. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete a training dataset feature_view . delete_training_dataset ( training_dataset_version = 1 ) Arguments training_dataset_version int : Version of the training dataset to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training dataset. [source] delete_training_dataset_tag # FeatureView . delete_training_dataset_tag ( training_dataset_version , name ) Delete a tag attached to a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete training dataset tag feature_view . delete_training_dataset_tag ( training_dataset_version = 1 , name = 'name_of_dataset' ) Arguments training_dataset_version int : training dataset version name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source] find_neighbors # FeatureView . find_neighbors ( embedding , feature = None , k = 10 , filter = None , min_score = 0 , external = None ) Finds the nearest neighbors for a given embedding in the vector database. Arguments embedding List[Union[int, float]] : The target embedding for which neighbors are to be found. feature Optional[hsfs.feature.Feature] : The feature used to compute similarity score. Required only if there are multiple embeddings (optional). k Optional[int] : The number of nearest neighbors to retrieve (default is 10). filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : A filter expression to restrict the search space (optional). min_score Optional[float] : The minimum similarity score for neighbors to be considered (default is 0). Returns A list of feature values Example embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=3) fg = fs.create_feature_group( name='air_quality', embedding_index=embedding_index, version=1, primary_key=['id1'], online_enabled=True, ) fg.insert(data) fv = fs.create_feature_view(\"air_quality\", fg.select_all()) fv.find_neighbors( [0.1, 0.2, 0.3], k=5, ) # apply filter fg.find_neighbors( [0.1, 0.2, 0.3], k=5, feature=fg.user_vector, # optional filter=(fg.id1 > 10) & (fg.id1 < 30) ) [source] from_response_json # FeatureView . from_response_json ( json_dict ) [source] get_batch_data # FeatureView . get_batch_data ( start_time = None , end_time = None , read_options = None , spine = None , primary_keys = False , event_time = False , inference_helper_columns = False , ) Get a batch of data from an event time interval from the offline feature store. Batch data for the last 24 hours # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates import datetime start_date = ( datetime . datetime . now () - datetime . timedelta ( hours = 24 )) end_date = ( datetime . datetime . now ()) # get a batch of data df = feature_view . get_batch_data ( start_time = start_date , end_time = end_date ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the batch query, inclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the batch query, exclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. read_options : User provided read options. Dictionary of read options for python engine: key \"use_hive\" and value True to read batch data with Hive instead of ArrowFlight Server . Defaults to {} . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. inference_helper_columns : whether to include inference helper columns or not. Inference helper columns are a list of feature names in the feature view, defined during its creation, that may not be used in training the model itself but can be used during batch or online inference for extra information. If inference helper columns were not defined in the feature view inference_helper_columns=True will not any effect. Defaults to False , no helper columns. Returns DataFrame : A dataframe [source] get_batch_query # FeatureView . get_batch_query ( start_time = None , end_time = None ) Get a query string of the batch query. Batch query for the last 24 hours # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates import datetime start_date = ( datetime . datetime . now () - datetime . timedelta ( hours = 24 )) end_date = ( datetime . datetime . now ()) # get a query string of batch query query_str = feature_view . get_batch_query ( start_time = start_date , end_time = end_date ) # print query string print ( query_str ) Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the batch query, inclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the batch query, exclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. Returns str : batch query [source] get_feature_monitoring_configs # FeatureView . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # fetch all feature monitoring configs attached to the feature view fm_configs = fv . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fv . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fv . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a particular id fm_config = fv . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source] get_feature_monitoring_history # FeatureView . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature view fv = fs . get_feature_view ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fv . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # or use the config id fm_history = fv . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_date : The start date of the feature monitoring history to fetch. Defaults to None. end_date : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source] get_feature_vector # FeatureView . get_feature_vector ( entry , passed_features = None , external = None , return_type = \"list\" , allow_missing = False , force_rest_client = False , force_sql_client = False , ) Returns assembled feature vector from online feature store. Call feature_view.init_serving before this method if the following configurations are needed. 1. The training dataset version of the transformation statistics 2. Additional configurations of online serving engine (e.g init_online_store_rest_client=True to use Online Store REST Client instead of SQL connector) !!! warning \"Missing primary key entries\" If the provided primary key entry can't be found in one or more of the feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting allow_missing to True returns a feature vector with missing values. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled serving vector as a python list feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) # get assembled serving vector as a pandas dataframe feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, return_type = \"pandas\" ) # get assembled serving vector as a numpy array feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, return_type = \"numpy\" ) Get feature vector with user-supplied features # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # the application provides a feature value 'app_attr' app_attr = ... # get a feature vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, passed_features = { \"app_feature\" : app_attr } ) Arguments entry Dict[str, Any] : dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys If the required primary keys is not provided, it will look for name of the primary key in feature group in the entry. passed_features Optional[Dict[str, Any]] : dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"list\" , \"pandas\" or \"numpy\" . Defaults to \"list\" . allow_missing Optional[bool] : Setting to True returns feature vectors with missing values. force_rest_client bool : bool, optional. If set to True, the Online Store REST Client will be used to retrieve the feature vector. Defaults to False. force_sql_client bool : bool, optional. If set to True, the SQL connector will be used to retrieve the feature vector. Defaults to False. Returns list , pd.DataFrame or np.ndarray if return type is set to \"list\" , \"pandas\" or \"numpy\" respectively. Defaults to list . Returned list , pd.DataFrame or np.ndarray contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query. Raises hsfs.client.exceptions.RestAPIError . If using the Online Store REST Client, and the response status code is not 200. - 400: Requested Metadata does not exist or the request is malformed. - 401: Access denied. API key does not give access to the feature store (e.g feature store not shared with user), or authorization header (x-api-key) is not properly set. - 500: Internal server error. ValueError . - A force_* parameter is set to True and the corresponding client is not initialised. - Both force_rest_client and force_sql_client are set to True . - The return_type is not one of \"list\" , \"pandas\" or \"numpy\" . - Training Dataset version is not set and the feature view is not initialised. - Serving keys do not match the provided entry dictionary [source] get_feature_vectors # FeatureView . get_feature_vectors ( entry , passed_features = None , external = None , return_type = \"list\" , allow_missing = False , force_rest_client = False , force_sql_client = False , ) Returns assembled feature vectors in batches from online feature store. Call feature_view.init_serving before this method if the following configurations are needed. 1. The training dataset version of the transformation statistics 2. Additional configurations of online serving engine (e.g init_online_store_rest_client=True to use Online Store REST Client instead of SQL connector) Missing primary key entries If any of the provided primary key elements in entry can't be found in any of the feature groups, no feature vector for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting allow_missing to True returns feature vectors with missing values. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled serving vectors as a python list of lists feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) # get assembled serving vectors as a pandas dataframe feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], return_type = \"pandas\" ) # get assembled serving vectors as a numpy array feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], return_type = \"numpy\" ) Arguments entry List[Dict[str, Any]] : a list of dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys If the required primary keys is not provided, it will look for name of the primary key in feature group in the entry. passed_features Optional[List[Dict[str, Any]]] : a list of dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"list\" , \"pandas\" or \"numpy\" . Defaults to \"list\" . allow_missing Optional[bool] : Setting to True returns feature vectors with missing values. Returns List[list] , pd.DataFrame or np.ndarray if return type is set to \"list\", \"pandas\" or \"numpy\" respectively. Defaults to List[list]`. Returned List[list] , pd.DataFrame or np.ndarray contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query. Raises hsfs.client.exceptions.RestAPIError . If using the Online Store REST client, and the response status code is not 200. - 400: Requested Metadata does not exist or the request is malformed. - 401: Access denied. API key does not give access to the feature store (e.g feature store not shared with user), or authorization header (x-api-key) is not properly set. - 500: Internal server error. ValueError . - A force_* parameter is set to True and the corresponding client is not initialised. - Both force_rest_client and force_sql_client are set to True . - The return_type is not one of \"list\" , \"pandas\" or \"numpy\" . - Training Dataset version is not set and the feature view is not initialised. - Serving keys do not match the provided entry dictionary [source] get_inference_helper # FeatureView . get_inference_helper ( entry , external = None , return_type = \"pandas\" ) Returns assembled inference helper column vectors from online feature store. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled inference helper column vector feature_view . get_inference_helper ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) Arguments entry Dict[str, Any] : dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"pandas\" or \"dict\" . Defaults to \"pandas\" . Returns pd.DataFrame or dict . Defaults to pd.DataFrame . Raises Exception . When primary key entry cannot be found in one or more of the feature groups used by this feature view. [source] get_inference_helpers # FeatureView . get_inference_helpers ( entry , external = None , return_type = \"pandas\" ) Returns assembled inference helper column vectors in batches from online feature store. Missing primary key entries If any of the provided primary key elements in entry can't be found in any of the feature groups, no inference helper column vectors for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled inference helper column vectors feature_view . get_inference_helpers ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) Arguments entry List[Dict[str, Any]] : a list of dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"pandas\" or \"dict\" . Defaults to \"dict\" . Returns pd.DataFrame or List[dict] . Defaults to pd.DataFrame . Returned pd.DataFrame or List[dict] contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query. Raises Exception . When primary key entry cannot be found in one or more of the feature groups used by this feature view. [source] get_parent_feature_groups # FeatureView . get_parent_feature_groups () Get the parents of this feature view, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. [source] get_tag # FeatureView . get_tag ( name ) Get the tags of a feature view. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get a tag of a feature view name = feature_view . get_tag ( 'tag_name' ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureView . get_tags () Returns all tags attached to a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get tags list_tags = feature_view . get_tags () Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source] get_train_test_split # FeatureView . get_train_test_split ( training_dataset_version , read_options = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Get training data created by feature_view.create_train_test_split or feature_view.train_test_split . Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_test , y_train , y_test = feature_view . get_train_test_split ( training_dataset_version = 1 ) Arguments training_dataset_version : training dataset version read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read training dataset with the Hopsworks API instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source] get_train_validation_test_split # FeatureView . get_train_validation_test_split ( training_dataset_version , read_options = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Get training data created by feature_view.create_train_validation_test_split or feature_view.train_validation_test_split . Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_val , X_test , y_train , y_val , y_test = feature_view . get_train_validation_test_splits ( training_dataset_version = 1 ) Arguments training_dataset_version : training dataset version read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read training dataset with the Hopsworks API instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source] get_training_data # FeatureView . get_training_data ( training_dataset_version , read_options = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Get training data created by feature_view.create_training_data or feature_view.training_data . Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data features_df , labels_df = feature_view . get_training_data ( training_dataset_version = 1 ) External Storage Support Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client. Arguments training_dataset_version : training dataset version read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read training dataset with the Hopsworks API instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X, y): Tuple of dataframe of features and labels [source] get_training_dataset_statistics # FeatureView . get_training_dataset_statistics ( training_dataset_version , before_transformation = False , feature_names = None ) Get statistics of a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training dataset statistics statistics = feature_view . get_training_dataset_statistics ( training_dataset_version = 1 ) Arguments training_dataset_version : Training dataset version before_transformation : Whether the statistics were computed before transformation functions or not. feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics [source] get_training_dataset_tag # FeatureView . get_training_dataset_tag ( training_dataset_version , name ) Get the tags of a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get a training dataset tag tag_str = feature_view . get_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) Arguments training_dataset_version int : training dataset version name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source] get_training_dataset_tags # FeatureView . get_training_dataset_tags ( training_dataset_version ) Returns all tags attached to a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get a training dataset tags list_tags = feature_view . get_training_dataset_tags ( training_dataset_version = 1 ) Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source] get_training_datasets # FeatureView . get_training_datasets () Returns the metadata of all training datasets created with this feature view. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get all training dataset metadata list_tds_meta = feature_view . get_training_datasets () Returns List[TrainingDatasetBase] List of training datasets metadata. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the training datasets metadata. [source] init_batch_scoring # FeatureView . init_batch_scoring ( training_dataset_version = None ) Initialise feature view to retrieve feature vector from offline feature store. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # initialise feature view to retrieve feature vector from offline feature store feature_view . init_batch_scoring ( training_dataset_version = 1 ) # get batch data batch_data = feature_view . get_batch_data ( ... ) Arguments training_dataset_version Optional[int] : int, optional. Default to be None. Transformation statistics are fetched from training dataset and applied to the feature vector. [source] init_serving # FeatureView . init_serving ( training_dataset_version = None , external = None , options = None , init_online_store_sql_client = None , init_online_store_rest_client = False , ) Initialise feature view to retrieve feature vector from online and offline feature store. The Online Feature Store now supports feature vector retrieval using either the SQL connector or a REST http client. Defaults to SQL connector to match the previous behaviour. To use the the REST client, set init_online_store_rest_client to True . Both get_feature_vector and get_feature_vectors methods will default to using the initialised client. If both are initialised, the SQL client will be used by default. You can override this behaviour on a per-call basis using the methods kwargs or set the default via set_default_online_store_client method. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # initialise feature view to retrieve a feature vector feature_view . init_serving ( training_dataset_version = 1 ) Initialising the Online Store REST Client to retrieve feature vectors from the online feature store, with additional configuration options. Example # initialise feature view to retrieve a feature vector using the RonDB REST http client feature_view . init_serving ( training_dataset_version = 1 , init_online_store_rest_client = True , ) You can reset the Online Store REST Client connection to fix configuration options. In particular, if you have called get_feature_vector or get_feature_vectors without first initialising the client, it results in a default configuration being set for the rest client. This will reset the client and apply the new configuration options. Example # reset the RonDB REST http client connection feature_view . init_serving ( training_dataset_version = 1 , init_online_store_rest_client = True , options = { \"reset_online_store_rest_client\" : True , \"config_online_store_rest_client\" : { \"host\" : \"new_host\" , \"timeout\" : 1000 }}, ) Note that both the SQL connector and the REST client can be initialised at the same time. This is useful if you want to fallback on one connector if the other fails. Example # initialise feature view to retrieve a feature vector using both the SQL connector and the RonDB REST http client feature_view . init_serving ( training_dataset_version = 1 , init_online_store_sql_client = True , init_online_store_rest_client = True , ) # When initialising both clients, the SQL connector will be used by default. Change the default client using `set_default_online_client`. feature_view . set_default_online_client ( \"rest\" ) Arguments training_dataset_version Optional[int] : int, optional. Default to be 1 for online feature store. Transformation statistics are fetched from training dataset and applied to the feature vector. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. init_online_store_sql_client Optional[bool] : bool, optional. If set to True, initialise the SQL client to retrieve feature vector(s) from the online feature store. Defaults to True if init_online_store_rest_client is False, otherwise False. init_online_store_rest_client bool : bool, optional. If set to True, initialise the Online Store REST Client to retrieve feature vector(s) from the online feature store. Defaults to False, meaning the sql client will be initialised. options Optional[dict] : Additional options as key/value pairs for configuring online serving engine. key: kwargs of SqlAlchemy engine creation (See: https://docs.sqlalchemy.org/en/20/core/engines.html#sqlalchemy.create_engine). For example: {\"pool_size\": 10} key: \"config_online_store_rest_client\" - dict, optional. Optional configuration options to override defaults for the Online Store REST Client. key: \"api_key\" - str. The API key to use for the Online Store REST Client. THIS IS REQUIRED FOR INTERNAL CLIENTS. key: \"host\" - str, optional. The host of the Online Store REST Client. key: \"port\" - int, optional. The port of the Online Store REST Client. key: \"verify_certs\" - bool, optional. If set to True, the Online Store REST Client will verify the server's certificate. key: \"ca_chain\" - str, optional. The path to the CA chain file. key: \"use_ssl\" - bool, optional. If set to True, the Online Store REST Client will use SSL. key: \"timeout\" - int, optional. The timeout of the Online Store REST Client. key: \"server_api_version\" - str, optional. The version of the RonDB Server FeatureStore API. key: \"http_authorization\" - str, optional. The HTTP authorization header to use for the Online Store REST Client. key: \"reset_online_store_rest_client\" - bool, optional. If set to True, the Online Store REST Client will be reset. Provide \"config_online_store_rest_client\" to override defaults. [source] json # FeatureView . json () [source] purge_all_training_data # FeatureView . purge_all_training_data () Delete all training datasets (data only). Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # purge all training data feature_view . purge_all_training_data () Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training datasets. [source] purge_training_data # FeatureView . purge_training_data ( training_dataset_version ) Delete a training dataset (data only). Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # purge training data feature_view . purge_training_data ( training_dataset_version = 1 ) Arguments training_dataset_version int : Version of the training dataset to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training dataset. [source] recreate_training_dataset # FeatureView . recreate_training_dataset ( training_dataset_version , statistics_config = None , write_options = None , spine = None ) Recreate a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # recreate a training dataset that has been deleted feature_view . recreate_training_dataset ( training_dataset_version = 1 ) Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments training_dataset_version int : training dataset version statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] set_default_online_client # FeatureView . set_default_online_client ( client ) Set the default client to either 'sql' or 'rest' to retrieve feature vectors from the online feature store. If only one client is initialised when calling init_serving , this client will be used by default. If both clients are initialised, the SQL client will be used by default. This method allows you to specify the default client. You can override this behaviour on a per-call basis using the methods kwargs. Arguments client str : str. The default online client to be used for the feature view. The default online client can be set to \"rest\" or \"sql\". Raises ValueError . - If vector server is not initialised via init_serving - If setting default to a client not initialised. Use init_serving with either init_online_store_sql_client or init_online_store_rest_client to initialise the client. - If client is not \"rest\" or \"sql\". [source] to_dict # FeatureView . to_dict () [source] train_test_split # FeatureView . train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train and test set at random or according to time ranges. The training data can be recreated by calling feature_view.get_train_test_split with the metadata created. Create random train/test splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_test , y_train , y_test = feature_view . train_test_split ( test_size = 0.2 ) Create time-series train/test splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates train_start = \"2022-05-01 00:00:00\" train_end = \"2022-06-04 23:59:59\" test_start = \"2022-07-01 00:00:00\" test_end = \"2022-08-04 23:59:59\" # you can also pass dates as datetime objects # get training data X_train , X_test , y_train , y_test = feature_view . train_test_split ( train_start = train_start , train_end = train_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, read_options can contain the following entries: key \"use_hive\" and value True to create in-memory training dataset with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source] train_validation_test_split # FeatureView . train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train, validation, and test set at random or according to time ranges. The training data can be recreated by calling feature_view.get_train_validation_test_split with the metadata created. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_val , X_test , y_train , y_val , y_test = feature_view . train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 ) Time Series split # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates start_time_train = '2017-01-01 00:00:01' end_time_train = '2018-02-01 23:59:59' start_time_val = '2018-02-02 23:59:59' end_time_val = '2019-02-01 23:59:59' start_time_test = '2019-02-02 23:59:59' end_time_test = '2020-02-01 23:59:59' # you can also pass dates as datetime objects # get training data X_train , X_val , X_test , y_train , y_val , y_test = feature_view . train_validation_test_split ( train_start = start_time_train , train_end = end_time_train , validation_start = start_time_val , validation_end = end_time_val , test_start = start_time_test , test_end = end_time_test ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments validation_size Optional[float] : size of validation set. Should be between 0 and 1. test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the validation split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the validation split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, read_options can contain the following entries: key \"use_hive\" and value True to create in-memory training dataset with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source] training_data # FeatureView . training_data ( start_time = None , end_time = None , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data can be recreated by calling feature_view.get_training_data with the metadata created. Create random splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data features_df , labels_df = feature_view . training_data ( description = 'Descriprion of a dataset' , ) Create time-series based splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up a date start_time = \"2022-05-01 00:00:00\" end_time = \"2022-06-04 23:59:59\" # you can also pass dates as datetime objects # get training data features_df , labels_df = feature_view . training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the training dataset query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the training dataset query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, read_options can contain the following entries: key \"use_hive\" and value True to create in-memory training dataset with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X, y): Tuple of dataframe of features and labels. If there are no labels, y returns None . [source] update # FeatureView . update () Update the description of the feature view. Update the feature view with a new description. # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) feature_view . description = \"new description\" feature_view . update () # Description is updated in the metadata. Below should return \"new description\". fs . get_feature_view ( \"feature_view_name\" , 1 ) . description Returns FeatureView Updated feature view. Raises hsfs.client.exceptions.RestAPIError . [source] update_from_response_json # FeatureView . update_from_response_json ( json_dict )","title":"FeatureView"},{"location":"generated/api/feature_view_api/#feature-view","text":"[source]","title":"Feature View"},{"location":"generated/api/feature_view_api/#featureview","text":"hsfs . feature_view . FeatureView ( name , query , featurestore_id , id = None , version = None , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, featurestore_name = None , serving_keys = None , ** kwargs )","title":"FeatureView"},{"location":"generated/api/feature_view_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_view_api/#create_feature_view","text":"FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], inference_helper_columns = [], training_helper_columns = [], transformation_functions = {}, ) Create a feature view metadata object and saved it to hopsworks. Example # connect to the Feature Store fs = ... # get the feature group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # get the transformation functions standard_scaler = fs . get_transformation_function ( name = 'standard_scaler' ) # construct dictionary of \"feature - transformation function\" pairs transformation_functions = { col_name : standard_scaler for col_name in df . columns } feature_view = fs . create_feature_view ( name = 'air_quality_fv' , version = 1 , transformation_functions = transformation_functions , query = query ) Example # get feature store instance fs = ... # define query object query = ... # define dictionary with column names and transformation functions pairs mapping_transformers = ... # create feature view feature_view = fs . create_feature_view ( name = 'feature_view_name' , version = 1 , transformation_functions = mapping_transformers , query = query ) Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. inference_helper_columns Optional[List[str]] : A list of feature names that are not used in training the model itself but can be used during batch or online inference for extra information. Inference helper column name(s) must be part of the Query object. If inference helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to be prepended to the original column name when defining inference_helper_columns list. When replaying a Query during model inference, the inference helper columns optionally can be omitted during batch ( get_batch_data ) and will be omitted during online inference ( get_feature_vector(s) ). To get inference helper column(s) during online inference use get_inference_helper(s) method. Defaults to `[], no helper columns. training_helper_columns Optional[List[str]] : A list of feature names that are not the part of the model schema itself but can be used during training as a helper for extra information. Training helper column name(s) must be part of the Query object. If training helper column name(s) belong to feature group that is part of a Join with prefix defined, then this prefix needs to prepended to the original column name when defining training_helper_columns list. When replaying a Query during model inference, the training helper columns will be omitted during both batch and online inference. Training helper columns can be optionally fetched with training data. For more details see documentation for feature view's get training data methods. Defaults to `[], no training helper columns. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object.","title":"create_feature_view"},{"location":"generated/api/feature_view_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_view_api/#get_feature_view","text":"FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( name = 'feature_view_name' , version = 1 ) Arguments name str : Name of the feature view to get. version int : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_view"},{"location":"generated/api/feature_view_api/#get_feature_views","text":"FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Example # get feature store instance fs = ... # get a list of all versions of a feature view feature_view = fs . get_feature_views ( name = 'feature_view_name' ) Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature view from the feature store.","title":"get_feature_views"},{"location":"generated/api/feature_view_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_view_api/#description","text":"Description of the feature view. [source]","title":"description"},{"location":"generated/api/feature_view_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/feature_view_api/#features","text":"Feature view schema. (alias) [source]","title":"features"},{"location":"generated/api/feature_view_api/#featurestore_id","text":"Feature store id. [source]","title":"featurestore_id"},{"location":"generated/api/feature_view_api/#id","text":"Feature view id. [source]","title":"id"},{"location":"generated/api/feature_view_api/#inference_helper_columns","text":"The helper column sof the feature view. Can be a composite of multiple features. [source]","title":"inference_helper_columns"},{"location":"generated/api/feature_view_api/#labels","text":"The labels/prediction feature of the feature view. Can be a composite of multiple features. [source]","title":"labels"},{"location":"generated/api/feature_view_api/#name","text":"Name of the feature view. [source]","title":"name"},{"location":"generated/api/feature_view_api/#primary_keys","text":"Set of primary key names that is required as keys in input dict object for get_feature_vector(s) method. When there are duplicated primary key names and prefix is not defined in the query, prefix is generated and prepended to the primary key name in this format \"fgId_{feature_group_id}_{join_index}\" where join_index is the order of the join. [source]","title":"primary_keys"},{"location":"generated/api/feature_view_api/#query","text":"Query of the feature view. [source]","title":"query"},{"location":"generated/api/feature_view_api/#schema","text":"Feature view schema. [source]","title":"schema"},{"location":"generated/api/feature_view_api/#serving_keys","text":"All primary keys of the feature groups included in the query. [source]","title":"serving_keys"},{"location":"generated/api/feature_view_api/#training_helper_columns","text":"The helper column sof the feature view. Can be a composite of multiple features. [source]","title":"training_helper_columns"},{"location":"generated/api/feature_view_api/#transformation_functions","text":"Get transformation functions. [source]","title":"transformation_functions"},{"location":"generated/api/feature_view_api/#version","text":"Version number of the feature view.","title":"version"},{"location":"generated/api/feature_view_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_view_api/#add_tag","text":"FeatureView . add_tag ( name , value ) Attach a tag to a feature view. A tag consists of a name and value pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # attach a tag to a feature view feature_view . add_tag ( name = \"tag_schema\" , value = { \"key\" , \"value\" }) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/feature_view_api/#add_training_dataset_tag","text":"FeatureView . add_training_dataset_tag ( training_dataset_version , name , value ) Attach a tag to a training dataset. Example # get feature store instance fs = ... # get feature feature view instance feature_view = fs . get_feature_view ( ... ) # attach a tag to a training dataset feature_view . add_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" , value = { \"key\" , \"value\" } ) Arguments training_dataset_version int : training dataset version name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source]","title":"add_training_dataset_tag"},{"location":"generated/api/feature_view_api/#clean","text":"FeatureView . clean ( feature_store_id , feature_view_name , feature_view_version ) Delete the feature view and all associated metadata and training data. This can delete corrupted feature view which cannot be retrieved due to a corrupted query for example. Example # delete a feature view and all associated metadata from hsfs.feature_view import FeatureView FeatureView . clean ( feature_store_id = 1 , feature_view_name = 'feature_view_name' , feature_view_version = 1 ) Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Arguments feature_store_id int : int. Id of feature store. feature_view_name str : str. Name of feature view. feature_view_version str : str. Version of feature view. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"clean"},{"location":"generated/api/feature_view_api/#create_feature_monitoring","text":"FeatureView . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fg = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # compare to a given value specific_value = 0.5 , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_feature_monitoring"},{"location":"generated/api/feature_view_api/#create_statistics_monitoring","text":"FeatureView . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # enable statistics monitoring my_config = fv . _create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature view. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_statistics_monitoring"},{"location":"generated/api/feature_view_api/#create_train_test_split","text":"FeatureView . create_train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"parquet\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and save the corresponding training data into location . The training data is split into train and test set at random or according to time ranges. The training data can be retrieved by calling feature_view.get_train_test_split . Create random splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # create a train-test split dataset version , job = feature_view . create_train_test_split ( test_size = 0.2 , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Create time series splits by specifying date as string # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates train_start = \"2022-01-01 00:00:00\" train_end = \"2022-06-06 23:59:59\" test_start = \"2022-06-07 00:00:00\" test_end = \"2022-12-25 23:59:59\" # create a train-test split dataset version , job = feature_view . create_train_test_split ( train_start = train_start , train_end = train_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Create time series splits by specifying date as datetime object # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates from datetime import datetime date_format = \"%Y-%m- %d %H:%M:%S\" train_start = datetime . strptime ( \"2022-01-01 00:00:00\" , date_format ) train_end = datetime . strptime ( \"2022-06-06 23:59:59\" , date_format ) test_start = datetime . strptime ( \"2022-06-07 00:00:00\" , date_format ) test_end = datetime . strptime ( \"2022-12-25 23:59:59\" , date_format ) # create a train-test split dataset version , job = feature_view . create_train_test_split ( train_start = train_start , train_end = train_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Write training dataset to external storage # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get storage connector instance external_storage_connector = fs . get_storage_connector ( \"storage_connector_name\" ) # create a train-test split dataset version , job = feature_view . create_train_test_split ( train_start =... , train_end =... , test_start =... , test_end =... , storage_connector = external_storage_connector , description =... , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format =... ) Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Warning, the following code will fail because category column contains sparse values and training dataset may not have all values available in test split. import pandas as pd df = pd . DataFrame ({ 'category_col' :[ 'category_a' , 'category_b' , 'category_c' , 'category_d' ], 'numeric_col' : [ 40 , 10 , 60 , 40 ] }) feature_group = fs . get_or_create_feature_group ( name = 'feature_group_name' , version = 1 , primary_key = [ 'category_col' ] ) feature_group . insert ( df ) label_encoder = fs . get_transformation_function ( name = 'label_encoder' ) feature_view = fs . create_feature_view ( name = 'feature_view_name' , query = feature_group . select_all (), transformation_functions = { 'category_col' : label_encoder } ) feature_view . create_train_test_split ( test_size = 0.5 ) # Output: KeyError: 'category_c' Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following ormats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . data_format Optional[str] : The data format used to save the training dataset, defaults to \"parquet\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"create_train_test_split"},{"location":"generated/api/feature_view_api/#create_train_validation_test_split","text":"FeatureView . create_train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"parquet\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and save the corresponding training data into location . The training data is split into train, validation, and test set at random or according to time range. The training data can be retrieved by calling feature_view.get_train_validation_test_split . Create random splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 , description = 'Description of a dataset' , data_format = 'csv' ) Create time series splits by specifying date as string # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates train_start = \"2022-01-01 00:00:00\" train_end = \"2022-06-01 23:59:59\" validation_start = \"2022-06-02 00:00:00\" validation_end = \"2022-07-01 23:59:59\" test_start = \"2022-07-02 00:00:00\" test_end = \"2022-08-01 23:59:59\" # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( train_start = train_start , train_end = train_end , validation_start = validation_start , validation_end = validation_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Create time series splits by specifying date as datetime object # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates from datetime import datetime date_format = \"%Y-%m- %d %H:%M:%S\" train_start = datetime . strptime ( \"2022-01-01 00:00:00\" , date_format ) train_end = datetime . strptime ( \"2022-06-06 23:59:59\" , date_format ) validation_start = datetime . strptime ( \"2022-06-02 00:00:00\" , date_format ) validation_end = datetime . strptime ( \"2022-07-01 23:59:59\" , date_format ) test_start = datetime . strptime ( \"2022-06-07 00:00:00\" , date_format ) test_end = datetime . strptime ( \"2022-12-25 23:59:59\" , date_format ) # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( train_start = train_start , train_end = train_end , validation_start = validation_start , validation_end = validation_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Write training dataset to external storage # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get storage connector instance external_storage_connector = fs . get_storage_connector ( \"storage_connector_name\" ) # create a train-validation-test split dataset version , job = feature_view . create_train_validation_test_split ( train_start =... , train_end =... , validation_start =... , validation_end =... , test_start =... , test_end =... , description =... , storage_connector = external_storage_connector , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format =... ) Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments validation_size Optional[float] : size of validation set. test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the validation split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the validation split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . data_format Optional[str] : The data format used to save the training dataset, defaults to \"parquet\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"create_train_validation_test_split"},{"location":"generated/api/feature_view_api/#create_training_data","text":"FeatureView . create_training_data ( start_time = \"\" , end_time = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"parquet\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and save the corresponding training data into location . The training data can be retrieved by calling feature_view.get_training_data . Create training dataset # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # create a training dataset version , job = feature_view . create_training_data ( description = 'Description of a dataset' , data_format = 'csv' , # async creation in order not to wait till finish of the job write_options = { \"wait_for_job\" : False } ) Create training data specifying date range with dates as strings # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates start_time = \"2022-01-01 00:00:00\" end_time = \"2022-06-06 23:59:59\" # create a training dataset version , job = feature_view . create_training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) # When we want to read the training data, we need to supply the training data version returned by the create_training_data method: X_train , X_test , y_train , y_test = feature_view . get_training_data ( version ) Create training data specifying date range with dates as datetime objects # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates from datetime import datetime date_format = \"%Y-%m- %d %H:%M:%S\" start_time = datetime . strptime ( \"2022-01-01 00:00:00\" , date_format ) end_time = datetime . strptime ( \"2022-06-06 23:59:59\" , date_format ) # create a training dataset version , job = feature_view . create_training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format = 'csv' ) Write training dataset to external storage # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get storage connector instance external_storage_connector = fs . get_storage_connector ( \"storage_connector_name\" ) # create a train-test split dataset version , job = feature_view . create_training_data ( start_time =... , end_time =... , storage_connector = external_storage_connector , description =... , # you can have different data formats such as csv, tsv, tfrecord, parquet and others data_format =... ) Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the training dataset query, inclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the training dataset query, exclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . data_format Optional[str] : The data format used to save the training dataset, defaults to \"parquet\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"create_training_data"},{"location":"generated/api/feature_view_api/#delete","text":"FeatureView . delete () Delete current feature view, all associated metadata and training data. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete a feature view feature_view . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_view_api/#delete_all_training_datasets","text":"FeatureView . delete_all_training_datasets () Delete all training datasets. This will delete both metadata and training data. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete all training datasets feature_view . delete_all_training_datasets () Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training datasets. [source]","title":"delete_all_training_datasets"},{"location":"generated/api/feature_view_api/#delete_tag","text":"FeatureView . delete_tag ( name ) Delete a tag attached to a feature view. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete a tag feature_view . delete_tag ( 'name_of_tag' ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/feature_view_api/#delete_training_dataset","text":"FeatureView . delete_training_dataset ( training_dataset_version ) Delete a training dataset. This will delete both metadata and training data. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete a training dataset feature_view . delete_training_dataset ( training_dataset_version = 1 ) Arguments training_dataset_version int : Version of the training dataset to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training dataset. [source]","title":"delete_training_dataset"},{"location":"generated/api/feature_view_api/#delete_training_dataset_tag","text":"FeatureView . delete_training_dataset_tag ( training_dataset_version , name ) Delete a tag attached to a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # delete training dataset tag feature_view . delete_training_dataset_tag ( training_dataset_version = 1 , name = 'name_of_dataset' ) Arguments training_dataset_version int : training dataset version name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_training_dataset_tag"},{"location":"generated/api/feature_view_api/#find_neighbors","text":"FeatureView . find_neighbors ( embedding , feature = None , k = 10 , filter = None , min_score = 0 , external = None ) Finds the nearest neighbors for a given embedding in the vector database. Arguments embedding List[Union[int, float]] : The target embedding for which neighbors are to be found. feature Optional[hsfs.feature.Feature] : The feature used to compute similarity score. Required only if there are multiple embeddings (optional). k Optional[int] : The number of nearest neighbors to retrieve (default is 10). filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : A filter expression to restrict the search space (optional). min_score Optional[float] : The minimum similarity score for neighbors to be considered (default is 0). Returns A list of feature values Example embedding_index = EmbeddingIndex() embedding_index.add_embedding(name=\"user_vector\", dimension=3) fg = fs.create_feature_group( name='air_quality', embedding_index=embedding_index, version=1, primary_key=['id1'], online_enabled=True, ) fg.insert(data) fv = fs.create_feature_view(\"air_quality\", fg.select_all()) fv.find_neighbors( [0.1, 0.2, 0.3], k=5, ) # apply filter fg.find_neighbors( [0.1, 0.2, 0.3], k=5, feature=fg.user_vector, # optional filter=(fg.id1 > 10) & (fg.id1 < 30) ) [source]","title":"find_neighbors"},{"location":"generated/api/feature_view_api/#from_response_json","text":"FeatureView . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_view_api/#get_batch_data","text":"FeatureView . get_batch_data ( start_time = None , end_time = None , read_options = None , spine = None , primary_keys = False , event_time = False , inference_helper_columns = False , ) Get a batch of data from an event time interval from the offline feature store. Batch data for the last 24 hours # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates import datetime start_date = ( datetime . datetime . now () - datetime . timedelta ( hours = 24 )) end_date = ( datetime . datetime . now ()) # get a batch of data df = feature_view . get_batch_data ( start_time = start_date , end_time = end_date ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the batch query, inclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the batch query, exclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. read_options : User provided read options. Dictionary of read options for python engine: key \"use_hive\" and value True to read batch data with Hive instead of ArrowFlight Server . Defaults to {} . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. inference_helper_columns : whether to include inference helper columns or not. Inference helper columns are a list of feature names in the feature view, defined during its creation, that may not be used in training the model itself but can be used during batch or online inference for extra information. If inference helper columns were not defined in the feature view inference_helper_columns=True will not any effect. Defaults to False , no helper columns. Returns DataFrame : A dataframe [source]","title":"get_batch_data"},{"location":"generated/api/feature_view_api/#get_batch_query","text":"FeatureView . get_batch_query ( start_time = None , end_time = None ) Get a query string of the batch query. Batch query for the last 24 hours # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates import datetime start_date = ( datetime . datetime . now () - datetime . timedelta ( hours = 24 )) end_date = ( datetime . datetime . now ()) # get a query string of batch query query_str = feature_view . get_batch_query ( start_time = start_date , end_time = end_date ) # print query string print ( query_str ) Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the batch query, inclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the batch query, exclusive. Optional. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. Returns str : batch query [source]","title":"get_batch_query"},{"location":"generated/api/feature_view_api/#get_feature_monitoring_configs","text":"FeatureView . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature view fv = fs . get_feature_view ( name = \"my_feature_view\" , version = 1 ) # fetch all feature monitoring configs attached to the feature view fm_configs = fv . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fv . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fv . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a particular id fm_config = fv . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source]","title":"get_feature_monitoring_configs"},{"location":"generated/api/feature_view_api/#get_feature_monitoring_history","text":"FeatureView . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature view fv = fs . get_feature_view ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fv . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # or use the config id fm_history = fv . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_date : The start date of the feature monitoring history to fetch. Defaults to None. end_date : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source]","title":"get_feature_monitoring_history"},{"location":"generated/api/feature_view_api/#get_feature_vector","text":"FeatureView . get_feature_vector ( entry , passed_features = None , external = None , return_type = \"list\" , allow_missing = False , force_rest_client = False , force_sql_client = False , ) Returns assembled feature vector from online feature store. Call feature_view.init_serving before this method if the following configurations are needed. 1. The training dataset version of the transformation statistics 2. Additional configurations of online serving engine (e.g init_online_store_rest_client=True to use Online Store REST Client instead of SQL connector) !!! warning \"Missing primary key entries\" If the provided primary key entry can't be found in one or more of the feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting allow_missing to True returns a feature vector with missing values. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled serving vector as a python list feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) # get assembled serving vector as a pandas dataframe feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, return_type = \"pandas\" ) # get assembled serving vector as a numpy array feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, return_type = \"numpy\" ) Get feature vector with user-supplied features # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # the application provides a feature value 'app_attr' app_attr = ... # get a feature vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 }, passed_features = { \"app_feature\" : app_attr } ) Arguments entry Dict[str, Any] : dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys If the required primary keys is not provided, it will look for name of the primary key in feature group in the entry. passed_features Optional[Dict[str, Any]] : dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"list\" , \"pandas\" or \"numpy\" . Defaults to \"list\" . allow_missing Optional[bool] : Setting to True returns feature vectors with missing values. force_rest_client bool : bool, optional. If set to True, the Online Store REST Client will be used to retrieve the feature vector. Defaults to False. force_sql_client bool : bool, optional. If set to True, the SQL connector will be used to retrieve the feature vector. Defaults to False. Returns list , pd.DataFrame or np.ndarray if return type is set to \"list\" , \"pandas\" or \"numpy\" respectively. Defaults to list . Returned list , pd.DataFrame or np.ndarray contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query. Raises hsfs.client.exceptions.RestAPIError . If using the Online Store REST Client, and the response status code is not 200. - 400: Requested Metadata does not exist or the request is malformed. - 401: Access denied. API key does not give access to the feature store (e.g feature store not shared with user), or authorization header (x-api-key) is not properly set. - 500: Internal server error. ValueError . - A force_* parameter is set to True and the corresponding client is not initialised. - Both force_rest_client and force_sql_client are set to True . - The return_type is not one of \"list\" , \"pandas\" or \"numpy\" . - Training Dataset version is not set and the feature view is not initialised. - Serving keys do not match the provided entry dictionary [source]","title":"get_feature_vector"},{"location":"generated/api/feature_view_api/#get_feature_vectors","text":"FeatureView . get_feature_vectors ( entry , passed_features = None , external = None , return_type = \"list\" , allow_missing = False , force_rest_client = False , force_sql_client = False , ) Returns assembled feature vectors in batches from online feature store. Call feature_view.init_serving before this method if the following configurations are needed. 1. The training dataset version of the transformation statistics 2. Additional configurations of online serving engine (e.g init_online_store_rest_client=True to use Online Store REST Client instead of SQL connector) Missing primary key entries If any of the provided primary key elements in entry can't be found in any of the feature groups, no feature vector for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting allow_missing to True returns feature vectors with missing values. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled serving vectors as a python list of lists feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) # get assembled serving vectors as a pandas dataframe feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], return_type = \"pandas\" ) # get assembled serving vectors as a numpy array feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ], return_type = \"numpy\" ) Arguments entry List[Dict[str, Any]] : a list of dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys If the required primary keys is not provided, it will look for name of the primary key in feature group in the entry. passed_features Optional[List[Dict[str, Any]]] : a list of dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"list\" , \"pandas\" or \"numpy\" . Defaults to \"list\" . allow_missing Optional[bool] : Setting to True returns feature vectors with missing values. Returns List[list] , pd.DataFrame or np.ndarray if return type is set to \"list\", \"pandas\" or \"numpy\" respectively. Defaults to List[list]`. Returned List[list] , pd.DataFrame or np.ndarray contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query. Raises hsfs.client.exceptions.RestAPIError . If using the Online Store REST client, and the response status code is not 200. - 400: Requested Metadata does not exist or the request is malformed. - 401: Access denied. API key does not give access to the feature store (e.g feature store not shared with user), or authorization header (x-api-key) is not properly set. - 500: Internal server error. ValueError . - A force_* parameter is set to True and the corresponding client is not initialised. - Both force_rest_client and force_sql_client are set to True . - The return_type is not one of \"list\" , \"pandas\" or \"numpy\" . - Training Dataset version is not set and the feature view is not initialised. - Serving keys do not match the provided entry dictionary [source]","title":"get_feature_vectors"},{"location":"generated/api/feature_view_api/#get_inference_helper","text":"FeatureView . get_inference_helper ( entry , external = None , return_type = \"pandas\" ) Returns assembled inference helper column vectors from online feature store. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled inference helper column vector feature_view . get_inference_helper ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) Arguments entry Dict[str, Any] : dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"pandas\" or \"dict\" . Defaults to \"pandas\" . Returns pd.DataFrame or dict . Defaults to pd.DataFrame . Raises Exception . When primary key entry cannot be found in one or more of the feature groups used by this feature view. [source]","title":"get_inference_helper"},{"location":"generated/api/feature_view_api/#get_inference_helpers","text":"FeatureView . get_inference_helpers ( entry , external = None , return_type = \"pandas\" ) Returns assembled inference helper column vectors in batches from online feature store. Missing primary key entries If any of the provided primary key elements in entry can't be found in any of the feature groups, no inference helper column vectors for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get assembled inference helper column vectors feature_view . get_inference_helpers ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) Arguments entry List[Dict[str, Any]] : a list of dictionary of feature group primary key and values provided by serving application. Set of required primary keys is feature_view.primary_keys external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. return_type Optional[str] : \"pandas\" or \"dict\" . Defaults to \"dict\" . Returns pd.DataFrame or List[dict] . Defaults to pd.DataFrame . Returned pd.DataFrame or List[dict] contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query. Raises Exception . When primary key entry cannot be found in one or more of the feature groups used by this feature view. [source]","title":"get_inference_helpers"},{"location":"generated/api/feature_view_api/#get_parent_feature_groups","text":"FeatureView . get_parent_feature_groups () Get the parents of this feature view, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. [source]","title":"get_parent_feature_groups"},{"location":"generated/api/feature_view_api/#get_tag","text":"FeatureView . get_tag ( name ) Get the tags of a feature view. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get a tag of a feature view name = feature_view . get_tag ( 'tag_name' ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/feature_view_api/#get_tags","text":"FeatureView . get_tags () Returns all tags attached to a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get tags list_tags = feature_view . get_tags () Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/feature_view_api/#get_train_test_split","text":"FeatureView . get_train_test_split ( training_dataset_version , read_options = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Get training data created by feature_view.create_train_test_split or feature_view.train_test_split . Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_test , y_train , y_test = feature_view . get_train_test_split ( training_dataset_version = 1 ) Arguments training_dataset_version : training dataset version read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read training dataset with the Hopsworks API instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source]","title":"get_train_test_split"},{"location":"generated/api/feature_view_api/#get_train_validation_test_split","text":"FeatureView . get_train_validation_test_split ( training_dataset_version , read_options = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Get training data created by feature_view.create_train_validation_test_split or feature_view.train_validation_test_split . Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_val , X_test , y_train , y_val , y_test = feature_view . get_train_validation_test_splits ( training_dataset_version = 1 ) Arguments training_dataset_version : training dataset version read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read training dataset with the Hopsworks API instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source]","title":"get_train_validation_test_split"},{"location":"generated/api/feature_view_api/#get_training_data","text":"FeatureView . get_training_data ( training_dataset_version , read_options = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Get training data created by feature_view.create_training_data or feature_view.training_data . Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data features_df , labels_df = feature_view . get_training_data ( training_dataset_version = 1 ) External Storage Support Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client. Arguments training_dataset_version : training dataset version read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. For python engine: key \"use_hive\" and value True to read training dataset with the Hopsworks API instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view or during materializing training dataset in the file system then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X, y): Tuple of dataframe of features and labels [source]","title":"get_training_data"},{"location":"generated/api/feature_view_api/#get_training_dataset_statistics","text":"FeatureView . get_training_dataset_statistics ( training_dataset_version , before_transformation = False , feature_names = None ) Get statistics of a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training dataset statistics statistics = feature_view . get_training_dataset_statistics ( training_dataset_version = 1 ) Arguments training_dataset_version : Training dataset version before_transformation : Whether the statistics were computed before transformation functions or not. feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics [source]","title":"get_training_dataset_statistics"},{"location":"generated/api/feature_view_api/#get_training_dataset_tag","text":"FeatureView . get_training_dataset_tag ( training_dataset_version , name ) Get the tags of a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get a training dataset tag tag_str = feature_view . get_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) Arguments training_dataset_version int : training dataset version name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_training_dataset_tag"},{"location":"generated/api/feature_view_api/#get_training_dataset_tags","text":"FeatureView . get_training_dataset_tags ( training_dataset_version ) Returns all tags attached to a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get a training dataset tags list_tags = feature_view . get_training_dataset_tags ( training_dataset_version = 1 ) Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_training_dataset_tags"},{"location":"generated/api/feature_view_api/#get_training_datasets","text":"FeatureView . get_training_datasets () Returns the metadata of all training datasets created with this feature view. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get all training dataset metadata list_tds_meta = feature_view . get_training_datasets () Returns List[TrainingDatasetBase] List of training datasets metadata. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the training datasets metadata. [source]","title":"get_training_datasets"},{"location":"generated/api/feature_view_api/#init_batch_scoring","text":"FeatureView . init_batch_scoring ( training_dataset_version = None ) Initialise feature view to retrieve feature vector from offline feature store. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # initialise feature view to retrieve feature vector from offline feature store feature_view . init_batch_scoring ( training_dataset_version = 1 ) # get batch data batch_data = feature_view . get_batch_data ( ... ) Arguments training_dataset_version Optional[int] : int, optional. Default to be None. Transformation statistics are fetched from training dataset and applied to the feature vector. [source]","title":"init_batch_scoring"},{"location":"generated/api/feature_view_api/#init_serving","text":"FeatureView . init_serving ( training_dataset_version = None , external = None , options = None , init_online_store_sql_client = None , init_online_store_rest_client = False , ) Initialise feature view to retrieve feature vector from online and offline feature store. The Online Feature Store now supports feature vector retrieval using either the SQL connector or a REST http client. Defaults to SQL connector to match the previous behaviour. To use the the REST client, set init_online_store_rest_client to True . Both get_feature_vector and get_feature_vectors methods will default to using the initialised client. If both are initialised, the SQL client will be used by default. You can override this behaviour on a per-call basis using the methods kwargs or set the default via set_default_online_store_client method. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # initialise feature view to retrieve a feature vector feature_view . init_serving ( training_dataset_version = 1 ) Initialising the Online Store REST Client to retrieve feature vectors from the online feature store, with additional configuration options. Example # initialise feature view to retrieve a feature vector using the RonDB REST http client feature_view . init_serving ( training_dataset_version = 1 , init_online_store_rest_client = True , ) You can reset the Online Store REST Client connection to fix configuration options. In particular, if you have called get_feature_vector or get_feature_vectors without first initialising the client, it results in a default configuration being set for the rest client. This will reset the client and apply the new configuration options. Example # reset the RonDB REST http client connection feature_view . init_serving ( training_dataset_version = 1 , init_online_store_rest_client = True , options = { \"reset_online_store_rest_client\" : True , \"config_online_store_rest_client\" : { \"host\" : \"new_host\" , \"timeout\" : 1000 }}, ) Note that both the SQL connector and the REST client can be initialised at the same time. This is useful if you want to fallback on one connector if the other fails. Example # initialise feature view to retrieve a feature vector using both the SQL connector and the RonDB REST http client feature_view . init_serving ( training_dataset_version = 1 , init_online_store_sql_client = True , init_online_store_rest_client = True , ) # When initialising both clients, the SQL connector will be used by default. Change the default client using `set_default_online_client`. feature_view . set_default_online_client ( \"rest\" ) Arguments training_dataset_version Optional[int] : int, optional. Default to be 1 for online feature store. Transformation statistics are fetched from training dataset and applied to the feature vector. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. init_online_store_sql_client Optional[bool] : bool, optional. If set to True, initialise the SQL client to retrieve feature vector(s) from the online feature store. Defaults to True if init_online_store_rest_client is False, otherwise False. init_online_store_rest_client bool : bool, optional. If set to True, initialise the Online Store REST Client to retrieve feature vector(s) from the online feature store. Defaults to False, meaning the sql client will be initialised. options Optional[dict] : Additional options as key/value pairs for configuring online serving engine. key: kwargs of SqlAlchemy engine creation (See: https://docs.sqlalchemy.org/en/20/core/engines.html#sqlalchemy.create_engine). For example: {\"pool_size\": 10} key: \"config_online_store_rest_client\" - dict, optional. Optional configuration options to override defaults for the Online Store REST Client. key: \"api_key\" - str. The API key to use for the Online Store REST Client. THIS IS REQUIRED FOR INTERNAL CLIENTS. key: \"host\" - str, optional. The host of the Online Store REST Client. key: \"port\" - int, optional. The port of the Online Store REST Client. key: \"verify_certs\" - bool, optional. If set to True, the Online Store REST Client will verify the server's certificate. key: \"ca_chain\" - str, optional. The path to the CA chain file. key: \"use_ssl\" - bool, optional. If set to True, the Online Store REST Client will use SSL. key: \"timeout\" - int, optional. The timeout of the Online Store REST Client. key: \"server_api_version\" - str, optional. The version of the RonDB Server FeatureStore API. key: \"http_authorization\" - str, optional. The HTTP authorization header to use for the Online Store REST Client. key: \"reset_online_store_rest_client\" - bool, optional. If set to True, the Online Store REST Client will be reset. Provide \"config_online_store_rest_client\" to override defaults. [source]","title":"init_serving"},{"location":"generated/api/feature_view_api/#json","text":"FeatureView . json () [source]","title":"json"},{"location":"generated/api/feature_view_api/#purge_all_training_data","text":"FeatureView . purge_all_training_data () Delete all training datasets (data only). Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # purge all training data feature_view . purge_all_training_data () Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training datasets. [source]","title":"purge_all_training_data"},{"location":"generated/api/feature_view_api/#purge_training_data","text":"FeatureView . purge_training_data ( training_dataset_version ) Delete a training dataset (data only). Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # purge training data feature_view . purge_training_data ( training_dataset_version = 1 ) Arguments training_dataset_version int : Version of the training dataset to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the training dataset. [source]","title":"purge_training_data"},{"location":"generated/api/feature_view_api/#recreate_training_dataset","text":"FeatureView . recreate_training_dataset ( training_dataset_version , statistics_config = None , write_options = None , spine = None ) Recreate a training dataset. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # recreate a training dataset that has been deleted feature_view . recreate_training_dataset ( training_dataset_version = 1 ) Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments training_dataset_version int : training dataset version statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, write_options can contain the following entries: key use_spark and value True to materialize training dataset with Spark instead of ArrowFlight Server . key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"recreate_training_dataset"},{"location":"generated/api/feature_view_api/#set_default_online_client","text":"FeatureView . set_default_online_client ( client ) Set the default client to either 'sql' or 'rest' to retrieve feature vectors from the online feature store. If only one client is initialised when calling init_serving , this client will be used by default. If both clients are initialised, the SQL client will be used by default. This method allows you to specify the default client. You can override this behaviour on a per-call basis using the methods kwargs. Arguments client str : str. The default online client to be used for the feature view. The default online client can be set to \"rest\" or \"sql\". Raises ValueError . - If vector server is not initialised via init_serving - If setting default to a client not initialised. Use init_serving with either init_online_store_sql_client or init_online_store_rest_client to initialise the client. - If client is not \"rest\" or \"sql\". [source]","title":"set_default_online_client"},{"location":"generated/api/feature_view_api/#to_dict","text":"FeatureView . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_view_api/#train_test_split","text":"FeatureView . train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train and test set at random or according to time ranges. The training data can be recreated by calling feature_view.get_train_test_split with the metadata created. Create random train/test splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_test , y_train , y_test = feature_view . train_test_split ( test_size = 0.2 ) Create time-series train/test splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates train_start = \"2022-05-01 00:00:00\" train_end = \"2022-06-04 23:59:59\" test_start = \"2022-07-01 00:00:00\" test_end = \"2022-08-04 23:59:59\" # you can also pass dates as datetime objects # get training data X_train , X_test , y_train , y_test = feature_view . train_test_split ( train_start = train_start , train_end = train_end , test_start = test_start , test_end = test_end , description = 'Description of a dataset' ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, read_options can contain the following entries: key \"use_hive\" and value True to create in-memory training dataset with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source]","title":"train_test_split"},{"location":"generated/api/feature_view_api/#train_validation_test_split","text":"FeatureView . train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train, validation, and test set at random or according to time ranges. The training data can be recreated by calling feature_view.get_train_validation_test_split with the metadata created. Example # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data X_train , X_val , X_test , y_train , y_val , y_test = feature_view . train_validation_test_split ( validation_size = 0.3 , test_size = 0.2 ) Time Series split # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up dates start_time_train = '2017-01-01 00:00:01' end_time_train = '2018-02-01 23:59:59' start_time_val = '2018-02-02 23:59:59' end_time_val = '2019-02-01 23:59:59' start_time_test = '2019-02-02 23:59:59' end_time_test = '2020-02-01 23:59:59' # you can also pass dates as datetime objects # get training data X_train , X_val , X_test , y_train , y_val , y_test = feature_view . train_validation_test_split ( train_start = start_time_train , train_end = end_time_train , validation_start = start_time_val , validation_end = end_time_val , test_start = start_time_test , test_end = end_time_test ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments validation_size Optional[float] : size of validation set. Should be between 0 and 1. test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the train split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the train split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the validation split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the validation split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the test split query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the test split query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, read_options can contain the following entries: key \"use_hive\" and value True to create in-memory training dataset with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source]","title":"train_validation_test_split"},{"location":"generated/api/feature_view_api/#training_data","text":"FeatureView . training_data ( start_time = None , end_time = None , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , spine = None , primary_keys = False , event_time = False , training_helper_columns = False , ) Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data can be recreated by calling feature_view.get_training_data with the metadata created. Create random splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # get training data features_df , labels_df = feature_view . training_data ( description = 'Descriprion of a dataset' , ) Create time-series based splits # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) # set up a date start_time = \"2022-05-01 00:00:00\" end_time = \"2022-06-04 23:59:59\" # you can also pass dates as datetime objects # get training data features_df , labels_df = feature_view . training_data ( start_time = start_time , end_time = end_time , description = 'Description of a dataset' ) Spine Groups/Dataframes Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Start event time for the training dataset query, inclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : End event time for the training dataset query, exclusive. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . Int, i.e Unix Epoch should be in seconds. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . extra_filter Optional[Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic]] : Additional filters to be attached to the training dataset. The filters will be also applied in get_batch_data . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional options as key/value pairs to pass to the execution engine. For spark engine: Dictionary of read options for Spark. When using the python engine, read_options can contain the following entries: key \"use_hive\" and value True to create in-memory training dataset with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Defaults to {} . spine Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list], SpineGroup]] : Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Defaults to None and is only required when feature view was created with spine group in the feature query. It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the feature join, however, the same features as in the original feature group that is being replaced need to be available in the spine group. primary_keys : whether to include primary key features or not. Defaults to False , no primary key features. event_time : whether to include event time feature or not. Defaults to False , no event time feature. training_helper_columns : whether to include training helper columns or not. Training helper columns are a list of feature names in the feature view, defined during its creation, that are not the part of the model schema itself but can be used during training as a helper for extra information. If training helper columns were not defined in the feature view then training_helper_columns=True will not have any effect. Defaults to False , no training helper columns. Returns (X, y): Tuple of dataframe of features and labels. If there are no labels, y returns None . [source]","title":"training_data"},{"location":"generated/api/feature_view_api/#update","text":"FeatureView . update () Update the description of the feature view. Update the feature view with a new description. # get feature store instance fs = ... # get feature view instance feature_view = fs . get_feature_view ( ... ) feature_view . description = \"new description\" feature_view . update () # Description is updated in the metadata. Below should return \"new description\". fs . get_feature_view ( \"feature_view_name\" , 1 ) . description Returns FeatureView Updated feature view. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"update"},{"location":"generated/api/feature_view_api/#update_from_response_json","text":"FeatureView . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/job/","text":"Job # [source] Job # hsfs . core . job . Job ( id , name , creation_time , config , job_type , creator , executions = None , type = None , job_schedule = None , href = None , expand = None , items = None , count = None , ** kwargs ) Methods # [source] get_state # Job . get_state () Get the state of the job. Returns state . Current state of the job, which can be one of the following: INITIALIZING , INITIALIZATION_FAILED , FINISHED , RUNNING , ACCEPTED , FAILED , KILLED , NEW , NEW_SAVING , SUBMITTED , AGGREGATING_LOGS , FRAMEWORK_FAILURE , STARTING_APP_MASTER , APP_MASTER_START_FAILED , GENERATING_SECURITY_MATERIAL , CONVERTING_NOTEBOOK [source] get_final_state # Job . get_final_state () Get the final state of the job. Returns final_state . Final state of the job, which can be one of the following: UNDEFINED , FINISHED , FAILED , KILLED , FRAMEWORK_FAILURE , APP_MASTER_START_FAILED , INITIALIZATION_FAILED . UNDEFINED indicates that the job is still running. Job Configuration # [source] JobConfiguration # hsfs . core . job_configuration . JobConfiguration ( am_memory = 2048 , am_cores = 1 , executor_memory = 4096 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , ** kwargs )","title":"Job"},{"location":"generated/api/job/#job","text":"[source]","title":"Job"},{"location":"generated/api/job/#job_1","text":"hsfs . core . job . Job ( id , name , creation_time , config , job_type , creator , executions = None , type = None , job_schedule = None , href = None , expand = None , items = None , count = None , ** kwargs )","title":"Job"},{"location":"generated/api/job/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/job/#get_state","text":"Job . get_state () Get the state of the job. Returns state . Current state of the job, which can be one of the following: INITIALIZING , INITIALIZATION_FAILED , FINISHED , RUNNING , ACCEPTED , FAILED , KILLED , NEW , NEW_SAVING , SUBMITTED , AGGREGATING_LOGS , FRAMEWORK_FAILURE , STARTING_APP_MASTER , APP_MASTER_START_FAILED , GENERATING_SECURITY_MATERIAL , CONVERTING_NOTEBOOK [source]","title":"get_state"},{"location":"generated/api/job/#get_final_state","text":"Job . get_final_state () Get the final state of the job. Returns final_state . Final state of the job, which can be one of the following: UNDEFINED , FINISHED , FAILED , KILLED , FRAMEWORK_FAILURE , APP_MASTER_START_FAILED , INITIALIZATION_FAILED . UNDEFINED indicates that the job is still running.","title":"get_final_state"},{"location":"generated/api/job/#job-configuration","text":"[source]","title":"Job Configuration"},{"location":"generated/api/job/#jobconfiguration","text":"hsfs . core . job_configuration . JobConfiguration ( am_memory = 2048 , am_cores = 1 , executor_memory = 4096 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , ** kwargs )","title":"JobConfiguration"},{"location":"generated/api/links/","text":"Provenance Links # Provenance Links are objects returned by methods such as get_parent_feature_group , get_generated_feature_groups , get_generated_feature_views and represent sections of the provenance graph, depending on the method invoked. Properties # [source] accessible # List of feature groups or feature views metadata objects which are part of the provenance graph requested. These entities exist in the feature store and the user has access to them. [source] deleted # List of Artifact objects which contains minimal information (name, version) about the entities (feature groups, feature views) they represent. These entities have been removed from the feature store. [source] faulty # List of Artifact objects which contains minimal information (name, version) about the entities (feature groups, feature views) they represent. These entities exist in the feature store, however they are corrupted. [source] inaccessible # List of Artifact objects which contains minimal information (name, version) about the entities (feature groups, feature views) they represent. These entities exist in the feature store, however the user does not have access to them anymore. Artifact # Artifacts objects are part of the provenance graph and contain a minimal set of information regarding the entities (feature groups, feature views) they represent. The provenance graph contains Artifact objects when the underlying entities have been deleted or they are corrupted or they are not accessible by the user. [source] feature_store_name # Name of the feature store in which the artifact is located. [source] name # Name of the artifact. [source] version # Version of the artifact","title":"Provenance Links"},{"location":"generated/api/links/#provenance-links","text":"Provenance Links are objects returned by methods such as get_parent_feature_group , get_generated_feature_groups , get_generated_feature_views and represent sections of the provenance graph, depending on the method invoked.","title":"Provenance Links"},{"location":"generated/api/links/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/links/#accessible","text":"List of feature groups or feature views metadata objects which are part of the provenance graph requested. These entities exist in the feature store and the user has access to them. [source]","title":"accessible"},{"location":"generated/api/links/#deleted","text":"List of Artifact objects which contains minimal information (name, version) about the entities (feature groups, feature views) they represent. These entities have been removed from the feature store. [source]","title":"deleted"},{"location":"generated/api/links/#faulty","text":"List of Artifact objects which contains minimal information (name, version) about the entities (feature groups, feature views) they represent. These entities exist in the feature store, however they are corrupted. [source]","title":"faulty"},{"location":"generated/api/links/#inaccessible","text":"List of Artifact objects which contains minimal information (name, version) about the entities (feature groups, feature views) they represent. These entities exist in the feature store, however the user does not have access to them anymore.","title":"inaccessible"},{"location":"generated/api/links/#artifact","text":"Artifacts objects are part of the provenance graph and contain a minimal set of information regarding the entities (feature groups, feature views) they represent. The provenance graph contains Artifact objects when the underlying entities have been deleted or they are corrupted or they are not accessible by the user. [source]","title":"Artifact"},{"location":"generated/api/links/#feature_store_name","text":"Name of the feature store in which the artifact is located. [source]","title":"feature_store_name"},{"location":"generated/api/links/#name","text":"Name of the artifact. [source]","title":"name"},{"location":"generated/api/links/#version","text":"Version of the artifact","title":"version"},{"location":"generated/api/query_api/","text":"Query # Query objects are strictly generated by HSFS APIs called on Feature Group objects . Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here. Methods # [source] append_feature # Query . append_feature ( feature ) Append a feature to the query. Arguments feature : [str, Feature] . Name of the feature to append to the query. [source] as_of # Query . as_of ( wallclock_time = None , exclude_until = None ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: query1 . as_of ( ... , ... ) . join ( query2 . as_of ( ... , ... )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: query1 . as_of ( ... , ... ) # as_of is not applied . join ( query2 . as_of ( ... , ... )) # as_of is not applied . as_of ( ... , ... ) Warning This function only works for queries on feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Read data as of this point in time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . exclude_until Optional[Union[str, int, datetime.datetime, datetime.date]] : Exclude commits until this point in time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . Returns Query . The query object with the applied time travel condition. [source] filter # Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) query . filter ( Feature ( \"name\" ) . like ( \"max%\" )) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Filters are fully compatible with joins fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) fg3 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( fg3 . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) . filter (( fg1 . location_id == 10 ) | ( fg1 . location_id == 20 )) Filters can be applied at any point of the query fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) fg3 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all () . filter ( fg2 . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) . join ( fg3 . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) . filter ( fg1 . location_id == 10 ) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # Query . from_response_json ( json_dict ) [source] get_feature # Query . get_feature ( feature_name ) Get a feature by name. Arguments feature_name : str . Name of the feature to get. Returns Feature . Feature object. [source] is_cache_feature_group_only # Query . is_cache_feature_group_only () Query contains only cached feature groups [source] is_time_travel # Query . is_time_travel () Query contains time travel [source] join # Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no nested joins. Join two feature groups fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all ()) More complex join fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) fg3 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( fg3 . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source] pull_changes # Query . pull_changes ( wallclock_start_time , wallclock_end_time ) Deprecated pull_changes method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead. [source] read # Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). External Feature Group Engine Support Spark only Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Dictionary of read options for Spark in spark engine. Only for python engine: key \"use_hive\" and value True to read query with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source] show # Query . show ( n , online = False ) Show the first N rows of the Query. Show the first 10 rows fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all ()) query . show ( 10 ) Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source] to_string # Query . to_string ( online = False , arrow_flight = False ) Example fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all ()) query . to_string () Properties # [source] featuregroups # List of feature groups used in the query [source] features # List of all features in the query [source] filters # All filters used in the query [source] joins # List of joins in the query [source] left_feature_group_end_time # End time of time travel for the left feature group. [source] left_feature_group_start_time # Start time of time travel for the left feature group.","title":"Query"},{"location":"generated/api/query_api/#query","text":"Query objects are strictly generated by HSFS APIs called on Feature Group objects . Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here.","title":"Query"},{"location":"generated/api/query_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/query_api/#append_feature","text":"Query . append_feature ( feature ) Append a feature to the query. Arguments feature : [str, Feature] . Name of the feature to append to the query. [source]","title":"append_feature"},{"location":"generated/api/query_api/#as_of","text":"Query . as_of ( wallclock_time = None , exclude_until = None ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: query1 . as_of ( ... , ... ) . join ( query2 . as_of ( ... , ... )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: query1 . as_of ( ... , ... ) # as_of is not applied . join ( query2 . as_of ( ... , ... )) # as_of is not applied . as_of ( ... , ... ) Warning This function only works for queries on feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Read data as of this point in time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . exclude_until Optional[Union[str, int, datetime.datetime, datetime.date]] : Exclude commits until this point in time. Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , or %Y-%m-%d %H:%M:%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/api/query_api/#filter","text":"Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) query . filter ( Feature ( \"name\" ) . like ( \"max%\" )) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Filters are fully compatible with joins fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) fg3 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( fg3 . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) . filter (( fg1 . location_id == 10 ) | ( fg1 . location_id == 20 )) Filters can be applied at any point of the query fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) fg3 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all () . filter ( fg2 . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) . join ( fg3 . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) . filter ( fg1 . location_id == 10 ) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/query_api/#from_response_json","text":"Query . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/query_api/#get_feature","text":"Query . get_feature ( feature_name ) Get a feature by name. Arguments feature_name : str . Name of the feature to get. Returns Feature . Feature object. [source]","title":"get_feature"},{"location":"generated/api/query_api/#is_cache_feature_group_only","text":"Query . is_cache_feature_group_only () Query contains only cached feature groups [source]","title":"is_cache_feature_group_only"},{"location":"generated/api/query_api/#is_time_travel","text":"Query . is_time_travel () Query contains time travel [source]","title":"is_time_travel"},{"location":"generated/api/query_api/#join","text":"Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no nested joins. Join two feature groups fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all ()) More complex join fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) fg3 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( fg3 . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], join_type = \"left\" ) Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source]","title":"join"},{"location":"generated/api/query_api/#pull_changes","text":"Query . pull_changes ( wallclock_start_time , wallclock_end_time ) Deprecated pull_changes method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead. [source]","title":"pull_changes"},{"location":"generated/api/query_api/#read","text":"Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). External Feature Group Engine Support Spark only Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Dictionary of read options for Spark in spark engine. Only for python engine: key \"use_hive\" and value True to read query with Hive instead of ArrowFlight Server . key \"arrow_flight_config\" to pass a dictionary of arrow flight configurations. For example: {\"arrow_flight_config\": {\"timeout\": 900}} key \"hive_config\" to pass a dictionary of hive or tez configurations. For example: {\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}} Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source]","title":"read"},{"location":"generated/api/query_api/#show","text":"Query . show ( n , online = False ) Show the first N rows of the Query. Show the first 10 rows fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all ()) query . show ( 10 ) Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source]","title":"show"},{"location":"generated/api/query_api/#to_string","text":"Query . to_string ( online = False , arrow_flight = False ) Example fg1 = fs . get_feature_group ( \"...\" ) fg2 = fs . get_feature_group ( \"...\" ) query = fg1 . select_all () . join ( fg2 . select_all ()) query . to_string ()","title":"to_string"},{"location":"generated/api/query_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/query_api/#featuregroups","text":"List of feature groups used in the query [source]","title":"featuregroups"},{"location":"generated/api/query_api/#features","text":"List of all features in the query [source]","title":"features"},{"location":"generated/api/query_api/#filters","text":"All filters used in the query [source]","title":"filters"},{"location":"generated/api/query_api/#joins","text":"List of joins in the query [source]","title":"joins"},{"location":"generated/api/query_api/#left_feature_group_end_time","text":"End time of time travel for the left feature group. [source]","title":"left_feature_group_end_time"},{"location":"generated/api/query_api/#left_feature_group_start_time","text":"Start time of time travel for the left feature group.","title":"left_feature_group_start_time"},{"location":"generated/api/rule_api/","text":"Rule # {{rule}} Properties # {{rule_properties}}","title":"Rule"},{"location":"generated/api/rule_api/#rule","text":"{{rule}}","title":"Rule"},{"location":"generated/api/rule_api/#properties","text":"{{rule_properties}}","title":"Properties"},{"location":"generated/api/rule_definition_api/","text":"Rule Definition # {{ruledefinition}} Properties # {{ruledefinition_properties}} Retrieval # {{ruledefinition_getall}} {{ruledefinition_get}}","title":"Rule Definition"},{"location":"generated/api/rule_definition_api/#rule-definition","text":"{{ruledefinition}}","title":"Rule Definition"},{"location":"generated/api/rule_definition_api/#properties","text":"{{ruledefinition_properties}}","title":"Properties"},{"location":"generated/api/rule_definition_api/#retrieval","text":"{{ruledefinition_getall}} {{ruledefinition_get}}","title":"Retrieval"},{"location":"generated/api/spine_group_api/","text":"SpineGroup # [source] SpineGroup # hsfs . feature_group . SpineGroup ( storage_connector = None , query = None , data_format = None , path = None , options = {}, name = None , version = None , description = None , primary_key = None , featurestore_id = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , href = None , online_topic_name = None , topic_name = None , spine = True , dataframe = \"spine\" , deprecated = False , ** kwargs ) Creation # [source] get_or_create_spine_group # FeatureStore . get_or_create_spine_group ( name , version = None , description = \"\" , primary_key = [], event_time = None , features = [], dataframe = None ) Create a spine group metadata object. Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins. Example # connect to the Feature Store fs = ... spine_df = pd . Dataframe () spine_group = fs . get_or_create_spine_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , dataframe = spine_df ) Note that you can inspect the dataframe in the spine group, or replace the dataframe: spine_group . dataframe . show () spine_group . dataframe = new_df The spine can then be used to construct queries, with only one speciality: Note Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against. If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving. These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again. For example, to generate training data: X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = training_data_entities ) Or to get batches of fresh data for batch scoring: feature_view_spine . get_batch_data ( spine = scoring_entities_df ) . show () Here you have the chance to pass a different set of entities to generate the training dataset. Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column. feature_view . get_batch_data ( spine = spine_group ) Arguments name str : Name of the spine group to create. version Optional[int] : Version of the spine group to retrieve, defaults to None and will create the spine group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the spine group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the spine group won't have any primary key. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins. Defaults to None . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the spine group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . dataframe : DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Returns SpineGroup . The spine group metadata object. Retrieval # [source] get_or_create_spine_group # FeatureStore . get_or_create_spine_group ( name , version = None , description = \"\" , primary_key = [], event_time = None , features = [], dataframe = None ) Create a spine group metadata object. Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins. Example # connect to the Feature Store fs = ... spine_df = pd . Dataframe () spine_group = fs . get_or_create_spine_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , dataframe = spine_df ) Note that you can inspect the dataframe in the spine group, or replace the dataframe: spine_group . dataframe . show () spine_group . dataframe = new_df The spine can then be used to construct queries, with only one speciality: Note Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against. If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving. These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again. For example, to generate training data: X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = training_data_entities ) Or to get batches of fresh data for batch scoring: feature_view_spine . get_batch_data ( spine = scoring_entities_df ) . show () Here you have the chance to pass a different set of entities to generate the training dataset. Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column. feature_view . get_batch_data ( spine = spine_group ) Arguments name str : Name of the spine group to create. version Optional[int] : Version of the spine group to retrieve, defaults to None and will create the spine group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the spine group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the spine group won't have any primary key. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins. Defaults to None . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the spine group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . dataframe : DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Returns SpineGroup . The spine group metadata object. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] dataframe # Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. [source] deprecated # Setting if the feature group is deprecated. [source] embedding_index # [source] event_time # Event time feature in the feature group. [source] feature_store # [source] feature_store_id # [source] features # Feature Group schema (alias) [source] name # Name of the feature group. [source] notification_topic_name # The topic used for feature group notifications. [source] primary_key # List of features building the primary key. [source] schema # Feature Group schema [source] topic_name # The topic used for feature group data ingestion. [source] version # Version number of the feature group. Methods # [source] add_tag # SpineGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . add_tag ( name = \"example_tag\" , value = \"42\" ) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source] check_deprecated # SpineGroup . check_deprecated () [source] create_feature_monitoring # SpineGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] create_statistics_monitoring # SpineGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source] delete # SpineGroup . delete () Drop the entire feature group along with its feature data. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , version = 1 ) # delete the feature group fg . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises hsfs.client.exceptions.RestAPIError . [source] delete_tag # SpineGroup . delete_tag ( name ) Delete a tag attached to a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_tag ( \"example_tag\" ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source] filter # SpineGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: Example fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: Example fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] get_all_statistics # SpineGroup . get_all_statistics ( computation_time = None , feature_names = None ) Returns all the statistics metadata computed before a specific time for the current feature group. If computation_time is None , all the statistics metadata are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source] get_complex_features # SpineGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. Example complex_dtype_features = fg . get_complex_features () [source] get_feature # SpineGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get Feature instanse fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Arguments: name: The name of the feature to retrieve Returns: Feature: The feature object Raises hsfs.client.exceptions.FeatureStoreException . [source] get_feature_monitoring_configs # SpineGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source] get_feature_monitoring_history # SpineGroup . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fg . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # fetch feature monitoring history for a given feature monitoring config id fm_history = fg . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The start date of the feature monitoring history to fetch. Defaults to None. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source] get_fg_name # SpineGroup . get_fg_name () [source] get_generated_feature_groups # SpineGroup . get_generated_feature_groups () Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_generated_feature_views # SpineGroup . get_generated_feature_views () Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_parent_feature_groups # SpineGroup . get_parent_feature_groups () Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source] get_tag # SpineGroup . get_tag ( name ) Get the tags of a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_tag_value = fg . get_tag ( \"example_tag\" ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # SpineGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source] json # SpineGroup . json () [source] select # SpineGroup . select ( features ) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select ([ \"id\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features List[Union[str, hsfs.feature.Feature]] : A list of Feature objects or feature names as strings to be selected. Returns Query : A query object with the selected features of the feature group. [source] select_all # SpineGroup . select_all ( include_primary_key = True , include_event_time = True ) Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view. Example # connect to the Feature Store fs = ... # get the Feature Group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # show first 5 rows query . show ( 5 ) # select all features exclude primary key and event time from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) query = fg . select_all () query . features # [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)] query = fg . select_all ( include_primary_key = False , include_event_time = False ) query . features # [Feature('f1', ...), Feature('f2', ...)] Arguments include_primary_key Optional[bool] : If True, include primary key of the feature group to the feature list. Defaults to True. include_event_time Optional[bool] : If True, include event time of the feature group to the feature list. Defaults to True. Returns Query . A query object with all features of the feature group. [source] select_except # SpineGroup . select_except ( features = []) Select all features including primary key and event time feature of the feature group except provided features and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select_except ([ \"ts\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features Optional[List[Union[str, hsfs.feature.Feature]]] : A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] to_dict # SpineGroup . to_dict () [source] update_deprecated # SpineGroup . update_deprecated ( deprecate = True ) Deprecate the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_deprecated ( deprecate = True ) Safe update This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged. Arguments deprecate bool : Boolean value identifying if the feature group should be deprecated. Defaults to True. Returns FeatureGroup . The updated feature group object. [source] update_description # SpineGroup . update_description ( description ) Update the description of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_description ( description = \"Much better description.\" ) Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # SpineGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_feature_description ( feature_name = \"min_temp\" , description = \"Much better feature description.\" ) Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # SpineGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_notification_topic_name # SpineGroup . update_notification_topic_name ( notification_topic_name ) Update the notification topic name of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_notification_topic_name ( notification_topic_name = \"notification_topic_name\" ) Safe update This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name. Arguments notification_topic_name str : Name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If set to None no notifications are sent. Returns FeatureGroup . The updated feature group object.","title":"SpineGroup"},{"location":"generated/api/spine_group_api/#spinegroup","text":"[source]","title":"SpineGroup"},{"location":"generated/api/spine_group_api/#spinegroup_1","text":"hsfs . feature_group . SpineGroup ( storage_connector = None , query = None , data_format = None , path = None , options = {}, name = None , version = None , description = None , primary_key = None , featurestore_id = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , statistics_config = None , event_time = None , expectation_suite = None , online_enabled = False , href = None , online_topic_name = None , topic_name = None , spine = True , dataframe = \"spine\" , deprecated = False , ** kwargs )","title":"SpineGroup"},{"location":"generated/api/spine_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/spine_group_api/#get_or_create_spine_group","text":"FeatureStore . get_or_create_spine_group ( name , version = None , description = \"\" , primary_key = [], event_time = None , features = [], dataframe = None ) Create a spine group metadata object. Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins. Example # connect to the Feature Store fs = ... spine_df = pd . Dataframe () spine_group = fs . get_or_create_spine_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , dataframe = spine_df ) Note that you can inspect the dataframe in the spine group, or replace the dataframe: spine_group . dataframe . show () spine_group . dataframe = new_df The spine can then be used to construct queries, with only one speciality: Note Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against. If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving. These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again. For example, to generate training data: X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = training_data_entities ) Or to get batches of fresh data for batch scoring: feature_view_spine . get_batch_data ( spine = scoring_entities_df ) . show () Here you have the chance to pass a different set of entities to generate the training dataset. Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column. feature_view . get_batch_data ( spine = spine_group ) Arguments name str : Name of the spine group to create. version Optional[int] : Version of the spine group to retrieve, defaults to None and will create the spine group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the spine group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the spine group won't have any primary key. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins. Defaults to None . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the spine group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . dataframe : DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Returns SpineGroup . The spine group metadata object.","title":"get_or_create_spine_group"},{"location":"generated/api/spine_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/spine_group_api/#get_or_create_spine_group_1","text":"FeatureStore . get_or_create_spine_group ( name , version = None , description = \"\" , primary_key = [], event_time = None , features = [], dataframe = None ) Create a spine group metadata object. Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins. Example # connect to the Feature Store fs = ... spine_df = pd . Dataframe () spine_group = fs . get_or_create_spine_group ( name = \"sales\" , version = 1 , description = \"Physical shop sales features\" , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' , dataframe = spine_df ) Note that you can inspect the dataframe in the spine group, or replace the dataframe: spine_group . dataframe . show () spine_group . dataframe = new_df The spine can then be used to construct queries, with only one speciality: Note Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against. If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving. These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again. For example, to generate training data: X_train , X_test , y_train , y_test = feature_view_spine . train_test_split ( 0.2 , spine = training_data_entities ) Or to get batches of fresh data for batch scoring: feature_view_spine . get_batch_data ( spine = scoring_entities_df ) . show () Here you have the chance to pass a different set of entities to generate the training dataset. Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column. feature_view . get_batch_data ( spine = spine_group ) Arguments name str : Name of the spine group to create. version Optional[int] : Version of the spine group to retrieve, defaults to None and will create the spine group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the spine group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the spine group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the spine group won't have any primary key. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this spine group. If event_time is set the spine group can be used for point-in-time joins. Defaults to None . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the spine group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. Event time data type restriction The supported data types for the event time column are: timestamp , date and bigint . dataframe : DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. Returns SpineGroup . The spine group metadata object.","title":"get_or_create_spine_group"},{"location":"generated/api/spine_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/spine_group_api/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/api/spine_group_api/#dataframe","text":"Spine dataframe with primary key, event time and label column to use for point in time join when fetching features. [source]","title":"dataframe"},{"location":"generated/api/spine_group_api/#deprecated","text":"Setting if the feature group is deprecated. [source]","title":"deprecated"},{"location":"generated/api/spine_group_api/#embedding_index","text":"[source]","title":"embedding_index"},{"location":"generated/api/spine_group_api/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/api/spine_group_api/#feature_store","text":"[source]","title":"feature_store"},{"location":"generated/api/spine_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/spine_group_api/#features","text":"Feature Group schema (alias) [source]","title":"features"},{"location":"generated/api/spine_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/spine_group_api/#notification_topic_name","text":"The topic used for feature group notifications. [source]","title":"notification_topic_name"},{"location":"generated/api/spine_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/spine_group_api/#schema","text":"Feature Group schema [source]","title":"schema"},{"location":"generated/api/spine_group_api/#topic_name","text":"The topic used for feature group data ingestion. [source]","title":"topic_name"},{"location":"generated/api/spine_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/spine_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/spine_group_api/#add_tag","text":"SpineGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . add_tag ( name = \"example_tag\" , value = \"42\" ) Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/spine_group_api/#check_deprecated","text":"SpineGroup . check_deprecated () [source]","title":"check_deprecated"},{"location":"generated/api/spine_group_api/#create_feature_monitoring","text":"SpineGroup . create_feature_monitoring ( name , feature_name , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Enable feature monitoring to compare statistics on snapshots of feature data over time. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable feature monitoring my_config = fg . create_feature_monitoring ( name = \"my_monitoring_config\" , feature_name = \"my_feature\" , description = \"my monitoring config description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Data inserted in the last day time_offset = \"1d\" , window_length = \"1d\" , ) . with_reference_window ( # Data inserted last week on the same day time_offset = \"1w1d\" , window_length = \"1d\" , ) . compare_on ( metric = \"mean\" , threshold = 0.5 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name str : Name of the feature to monitor. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_feature_monitoring"},{"location":"generated/api/spine_group_api/#create_statistics_monitoring","text":"SpineGroup . create_statistics_monitoring ( name , feature_name = None , description = None , start_date_time = None , end_date_time = None , cron_expression = \"0 0 12 ? * * *\" , ) Run a job to compute statistics on snapshot of feature data on a schedule. Experimental Public API is subject to change, this feature is not suitable for production use-cases. Example # fetch feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # enable statistics monitoring my_config = fg . create_statistics_monitoring ( name = \"my_config\" , start_date_time = \"2021-01-01 00:00:00\" , description = \"my description\" , cron_expression = \"0 0 12 ? * * *\" , ) . with_detection_window ( # Statistics computed on 10% of the last week of data time_offset = \"1w\" , row_percentage = 0.1 , ) . save () Arguments name str : Name of the feature monitoring configuration. name must be unique for all configurations attached to the feature group. feature_name Optional[str] : Name of the feature to monitor. If not specified, statistics will be computed for all features. description Optional[str] : Description of the feature monitoring configuration. start_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : Start date and time from which to start computing statistics. end_date_time Optional[Union[str, int, datetime.date, datetime.datetime, pandas._libs.tslibs.timestamps.Timestamp]] : End date and time at which to stop computing statistics. cron_expression Optional[str] : Cron expression to use to schedule the job. The cron expression must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * ', every day at 12pm UTC. Raises hsfs.client.exceptions.FeatureStoreException . Return FeatureMonitoringConfig Configuration with minimal information about the feature monitoring. Additional information are required before feature monitoring is enabled. [source]","title":"create_statistics_monitoring"},{"location":"generated/api/spine_group_api/#delete","text":"SpineGroup . delete () Drop the entire feature group along with its feature data. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , version = 1 ) # delete the feature group fg . delete () Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete"},{"location":"generated/api/spine_group_api/#delete_tag","text":"SpineGroup . delete_tag ( name ) Delete a tag attached to a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . delete_tag ( \"example_tag\" ) Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/spine_group_api/#filter","text":"SpineGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. Example from hsfs.feature import Feature # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: Example fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: Example fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/spine_group_api/#get_all_statistics","text":"SpineGroup . get_all_statistics ( computation_time = None , feature_names = None ) Returns all the statistics metadata computed before a specific time for the current feature group. If computation_time is None , all the statistics metadata are returned. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_statistics = fg . get_statistics ( computation_time = None ) Arguments computation_time Optional[Union[str, int, float, datetime.datetime, datetime.date]] : Date and time when statistics were computed. Defaults to None . Strings should be formatted in one of the following formats %Y-%m-%d , %Y-%m-%d %H , %Y-%m-%d %H:%M , %Y-%m-%d %H:%M:%S , or %Y-%m-%d %H:%M:%S.%f . feature_names Optional[List[str]] : List of feature names of which statistics are retrieved. Returns Statistics . Statistics object. Raises hsfs.client.exceptions.RestAPIError hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_all_statistics"},{"location":"generated/api/spine_group_api/#get_complex_features","text":"SpineGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. Example complex_dtype_features = fg . get_complex_features () [source]","title":"get_complex_features"},{"location":"generated/api/spine_group_api/#get_feature","text":"SpineGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) # get Feature instanse fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Arguments: name: The name of the feature to retrieve Returns: Feature: The feature object Raises hsfs.client.exceptions.FeatureStoreException . [source]","title":"get_feature"},{"location":"generated/api/spine_group_api/#get_feature_monitoring_configs","text":"SpineGroup . get_feature_monitoring_configs ( name = None , feature_name = None , config_id = None ) Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch all feature monitoring configs attached to the feature group fm_configs = fg . get_feature_monitoring_configs () # fetch a single feature monitoring config by name fm_config = fg . get_feature_monitoring_configs ( name = \"my_config\" ) # fetch all feature monitoring configs attached to a particular feature fm_configs = fg . get_feature_monitoring_configs ( feature_name = \"my_feature\" ) # fetch a single feature monitoring config with a given id fm_config = fg . get_feature_monitoring_configs ( config_id = 1 ) Arguments name Optional[str] : If provided fetch only the feature monitoring config with the given name. Defaults to None. feature_name Optional[str] : If provided, fetch only configs attached to a particular feature. Defaults to None. config_id Optional[int] : If provided, fetch only the feature monitoring config with the given id. Defaults to None. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both name and feature_name are provided. - TypeError : if name or feature_name are not string or None. Return Union[ FeatureMonitoringConfig , List[ FeatureMonitoringConfig ], None] A list of feature monitoring configs. If name provided, returns either a single config or None if not found. [source]","title":"get_feature_monitoring_configs"},{"location":"generated/api/spine_group_api/#get_feature_monitoring_history","text":"SpineGroup . get_feature_monitoring_history ( config_name = None , config_id = None , start_time = None , end_time = None , with_statistics = True ) Fetch feature monitoring history for a given feature monitoring config. Example # fetch your feature group fg = fs . get_feature_group ( name = \"my_feature_group\" , version = 1 ) # fetch feature monitoring history for a given feature monitoring config fm_history = fg . get_feature_monitoring_history ( config_name = \"my_config\" , start_time = \"2020-01-01\" , ) # fetch feature monitoring history for a given feature monitoring config id fm_history = fg . get_feature_monitoring_history ( config_id = 1 , start_time = datetime . now () - timedelta ( weeks = 2 ), end_time = datetime . now () - timedelta ( weeks = 1 ), with_statistics = False , ) Arguments config_name Optional[str] : The name of the feature monitoring config to fetch history for. Defaults to None. config_id Optional[int] : The id of the feature monitoring config to fetch history for. Defaults to None. start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The start date of the feature monitoring history to fetch. Defaults to None. end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : The end date of the feature monitoring history to fetch. Defaults to None. with_statistics Optional[bool] : Whether to include statistics in the feature monitoring history. Defaults to True. If False, only metadata about the monitoring will be fetched. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . - ValueError : if both config_name and config_id are provided. - TypeError : if config_name or config_id are not respectively string, int or None. Return List[ FeatureMonitoringResult ] A list of feature monitoring results containing the monitoring metadata as well as the computed statistics for the detection and reference window if requested. [source]","title":"get_feature_monitoring_history"},{"location":"generated/api/spine_group_api/#get_fg_name","text":"SpineGroup . get_fg_name () [source]","title":"get_fg_name"},{"location":"generated/api/spine_group_api/#get_generated_feature_groups","text":"SpineGroup . get_generated_feature_groups () Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_generated_feature_groups"},{"location":"generated/api/spine_group_api/#get_generated_feature_views","text":"SpineGroup . get_generated_feature_views () Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_generated_feature_views"},{"location":"generated/api/spine_group_api/#get_parent_feature_groups","text":"SpineGroup . get_parent_feature_groups () Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned. Returns ProvenanceLinks : Object containing the section of provenance graph requested. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_parent_feature_groups"},{"location":"generated/api/spine_group_api/#get_tag","text":"SpineGroup . get_tag ( name ) Get the tags of a feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg_tag_value = fg . get_tag ( \"example_tag\" ) Arguments name str : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/spine_group_api/#get_tags","text":"SpineGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/spine_group_api/#json","text":"SpineGroup . json () [source]","title":"json"},{"location":"generated/api/spine_group_api/#select","text":"SpineGroup . select ( features ) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select ([ \"id\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features List[Union[str, hsfs.feature.Feature]] : A list of Feature objects or feature names as strings to be selected. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/spine_group_api/#select_all","text":"SpineGroup . select_all ( include_primary_key = True , include_event_time = True ) Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a feature view. Example # connect to the Feature Store fs = ... # get the Feature Group instances fg1 = fs . get_or_create_feature_group ( ... ) fg2 = fs . get_or_create_feature_group ( ... ) # construct the query query = fg1 . select_all () . join ( fg2 . select_all ()) # show first 5 rows query . show ( 5 ) # select all features exclude primary key and event time from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) query = fg . select_all () query . features # [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)] query = fg . select_all ( include_primary_key = False , include_event_time = False ) query . features # [Feature('f1', ...), Feature('f2', ...)] Arguments include_primary_key Optional[bool] : If True, include primary key of the feature group to the feature list. Defaults to True. include_event_time Optional[bool] : If True, include event time of the feature group to the feature list. Defaults to True. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/spine_group_api/#select_except","text":"SpineGroup . select_except ( features = []) Select all features including primary key and event time feature of the feature group except provided features and return a query object. The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance from hsfs.feature import Feature fg = fs . create_feature_group ( \"fg\" , features = [ Feature ( \"id\" , type = \"string\" ), Feature ( \"ts\" , type = \"bigint\" ), Feature ( \"f1\" , type = \"date\" ), Feature ( \"f2\" , type = \"double\" ) ], primary_key = [ \"id\" ], event_time = \"ts\" ) # construct query query = fg . select_except ([ \"ts\" , \"f1\" ]) query . features # [Feature('id', ...), Feature('f1', ...)] Arguments features Optional[List[Union[str, hsfs.feature.Feature]]] : A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/spine_group_api/#to_dict","text":"SpineGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/spine_group_api/#update_deprecated","text":"SpineGroup . update_deprecated ( deprecate = True ) Deprecate the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_deprecated ( deprecate = True ) Safe update This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged. Arguments deprecate bool : Boolean value identifying if the feature group should be deprecated. Defaults to True. Returns FeatureGroup . The updated feature group object. [source]","title":"update_deprecated"},{"location":"generated/api/spine_group_api/#update_description","text":"SpineGroup . update_description ( description ) Update the description of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_description ( description = \"Much better description.\" ) Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/spine_group_api/#update_feature_description","text":"SpineGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_feature_description ( feature_name = \"min_temp\" , description = \"Much better feature description.\" ) Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/api/spine_group_api/#update_features","text":"SpineGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/api/spine_group_api/#update_notification_topic_name","text":"SpineGroup . update_notification_topic_name ( notification_topic_name ) Update the notification topic name of the feature group. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) fg . update_notification_topic_name ( notification_topic_name = \"notification_topic_name\" ) Safe update This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name. Arguments notification_topic_name str : Name of the topic used for sending notifications when entries are inserted or updated on the online feature store. If set to None no notifications are sent. Returns FeatureGroup . The updated feature group object.","title":"update_notification_topic_name"},{"location":"generated/api/split_statistics_api/","text":"Split Statistics # [source] SplitStatistics # hsfs . split_statistics . SplitStatistics ( name , feature_descriptive_statistics , href = None , expand = None , items = None , count = None , type = None , ** kwargs ) Properties # [source] feature_descriptive_statistics # List of feature descriptive statistics. [source] name # Name of the training dataset split.","title":"Split Statistics"},{"location":"generated/api/split_statistics_api/#split-statistics","text":"[source]","title":"Split Statistics"},{"location":"generated/api/split_statistics_api/#splitstatistics","text":"hsfs . split_statistics . SplitStatistics ( name , feature_descriptive_statistics , href = None , expand = None , items = None , count = None , type = None , ** kwargs )","title":"SplitStatistics"},{"location":"generated/api/split_statistics_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/split_statistics_api/#feature_descriptive_statistics","text":"List of feature descriptive statistics. [source]","title":"feature_descriptive_statistics"},{"location":"generated/api/split_statistics_api/#name","text":"Name of the training dataset split.","title":"name"},{"location":"generated/api/statistics_api/","text":"Statistics # [source] Statistics # hsfs . statistics . Statistics ( computation_time , row_percentage = 1.0 , feature_descriptive_statistics = None , feature_group_id = None , window_start_commit_time = None , window_end_commit_time = None , feature_view_name = None , feature_view_version = None , training_dataset_version = None , split_statistics = None , before_transformation = False , href = None , expand = None , items = None , count = None , type = None , ** kwargs ) Properties # [source] before_transformation # Whether or not the statistics were computed on feature values before applying model-dependent transformations. [source] computation_time # Time at which the statistics were computed. [source] feature_descriptive_statistics # List of feature descriptive statistics. [source] feature_group_id # Id of the feature group on whose data the statistics were computed. [source] feature_view_name # Name of the feature view whose query was used to retrieve the data on which the statistics were computed. [source] feature_view_version # Id of the feature view whose query was used to retrieve the data on which the statistics were computed. [source] row_percentage # Percentage of data on which statistics were computed. [source] split_statistics # List of statistics computed on each split of a training dataset. [source] training_dataset_version # Version of the training dataset on which statistics were computed. [source] window_end_commit_time # End time of the window of data on which statistics were computed. [source] window_start_commit_time # Start time of the window of data on which statistics were computed.","title":"Statistics"},{"location":"generated/api/statistics_api/#statistics","text":"[source]","title":"Statistics"},{"location":"generated/api/statistics_api/#statistics_1","text":"hsfs . statistics . Statistics ( computation_time , row_percentage = 1.0 , feature_descriptive_statistics = None , feature_group_id = None , window_start_commit_time = None , window_end_commit_time = None , feature_view_name = None , feature_view_version = None , training_dataset_version = None , split_statistics = None , before_transformation = False , href = None , expand = None , items = None , count = None , type = None , ** kwargs )","title":"Statistics"},{"location":"generated/api/statistics_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/statistics_api/#before_transformation","text":"Whether or not the statistics were computed on feature values before applying model-dependent transformations. [source]","title":"before_transformation"},{"location":"generated/api/statistics_api/#computation_time","text":"Time at which the statistics were computed. [source]","title":"computation_time"},{"location":"generated/api/statistics_api/#feature_descriptive_statistics","text":"List of feature descriptive statistics. [source]","title":"feature_descriptive_statistics"},{"location":"generated/api/statistics_api/#feature_group_id","text":"Id of the feature group on whose data the statistics were computed. [source]","title":"feature_group_id"},{"location":"generated/api/statistics_api/#feature_view_name","text":"Name of the feature view whose query was used to retrieve the data on which the statistics were computed. [source]","title":"feature_view_name"},{"location":"generated/api/statistics_api/#feature_view_version","text":"Id of the feature view whose query was used to retrieve the data on which the statistics were computed. [source]","title":"feature_view_version"},{"location":"generated/api/statistics_api/#row_percentage","text":"Percentage of data on which statistics were computed. [source]","title":"row_percentage"},{"location":"generated/api/statistics_api/#split_statistics","text":"List of statistics computed on each split of a training dataset. [source]","title":"split_statistics"},{"location":"generated/api/statistics_api/#training_dataset_version","text":"Version of the training dataset on which statistics were computed. [source]","title":"training_dataset_version"},{"location":"generated/api/statistics_api/#window_end_commit_time","text":"End time of the window of data on which statistics were computed. [source]","title":"window_end_commit_time"},{"location":"generated/api/statistics_api/#window_start_commit_time","text":"Start time of the window of data on which statistics were computed.","title":"window_start_commit_time"},{"location":"generated/api/statistics_config_api/","text":"StatisticsConfig # [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [], ** kwargs ) Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] exact_uniqueness # Enable exact uniqueness as an additional statistic to be computed for each feature. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig","text":"[source]","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [], ** kwargs )","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/statistics_config_api/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/api/statistics_config_api/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/api/statistics_config_api/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/api/statistics_config_api/#exact_uniqueness","text":"Enable exact uniqueness as an additional statistic to be computed for each feature. [source]","title":"exact_uniqueness"},{"location":"generated/api/statistics_config_api/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/api/storage_connector_api/","text":"Storage Connector # Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Example # connect to the Feature Store fs = ... sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Example # connect to the Feature Store fs = ... online_storage_connector = fs . get_online_storage_connector () Returns StorageConnector . JDBC storage connector to the Online Feature Store. HopsFS # Properties # [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] connector_options # HopsFSConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] read # HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. Arguments query Optional[str] : By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to None . data_format Optional[str] : When reading from object stores such as S3, HopsFS and ADLS, specify the file format to be read, e.g. csv , parquet . options dict : Any additional key/value options to be passed to the connector. path Optional[str] : Path to be read from within the bucket of the storage connector. Not relevant for JDBC or database based connectors such as Snowflake, JDBC or Redshift. Returns DataFrame . [source] refetch # HopsFSConnector . refetch () Refetch storage connector. [source] spark_options # HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # HopsFSConnector . to_dict () [source] update_from_response_json # HopsFSConnector . update_from_response_json ( json_dict ) JDBC # Properties # [source] arguments # Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source] connection_string # JDBC connection string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] connector_options # JdbcConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] read # JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. Arguments query str : A SQL query to be read. data_format Optional[str] : Not relevant for JDBC based connectors. options dict : Any additional key/value options to be passed to the JDBC connector. path Optional[str] : Not relevant for JDBC based connectors. Returns DataFrame . [source] refetch # JdbcConnector . refetch () Refetch storage connector. [source] spark_options # JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # JdbcConnector . to_dict () [source] update_from_response_json # JdbcConnector . update_from_response_json ( json_dict ) S3 # Properties # [source] access_key # Access key. [source] arguments # [source] bucket # Return the bucket for S3 connectors. [source] description # User provided description of the storage connector. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] session_token # Session token. Methods # [source] connector_options # S3Connector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] prepare_spark # S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # S3Connector . read ( query = None , data_format = None , options = {}, path = \"\" ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. Arguments query Optional[str] : Not relevant for S3 connectors. data_format Optional[str] : The file format of the files to be read, e.g. csv , parquet . options dict : Any additional key/value options to be passed to the S3 connector. path str : Path within the bucket to be read. Returns DataFrame . [source] refetch # S3Connector . refetch () Refetch storage connector. [source] spark_options # S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # S3Connector . to_dict () [source] update_from_response_json # S3Connector . update_from_response_json ( json_dict ) Redshift # Properties # [source] arguments # Additional JDBC, REDSHIFT, or Snowflake arguments. [source] auto_create # Database username for redshift cluster. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] description # User provided description of the storage connector. [source] expiration # Cluster temporary credential expiration time. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] table_name # Table name for redshift cluster. Methods # [source] connector_options # RedshiftConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] read # RedshiftConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a table or query into a dataframe using the storage connector. Arguments query Optional[str] : By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to None . data_format Optional[str] : Not relevant for JDBC based connectors such as Redshift. options dict : Any additional key/value options to be passed to the JDBC connector. path Optional[str] : Not relevant for JDBC based connectors such as Redshift. Returns DataFrame . [source] refetch # RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # RedshiftConnector . to_dict () [source] update_from_response_json # RedshiftConnector . update_from_response_json ( json_dict ) Azure Data Lake Storage # Properties # [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] container_name # Container name of the ADLS storage connector [source] description # User provided description of the storage connector. [source] directory_id # Directory ID of the ADLS storage connector [source] generation # Generation of the ADLS storage connector [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. ADLS) - return the path of the connector [source] service_credential # Service credential of the ADLS storage connector Methods # [source] connector_options # AdlsConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] prepare_spark # AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # AdlsConnector . read ( query = None , data_format = None , options = {}, path = \"\" ) Reads a path into a dataframe using the storage connector. Arguments query Optional[str] : Not relevant for ADLS connectors. data_format Optional[str] : The file format of the files to be read, e.g. csv , parquet . options dict : Any additional key/value options to be passed to the ADLS connector. path str : Path within the bucket to be read. For example, path= path will read directly from the container specified on connector by constructing the URI as 'abfss://[container-name]@[account_name].dfs.core.windows.net/[path]'. If no path is specified default container path will be used from connector. Returns DataFrame . [source] refetch # AdlsConnector . refetch () Refetch storage connector. [source] spark_options # AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # AdlsConnector . to_dict () [source] update_from_response_json # AdlsConnector . update_from_response_json ( json_dict ) Snowflake # Properties # [source] account # Account of the Snowflake storage connector [source] application # Application of the Snowflake storage connector [source] database # Database of the Snowflake storage connector [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Additional options for the Snowflake storage connector [source] password # Password of the Snowflake storage connector [source] role # Role of the Snowflake storage connector [source] schema # Schema of the Snowflake storage connector [source] table # Table of the Snowflake storage connector [source] token # OAuth token of the Snowflake storage connector [source] url # URL of the Snowflake storage connector [source] user # User of the Snowflake storage connector [source] warehouse # Warehouse of the Snowflake storage connector Methods # [source] connector_options # SnowflakeConnector . connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . connector_options ()) [source] read # SnowflakeConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a table or query into a dataframe using the storage connector. Arguments query Optional[str] : By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to None . data_format Optional[str] : Not relevant for Snowflake connectors. options dict : Any additional key/value options to be passed to the engine. path Optional[str] : Not relevant for Snowflake connectors. Returns DataFrame . [source] refetch # SnowflakeConnector . refetch () Refetch storage connector. [source] snowflake_connector_options # SnowflakeConnector . snowflake_connector_options () Alias for connector_options [source] spark_options # SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # SnowflakeConnector . to_dict () [source] update_from_response_json # SnowflakeConnector . update_from_response_json ( json_dict ) Google Cloud Storage # This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Properties # [source] algorithm # Encryption Algorithm [source] bucket # GCS Bucket [source] description # User provided description of the storage connector. [source] encryption_key # Encryption Key [source] encryption_key_hash # Encryption Key Hash [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] name # Name of the storage connector. [source] path # the path of the connector along with gs file system prefixed Methods # [source] connector_options # GcsConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] prepare_spark # GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source] read # GcsConnector . read ( query = None , data_format = None , options = {}, path = \"\" ) Reads GCS path into a dataframe using the storage connector. To read directly from the default bucket, you can omit the path argument: conn . read ( data_format = 'spark_formats' ) Or to read objects from default bucket provide the object path without gsUtil URI schema. For example, following will read from a path gs://bucket_on_connector/Path/object : conn . read ( data_format = 'spark_formats' , paths = 'Path/object' ) Or to read with full gsUtil URI path, conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments query Optional[str] : Not relevant for GCS connectors. data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path str : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # GcsConnector . refetch () Refetch storage connector. [source] spark_options # GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # GcsConnector . to_dict () [source] update_from_response_json # GcsConnector . update_from_response_json ( json_dict ) BigQuery # The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Properties # [source] arguments # Additional spark options [source] dataset # BigQuery dataset (The dataset containing the table) [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] materialization_dataset # BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source] name # Name of the storage connector. [source] parent_project # BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source] query_project # BigQuery project (The Google Cloud Project ID of the table) [source] query_table # BigQuery table name Methods # [source] connector_options # BigQueryConnector . connector_options () Return options to be passed to an external BigQuery connector library [source] read # BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # BigQueryConnector . refetch () Refetch storage connector. [source] spark_options # BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source] to_dict # BigQueryConnector . to_dict () [source] update_from_response_json # BigQueryConnector . update_from_response_json ( json_dict ) Kafka # Properties # [source] bootstrap_servers # Bootstrap servers string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Bootstrap servers string. [source] security_protocol # Bootstrap servers string. [source] ssl_endpoint_identification_algorithm # Bootstrap servers string. [source] ssl_keystore_location # Bootstrap servers string. [source] ssl_truststore_location # Bootstrap servers string. Methods # [source] confluent_options # KafkaConnector . confluent_options () Return prepared options to be passed to confluent_kafka, based on the provided apache spark configuration. Right now only producer values with Importance >= medium are implemented. https://docs.confluent.io/platform/current/clients/librdkafka/html/md_CONFIGURATION.html [source] connector_options # KafkaConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source] kafka_options # KafkaConnector . kafka_options () Return prepared options to be passed to kafka, based on the additional arguments. https://kafka.apache.org/documentation/ [source] read # KafkaConnector . read ( query = None , data_format = None , options = {}, path = None ) NOT SUPPORTED. [source] read_stream # KafkaConnector . read_stream ( topic , topic_pattern = False , message_format = \"avro\" , schema = None , options = {}, include_metadata = False , ) Reads a Kafka stream from a topic or multiple topics into a Dataframe. Engine Support Spark only Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Arguments topic str : Name or pattern of the topic(s) to subscribe to. topic_pattern bool : Flag to indicate if topic string is a pattern. Defaults to False . message_format str : The format of the messages to use for decoding. Can be \"avro\" or \"json\" . Defaults to \"avro\" . schema Optional[str] : Optional schema, to use for decoding, can be an Avro schema string for \"avro\" message format, or for JSON encoding a Spark StructType schema, or a DDL formatted string. Defaults to None . options dict : Additional options as key/value string pairs to be passed to Spark. Defaults to {} . include_metadata bool : Indicate whether to return additional metadata fields from messages in the stream. Otherwise, only the decoded value fields are returned. Defaults to False . Raises ValueError : Malformed arguments. Returns StreamingDataframe : A Spark streaming dataframe. [source] refetch # KafkaConnector . refetch () Refetch storage connector. [source] spark_options # KafkaConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. This is done by just adding 'kafka.' prefix to kafka_options. https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations [source] to_dict # KafkaConnector . to_dict () [source] update_from_response_json # KafkaConnector . update_from_response_json ( json_dict )","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#storage-connector","text":"","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/storage_connector_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Example # connect to the Feature Store fs = ... sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Example # connect to the Feature Store fs = ... online_storage_connector = fs . get_online_storage_connector () Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/api/storage_connector_api/#hopsfs","text":"","title":"HopsFS"},{"location":"generated/api/storage_connector_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#description","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options","text":"HopsFSConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#read","text":"HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. Arguments query Optional[str] : By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to None . data_format Optional[str] : When reading from object stores such as S3, HopsFS and ADLS, specify the file format to be read, e.g. csv , parquet . options dict : Any additional key/value options to be passed to the connector. path Optional[str] : Path to be read from within the bucket of the storage connector. Not relevant for JDBC or database based connectors such as Snowflake, JDBC or Redshift. Returns DataFrame . [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch","text":"HopsFSConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options","text":"HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict","text":"HopsFSConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json","text":"HopsFSConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#jdbc","text":"","title":"JDBC"},{"location":"generated/api/storage_connector_api/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments","text":"Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/api/storage_connector_api/#description_1","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_1","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_1","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods_1","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_1","text":"JdbcConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#read_1","text":"JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. Arguments query str : A SQL query to be read. data_format Optional[str] : Not relevant for JDBC based connectors. options dict : Any additional key/value options to be passed to the JDBC connector. path Optional[str] : Not relevant for JDBC based connectors. Returns DataFrame . [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_1","text":"JdbcConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_1","text":"JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_1","text":"JdbcConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_1","text":"JdbcConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#s3","text":"","title":"S3"},{"location":"generated/api/storage_connector_api/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/api/storage_connector_api/#arguments_1","text":"[source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_2","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_2","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_2","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/api/storage_connector_api/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/api/storage_connector_api/#session_token","text":"Session token.","title":"session_token"},{"location":"generated/api/storage_connector_api/#methods_2","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_2","text":"S3Connector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#prepare_spark","text":"S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_2","text":"S3Connector . read ( query = None , data_format = None , options = {}, path = \"\" ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. Arguments query Optional[str] : Not relevant for S3 connectors. data_format Optional[str] : The file format of the files to be read, e.g. csv , parquet . options dict : Any additional key/value options to be passed to the S3 connector. path str : Path within the bucket to be read. Returns DataFrame . [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_2","text":"S3Connector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_2","text":"S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_2","text":"S3Connector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_2","text":"S3Connector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#redshift","text":"","title":"Redshift"},{"location":"generated/api/storage_connector_api/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_2","text":"Additional JDBC, REDSHIFT, or Snowflake arguments. [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/api/storage_connector_api/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/api/storage_connector_api/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/api/storage_connector_api/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/api/storage_connector_api/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/api/storage_connector_api/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/api/storage_connector_api/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/api/storage_connector_api/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/api/storage_connector_api/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/api/storage_connector_api/#description_3","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/api/storage_connector_api/#iam_role_1","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_3","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_3","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/api/storage_connector_api/#methods_3","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_3","text":"RedshiftConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#read_3","text":"RedshiftConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a table or query into a dataframe using the storage connector. Arguments query Optional[str] : By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to None . data_format Optional[str] : Not relevant for JDBC based connectors such as Redshift. options dict : Any additional key/value options to be passed to the JDBC connector. path Optional[str] : Not relevant for JDBC based connectors such as Redshift. Returns DataFrame . [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_3","text":"RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_3","text":"RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_3","text":"RedshiftConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_3","text":"RedshiftConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#azure-data-lake-storage","text":"","title":"Azure Data Lake Storage"},{"location":"generated/api/storage_connector_api/#properties_4","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/api/storage_connector_api/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/api/storage_connector_api/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/api/storage_connector_api/#description_4","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/api/storage_connector_api/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/api/storage_connector_api/#id_4","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_4","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_1","text":"If the connector refers to a path (e.g. ADLS) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#service_credential","text":"Service credential of the ADLS storage connector","title":"service_credential"},{"location":"generated/api/storage_connector_api/#methods_4","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_4","text":"AdlsConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#prepare_spark_1","text":"AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_4","text":"AdlsConnector . read ( query = None , data_format = None , options = {}, path = \"\" ) Reads a path into a dataframe using the storage connector. Arguments query Optional[str] : Not relevant for ADLS connectors. data_format Optional[str] : The file format of the files to be read, e.g. csv , parquet . options dict : Any additional key/value options to be passed to the ADLS connector. path str : Path within the bucket to be read. For example, path= path will read directly from the container specified on connector by constructing the URI as 'abfss://[container-name]@[account_name].dfs.core.windows.net/[path]'. If no path is specified default container path will be used from connector. Returns DataFrame . [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_4","text":"AdlsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_4","text":"AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_4","text":"AdlsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_4","text":"AdlsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#snowflake","text":"","title":"Snowflake"},{"location":"generated/api/storage_connector_api/#properties_5","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account","text":"Account of the Snowflake storage connector [source]","title":"account"},{"location":"generated/api/storage_connector_api/#application","text":"Application of the Snowflake storage connector [source]","title":"application"},{"location":"generated/api/storage_connector_api/#database","text":"Database of the Snowflake storage connector [source]","title":"database"},{"location":"generated/api/storage_connector_api/#description_5","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_5","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_5","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#options","text":"Additional options for the Snowflake storage connector [source]","title":"options"},{"location":"generated/api/storage_connector_api/#password","text":"Password of the Snowflake storage connector [source]","title":"password"},{"location":"generated/api/storage_connector_api/#role","text":"Role of the Snowflake storage connector [source]","title":"role"},{"location":"generated/api/storage_connector_api/#schema","text":"Schema of the Snowflake storage connector [source]","title":"schema"},{"location":"generated/api/storage_connector_api/#table","text":"Table of the Snowflake storage connector [source]","title":"table"},{"location":"generated/api/storage_connector_api/#token","text":"OAuth token of the Snowflake storage connector [source]","title":"token"},{"location":"generated/api/storage_connector_api/#url","text":"URL of the Snowflake storage connector [source]","title":"url"},{"location":"generated/api/storage_connector_api/#user","text":"User of the Snowflake storage connector [source]","title":"user"},{"location":"generated/api/storage_connector_api/#warehouse","text":"Warehouse of the Snowflake storage connector","title":"warehouse"},{"location":"generated/api/storage_connector_api/#methods_5","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_5","text":"SnowflakeConnector . connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . connector_options ()) [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#read_5","text":"SnowflakeConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a table or query into a dataframe using the storage connector. Arguments query Optional[str] : By default, the storage connector will read the table configured together with the connector, if any. It's possible to overwrite this by passing a SQL query here. Defaults to None . data_format Optional[str] : Not relevant for Snowflake connectors. options dict : Any additional key/value options to be passed to the engine. path Optional[str] : Not relevant for Snowflake connectors. Returns DataFrame . [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_5","text":"SnowflakeConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#snowflake_connector_options","text":"SnowflakeConnector . snowflake_connector_options () Alias for connector_options [source]","title":"snowflake_connector_options"},{"location":"generated/api/storage_connector_api/#spark_options_5","text":"SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_5","text":"SnowflakeConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_5","text":"SnowflakeConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#google-cloud-storage","text":"This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop","title":"Google Cloud Storage"},{"location":"generated/api/storage_connector_api/#properties_6","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#algorithm","text":"Encryption Algorithm [source]","title":"algorithm"},{"location":"generated/api/storage_connector_api/#bucket_1","text":"GCS Bucket [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_6","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#encryption_key","text":"Encryption Key [source]","title":"encryption_key"},{"location":"generated/api/storage_connector_api/#encryption_key_hash","text":"Encryption Key Hash [source]","title":"encryption_key_hash"},{"location":"generated/api/storage_connector_api/#id_6","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#key_path","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/api/storage_connector_api/#name_6","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_2","text":"the path of the connector along with gs file system prefixed","title":"path"},{"location":"generated/api/storage_connector_api/#methods_6","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_6","text":"GcsConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#prepare_spark_2","text":"GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_6","text":"GcsConnector . read ( query = None , data_format = None , options = {}, path = \"\" ) Reads GCS path into a dataframe using the storage connector. To read directly from the default bucket, you can omit the path argument: conn . read ( data_format = 'spark_formats' ) Or to read objects from default bucket provide the object path without gsUtil URI schema. For example, following will read from a path gs://bucket_on_connector/Path/object : conn . read ( data_format = 'spark_formats' , paths = 'Path/object' ) Or to read with full gsUtil URI path, conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments query Optional[str] : Not relevant for GCS connectors. data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path str : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_6","text":"GcsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_6","text":"GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_6","text":"GcsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_6","text":"GcsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#bigquery","text":"The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.","title":"BigQuery"},{"location":"generated/api/storage_connector_api/#properties_7","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_3","text":"Additional spark options [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#dataset","text":"BigQuery dataset (The dataset containing the table) [source]","title":"dataset"},{"location":"generated/api/storage_connector_api/#description_7","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_7","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#key_path_1","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/api/storage_connector_api/#materialization_dataset","text":"BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source]","title":"materialization_dataset"},{"location":"generated/api/storage_connector_api/#name_7","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#parent_project","text":"BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source]","title":"parent_project"},{"location":"generated/api/storage_connector_api/#query_project","text":"BigQuery project (The Google Cloud Project ID of the table) [source]","title":"query_project"},{"location":"generated/api/storage_connector_api/#query_table","text":"BigQuery table name","title":"query_table"},{"location":"generated/api/storage_connector_api/#methods_7","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#connector_options_7","text":"BigQueryConnector . connector_options () Return options to be passed to an external BigQuery connector library [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#read_7","text":"BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_7","text":"BigQueryConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_7","text":"BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_7","text":"BigQueryConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_7","text":"BigQueryConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#kafka","text":"","title":"Kafka"},{"location":"generated/api/storage_connector_api/#properties_8","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#bootstrap_servers","text":"Bootstrap servers string. [source]","title":"bootstrap_servers"},{"location":"generated/api/storage_connector_api/#description_8","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_8","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_8","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#options_1","text":"Bootstrap servers string. [source]","title":"options"},{"location":"generated/api/storage_connector_api/#security_protocol","text":"Bootstrap servers string. [source]","title":"security_protocol"},{"location":"generated/api/storage_connector_api/#ssl_endpoint_identification_algorithm","text":"Bootstrap servers string. [source]","title":"ssl_endpoint_identification_algorithm"},{"location":"generated/api/storage_connector_api/#ssl_keystore_location","text":"Bootstrap servers string. [source]","title":"ssl_keystore_location"},{"location":"generated/api/storage_connector_api/#ssl_truststore_location","text":"Bootstrap servers string.","title":"ssl_truststore_location"},{"location":"generated/api/storage_connector_api/#methods_8","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#confluent_options","text":"KafkaConnector . confluent_options () Return prepared options to be passed to confluent_kafka, based on the provided apache spark configuration. Right now only producer values with Importance >= medium are implemented. https://docs.confluent.io/platform/current/clients/librdkafka/html/md_CONFIGURATION.html [source]","title":"confluent_options"},{"location":"generated/api/storage_connector_api/#connector_options_8","text":"KafkaConnector . connector_options () Return prepared options to be passed to an external connector library. Not implemented for this connector type. [source]","title":"connector_options"},{"location":"generated/api/storage_connector_api/#kafka_options","text":"KafkaConnector . kafka_options () Return prepared options to be passed to kafka, based on the additional arguments. https://kafka.apache.org/documentation/ [source]","title":"kafka_options"},{"location":"generated/api/storage_connector_api/#read_8","text":"KafkaConnector . read ( query = None , data_format = None , options = {}, path = None ) NOT SUPPORTED. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#read_stream","text":"KafkaConnector . read_stream ( topic , topic_pattern = False , message_format = \"avro\" , schema = None , options = {}, include_metadata = False , ) Reads a Kafka stream from a topic or multiple topics into a Dataframe. Engine Support Spark only Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Arguments topic str : Name or pattern of the topic(s) to subscribe to. topic_pattern bool : Flag to indicate if topic string is a pattern. Defaults to False . message_format str : The format of the messages to use for decoding. Can be \"avro\" or \"json\" . Defaults to \"avro\" . schema Optional[str] : Optional schema, to use for decoding, can be an Avro schema string for \"avro\" message format, or for JSON encoding a Spark StructType schema, or a DDL formatted string. Defaults to None . options dict : Additional options as key/value string pairs to be passed to Spark. Defaults to {} . include_metadata bool : Indicate whether to return additional metadata fields from messages in the stream. Otherwise, only the decoded value fields are returned. Defaults to False . Raises ValueError : Malformed arguments. Returns StreamingDataframe : A Spark streaming dataframe. [source]","title":"read_stream"},{"location":"generated/api/storage_connector_api/#refetch_8","text":"KafkaConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_8","text":"KafkaConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. This is done by just adding 'kafka.' prefix to kafka_options. https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_8","text":"KafkaConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_8","text":"KafkaConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/","text":"Training Dataset # [source] TrainingDataset # hsfs . training_dataset . TrainingDataset ( name , version , data_format , featurestore_id , location = \"\" , event_start_time = None , event_end_time = None , coalesce = False , description = None , storage_connector = None , splits = None , validation_size = None , test_size = None , train_start = None , train_end = None , validation_start = None , validation_end = None , test_start = None , test_end = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , train_split = None , time_split_size = None , extra_filter = None , ** kwargs ) Creation # [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] coalesce # If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source] data_format # File format of the training dataset. [source] description # [source] event_end_time # [source] event_start_time # [source] extra_filter # [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get computed statistics for the training dataset. Returns Statistics . Object with statistics information. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] test_end # [source] test_size # [source] test_start # [source] train_end # [source] train_split # Set name of training dataset split that is used for training. [source] train_start # [source] training_dataset_type # [source] transformation_functions # Set transformation functions. [source] validation_end # [source] validation_size # [source] validation_start # [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source] compute_statistics # TrainingDataset . compute_statistics () Compute the statistics for the training dataset and save them to the feature store. [source] delete # TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises hsfs.client.exceptions.RestAPIError . [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source] from_response_json # TrainingDataset . from_response_json ( json_dict ) [source] from_response_json_single # TrainingDataset . from_response_json_single ( json_dict ) [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_serving_vector # TrainingDataset . get_serving_vector ( entry , external = None ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_serving_vectors # TrainingDataset . get_serving_vectors ( entry , external = None ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source] init_prepared_statement # TrainingDataset . init_prepared_statement ( batch = None , external = None ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. Deprecated insert method is deprecated. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises hsfs.client.exceptions.RestAPIError : Unable to create training dataset metadata. [source] json # TrainingDataset . json () [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Engine Support Creating Training Datasets from Dataframes is only supported using Spark as Engine. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises hsfs.client.exceptions.RestAPIError : Unable to create training dataset metadata. [source] serving_keys # TrainingDataset . serving_keys () Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] to_dict # TrainingDataset . to_dict () [source] update_from_response_json # TrainingDataset . update_from_response_json ( json_dict ) [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises hsfs.client.exceptions.RestAPIError .","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#training-dataset","text":"[source]","title":"Training Dataset"},{"location":"generated/api/training_dataset_api/#trainingdataset","text":"hsfs . training_dataset . TrainingDataset ( name , version , data_format , featurestore_id , location = \"\" , event_start_time = None , event_end_time = None , coalesce = False , description = None , storage_connector = None , splits = None , validation_size = None , test_size = None , train_start = None , train_end = None , validation_start = None , validation_end = None , test_start = None , test_end = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , train_split = None , time_split_size = None , extra_filter = None , ** kwargs )","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/training_dataset_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/api/training_dataset_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/training_dataset_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises hsfs.client.exceptions.RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/api/training_dataset_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/training_dataset_api/#coalesce","text":"If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source]","title":"coalesce"},{"location":"generated/api/training_dataset_api/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/api/training_dataset_api/#description","text":"[source]","title":"description"},{"location":"generated/api/training_dataset_api/#event_end_time","text":"[source]","title":"event_end_time"},{"location":"generated/api/training_dataset_api/#event_start_time","text":"[source]","title":"event_start_time"},{"location":"generated/api/training_dataset_api/#extra_filter","text":"[source]","title":"extra_filter"},{"location":"generated/api/training_dataset_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/training_dataset_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/training_dataset_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/training_dataset_api/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/api/training_dataset_api/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/api/training_dataset_api/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/api/training_dataset_api/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/api/training_dataset_api/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/api/training_dataset_api/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/api/training_dataset_api/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/api/training_dataset_api/#statistics","text":"Get computed statistics for the training dataset. Returns Statistics . Object with statistics information. [source]","title":"statistics"},{"location":"generated/api/training_dataset_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/api/training_dataset_api/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/api/training_dataset_api/#test_end","text":"[source]","title":"test_end"},{"location":"generated/api/training_dataset_api/#test_size","text":"[source]","title":"test_size"},{"location":"generated/api/training_dataset_api/#test_start","text":"[source]","title":"test_start"},{"location":"generated/api/training_dataset_api/#train_end","text":"[source]","title":"train_end"},{"location":"generated/api/training_dataset_api/#train_split","text":"Set name of training dataset split that is used for training. [source]","title":"train_split"},{"location":"generated/api/training_dataset_api/#train_start","text":"[source]","title":"train_start"},{"location":"generated/api/training_dataset_api/#training_dataset_type","text":"[source]","title":"training_dataset_type"},{"location":"generated/api/training_dataset_api/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/api/training_dataset_api/#validation_end","text":"[source]","title":"validation_end"},{"location":"generated/api/training_dataset_api/#validation_size","text":"[source]","title":"validation_size"},{"location":"generated/api/training_dataset_api/#validation_start","text":"[source]","title":"validation_start"},{"location":"generated/api/training_dataset_api/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/api/training_dataset_api/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/api/training_dataset_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/training_dataset_api/#add_tag","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/training_dataset_api/#compute_statistics","text":"TrainingDataset . compute_statistics () Compute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/api/training_dataset_api/#delete","text":"TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"delete"},{"location":"generated/api/training_dataset_api/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/training_dataset_api/#from_response_json","text":"TrainingDataset . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/training_dataset_api/#from_response_json_single","text":"TrainingDataset . from_response_json_single ( json_dict ) [source]","title":"from_response_json_single"},{"location":"generated/api/training_dataset_api/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/api/training_dataset_api/#get_serving_vector","text":"TrainingDataset . get_serving_vector ( entry , external = None ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vector"},{"location":"generated/api/training_dataset_api/#get_serving_vectors","text":"TrainingDataset . get_serving_vectors ( entry , external = None ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vectors"},{"location":"generated/api/training_dataset_api/#get_tag","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/training_dataset_api/#get_tags","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises hsfs.client.exceptions.RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/training_dataset_api/#init_prepared_statement","text":"TrainingDataset . init_prepared_statement ( batch = None , external = None ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. [source]","title":"init_prepared_statement"},{"location":"generated/api/training_dataset_api/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. Deprecated insert method is deprecated. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises hsfs.client.exceptions.RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/api/training_dataset_api/#json","text":"TrainingDataset . json () [source]","title":"json"},{"location":"generated/api/training_dataset_api/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/api/training_dataset_api/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Engine Support Creating Training Datasets from Dataframes is only supported using Spark as Engine. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises hsfs.client.exceptions.RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/api/training_dataset_api/#serving_keys","text":"TrainingDataset . serving_keys () Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source]","title":"serving_keys"},{"location":"generated/api/training_dataset_api/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/api/training_dataset_api/#to_dict","text":"TrainingDataset . to_dict () [source]","title":"to_dict"},{"location":"generated/api/training_dataset_api/#update_from_response_json","text":"TrainingDataset . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises hsfs.client.exceptions.RestAPIError .","title":"update_statistics_config"},{"location":"generated/api/transformation_functions_api/","text":"Transformation Function # [source] TransformationFunction # hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ** kwargs ) Properties # [source] id # Training dataset id. [source] name # [source] output_type # [source] source_code_content # [source] transformation_fn # [source] transformer_code # [source] version # Methods # [source] delete # TransformationFunction . delete () Delete transformation function from backend. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () # retrieve transformation function plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) # delete transformation function from backend plus_one_fn . delete () [source] save # TransformationFunction . save () Persist transformation function in backend. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () Creation # [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. Retrieval # [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Get transformation function by name. This will default to version 1 # get feature store instance fs = ... # get transformation function metadata object plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) Get built-in transformation function min max scaler # get feature store instance fs = ... # get transformation function metadata object min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) Get transformation function by name and version # get feature store instance fs = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 2 ) You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s). Attach transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 1 ) # attach transformation functions feature_view = fs . create_feature_view ( name = 'feature_view_name' , query = query , labels = [ \"target_column\" ], transformation_functions = { \"column_to_transform\" : min_max_scaler } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Attach built-in transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # retrieve transformation functions min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) # attach built-in transformation functions while creating feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category_column\" : label_encoder , \"weight\" : robust_scaler , \"age\" : min_max_scaler , \"salary\" : standard_scaler } ) Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved. Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Get all transformation functions # get feature store instance fs = ... # get all transformation functions list_transformation_fns = fs . get_transformation_functions () Returns: List[TransformationFunction] . List of transformation function instances.","title":"Transformation Functions"},{"location":"generated/api/transformation_functions_api/#transformation-function","text":"[source]","title":"Transformation Function"},{"location":"generated/api/transformation_functions_api/#transformationfunction","text":"hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ** kwargs )","title":"TransformationFunction"},{"location":"generated/api/transformation_functions_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/transformation_functions_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/transformation_functions_api/#name","text":"[source]","title":"name"},{"location":"generated/api/transformation_functions_api/#output_type","text":"[source]","title":"output_type"},{"location":"generated/api/transformation_functions_api/#source_code_content","text":"[source]","title":"source_code_content"},{"location":"generated/api/transformation_functions_api/#transformation_fn","text":"[source]","title":"transformation_fn"},{"location":"generated/api/transformation_functions_api/#transformer_code","text":"[source]","title":"transformer_code"},{"location":"generated/api/transformation_functions_api/#version","text":"","title":"version"},{"location":"generated/api/transformation_functions_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/transformation_functions_api/#delete","text":"TransformationFunction . delete () Delete transformation function from backend. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () # retrieve transformation function plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) # delete transformation function from backend plus_one_fn . delete () [source]","title":"delete"},{"location":"generated/api/transformation_functions_api/#save","text":"TransformationFunction . save () Persist transformation function in backend. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save ()","title":"save"},{"location":"generated/api/transformation_functions_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/transformation_functions_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Example # define function def plus_one ( value ): return value + 1 # create transformation function plus_one_meta = fs . create_transformation_function ( transformation_function = plus_one , output_type = int , version = 1 ) # persist transformation function in backend plus_one_meta . save () Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object.","title":"create_transformation_function"},{"location":"generated/api/transformation_functions_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/transformation_functions_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Get transformation function by name. This will default to version 1 # get feature store instance fs = ... # get transformation function metadata object plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) Get built-in transformation function min max scaler # get feature store instance fs = ... # get transformation function metadata object min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) Get transformation function by name and version # get feature store instance fs = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 2 ) You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s). Attach transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # get transformation function metadata object min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" , version = 1 ) # attach transformation functions feature_view = fs . create_feature_view ( name = 'feature_view_name' , query = query , labels = [ \"target_column\" ], transformation_functions = { \"column_to_transform\" : min_max_scaler } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Attach built-in transformation functions to the feature view # get feature store instance fs = ... # define query object query = ... # retrieve transformation functions min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) # attach built-in transformation functions while creating feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category_column\" : label_encoder , \"weight\" : robust_scaler , \"age\" : min_max_scaler , \"salary\" : standard_scaler } ) Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/transformation_functions_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Get all transformation functions # get feature store instance fs = ... # get all transformation functions list_transformation_fns = fs . get_transformation_functions () Returns: List[TransformationFunction] . List of transformation function instances.","title":"get_transformation_functions"},{"location":"generated/api/validation_api/","text":"Validation # {{validation_result}} Properties # {{validation_result_properties}} Methods # {{expectation_methods}} Validate a dataframe # {{validate}} Retrieval # {{validation_result_get}}","title":"Validation"},{"location":"generated/api/validation_api/#validation","text":"{{validation_result}}","title":"Validation"},{"location":"generated/api/validation_api/#properties","text":"{{validation_result_properties}}","title":"Properties"},{"location":"generated/api/validation_api/#methods","text":"{{expectation_methods}}","title":"Methods"},{"location":"generated/api/validation_api/#validate-a-dataframe","text":"{{validate}}","title":"Validate a dataframe"},{"location":"generated/api/validation_api/#retrieval","text":"{{validation_result_get}}","title":"Retrieval"},{"location":"generated/api/validation_report_api/","text":"Validation Report # [source] ValidationReport # hsfs . validation_report . ValidationReport ( success , results , meta , statistics , evaluation_parameters = None , id = None , full_report_path = None , featurestore_id = None , featuregroup_id = None , href = None , expand = None , items = None , count = None , type = None , validation_time = None , ingestion_result = \"UNKNOWN\" , ** kwargs ) Metadata object representing a validation report generated by Great Expectations in the Feature Store. Creation # [source] validate # FeatureGroup . validate ( dataframe = None , expectation_suite = None , save_report = False , validation_options = {}, ingestion_result = \"UNKNOWN\" , ge_type = True , ) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Example # connect to the Feature Store fs = ... # get feature group instance fg = fs . get_or_create_feature_group ( ... ) ge_report = fg . validate ( df , save_report = False ) Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The dataframe to run the data validation expectations against. expectation_suite Optional[hsfs.expectation_suite.ExpectationSuite] : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. save_report Optional[bool] : Whether to save the report to the backend. This is only possible if the Expectation suite is initialised and attached to the Feature Group. Defaults to False. ge_type bool : Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True. Returns A Validation Report produced by Great Expectations. [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, save_code = True , wait = False , ) Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified features dataframe as feature group to the online/offline feature store. Changed in 3.3.0 insert and save methods are now async by default in non-spark clients. To achieve the old behaviour, set wait argument to True . Upsert new feature data with time travel format HUDI # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , description = 'Bitcoin price aggregated for days' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_bitcoin_processed ) Async insert # connect to the Feature Store fs = ... fg1 = fs . get_or_create_feature_group ( name = 'feature_group_name1' , description = 'Description of the first FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) # async insertion in order not to wait till finish of the job fg . insert ( df_for_fg1 , write_options = { \"wait_for_job\" : False }) fg2 = fs . get_or_create_feature_group ( name = 'feature_group_name2' , description = 'Description of the second FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_for_fg2 ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None (If the streaming APIs are enabled, specifying the storage option is not supported). write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job gets started immediately. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default True , to control whether the expectation suite of the feature group should be fetched before every insert. save_code Optional[bool] : When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create the feature group or used to insert data to it. When calling the insert method repeatedly with small batches of data, this can slow down the writes. Use this option to turn off saving code. Defaults to True . wait bool : Wait for job to finish before returning, defaults to False . Shortcut for read_options {\"wait_for_job\": False} . Returns ( Job , ValidationReport ) A tuple with job information if python engine is used and the validation report if validation is enabled. Raises hsfs.client.exceptions.RestAPIError . e.g fail to create feature group, dataframe schema does not match existing feature group schema, etc. hsfs.client.exceptions.DataValidationException . If data validation fails and the expectation suite validation_ingestion_policy is set to STRICT . Data is NOT ingested. Retrieval # [source] get_latest_validation_report # FeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the Feature Group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) latest_val_report = fg . get_latest_validation_report () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError . [source] get_all_validation_reports # FeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) val_reports = fg . get_all_validation_reports () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns Union[List[ ValidationReport ], ValidationReport ]. All validation reports attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException . Properties # [source] evaluation_parameters # Evaluation parameters field of the validation report which store kwargs of the validation. [source] id # Id of the validation report, set by backend. [source] ingestion_result # Overall success of the validation run together with the ingestion validation policy. Indicating if dataframe was ingested or rejected. [source] meta # Meta field of the validation report to store additional informations. [source] results # List of expectation results obtained after validation. [source] statistics # Statistics field of the validation report which store overall statistics about the validation result, e.g number of failing/successful expectations. [source] success # Overall success of the validation step Methods # [source] from_response_json # ValidationReport . from_response_json ( json_dict ) [source] json # ValidationReport . json () [source] to_dict # ValidationReport . to_dict () [source] to_ge_type # ValidationReport . to_ge_type () [source] to_json_dict # ValidationReport . to_json_dict ()","title":"ValidationReport"},{"location":"generated/api/validation_report_api/#validation-report","text":"[source]","title":"Validation Report"},{"location":"generated/api/validation_report_api/#validationreport","text":"hsfs . validation_report . ValidationReport ( success , results , meta , statistics , evaluation_parameters = None , id = None , full_report_path = None , featurestore_id = None , featuregroup_id = None , href = None , expand = None , items = None , count = None , type = None , validation_time = None , ingestion_result = \"UNKNOWN\" , ** kwargs ) Metadata object representing a validation report generated by Great Expectations in the Feature Store.","title":"ValidationReport"},{"location":"generated/api/validation_report_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/validation_report_api/#validate","text":"FeatureGroup . validate ( dataframe = None , expectation_suite = None , save_report = False , validation_options = {}, ingestion_result = \"UNKNOWN\" , ge_type = True , ) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Example # connect to the Feature Store fs = ... # get feature group instance fg = fs . get_or_create_feature_group ( ... ) ge_report = fg . validate ( df , save_report = False ) Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The dataframe to run the data validation expectations against. expectation_suite Optional[hsfs.expectation_suite.ExpectationSuite] : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. ingestion_result str : Specify the fate of the associated data, defaults to \"UNKNOWN\". Supported options are \"UNKNOWN\", \"INGESTED\", \"REJECTED\", \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\" for testing and development and \"FG_DATA\" when validating data already in the Feature Group. save_report Optional[bool] : Whether to save the report to the backend. This is only possible if the Expectation suite is initialised and attached to the Feature Group. Defaults to False. ge_type bool : Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True. Returns A Validation Report produced by Great Expectations. [source]","title":"validate"},{"location":"generated/api/validation_report_api/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, save_code = True , wait = False , ) Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified features dataframe as feature group to the online/offline feature store. Changed in 3.3.0 insert and save methods are now async by default in non-spark clients. To achieve the old behaviour, set wait argument to True . Upsert new feature data with time travel format HUDI # connect to the Feature Store fs = ... fg = fs . get_or_create_feature_group ( name = 'bitcoin_price' , description = 'Bitcoin price aggregated for days' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_bitcoin_processed ) Async insert # connect to the Feature Store fs = ... fg1 = fs . get_or_create_feature_group ( name = 'feature_group_name1' , description = 'Description of the first FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) # async insertion in order not to wait till finish of the job fg . insert ( df_for_fg1 , write_options = { \"wait_for_job\" : False }) fg2 = fs . get_or_create_feature_group ( name = 'feature_group_name2' , description = 'Description of the second FG' , version = 1 , primary_key = [ 'unix' ], online_enabled = True , event_time = 'unix' ) fg . insert ( df_for_fg2 ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None (If the streaming APIs are enabled, specifying the storage option is not supported). write_options Optional[Dict[str, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the materialization job to write data to the offline storage. start_offline_backfill is deprecated. Use start_offline_materialization instead. key start_offline_materialization and value True or False to configure whether or not to start the materialization job to write data to the offline storage. By default the materialization job gets started immediately. key kafka_producer_config and value an object of type properties used to configure the Kafka client. To optimize for throughput in high latency connection consider changing producer properties . key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[str, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. key fetch_expectation_suite a boolean value, by default True , to control whether the expectation suite of the feature group should be fetched before every insert. save_code Optional[bool] : When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create the feature group or used to insert data to it. When calling the insert method repeatedly with small batches of data, this can slow down the writes. Use this option to turn off saving code. Defaults to True . wait bool : Wait for job to finish before returning, defaults to False . Shortcut for read_options {\"wait_for_job\": False} . Returns ( Job , ValidationReport ) A tuple with job information if python engine is used and the validation report if validation is enabled. Raises hsfs.client.exceptions.RestAPIError . e.g fail to create feature group, dataframe schema does not match existing feature group schema, etc. hsfs.client.exceptions.DataValidationException . If data validation fails and the expectation suite validation_ingestion_policy is set to STRICT . Data is NOT ingested.","title":"insert"},{"location":"generated/api/validation_report_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/validation_report_api/#get_latest_validation_report","text":"FeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the Feature Group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) latest_val_report = fg . get_latest_validation_report () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the Feature Group. Raises hsfs.client.exceptions.RestAPIError . [source]","title":"get_latest_validation_report"},{"location":"generated/api/validation_report_api/#get_all_validation_reports","text":"FeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Example # connect to the Feature Store fs = ... # get the Feature Group instance fg = fs . get_or_create_feature_group ( ... ) val_reports = fg . get_all_validation_reports () Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns Union[List[ ValidationReport ], ValidationReport ]. All validation reports attached to the feature group. Raises hsfs.client.exceptions.RestAPIError . hsfs.client.exceptions.FeatureStoreException .","title":"get_all_validation_reports"},{"location":"generated/api/validation_report_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/validation_report_api/#evaluation_parameters","text":"Evaluation parameters field of the validation report which store kwargs of the validation. [source]","title":"evaluation_parameters"},{"location":"generated/api/validation_report_api/#id","text":"Id of the validation report, set by backend. [source]","title":"id"},{"location":"generated/api/validation_report_api/#ingestion_result","text":"Overall success of the validation run together with the ingestion validation policy. Indicating if dataframe was ingested or rejected. [source]","title":"ingestion_result"},{"location":"generated/api/validation_report_api/#meta","text":"Meta field of the validation report to store additional informations. [source]","title":"meta"},{"location":"generated/api/validation_report_api/#results","text":"List of expectation results obtained after validation. [source]","title":"results"},{"location":"generated/api/validation_report_api/#statistics","text":"Statistics field of the validation report which store overall statistics about the validation result, e.g number of failing/successful expectations. [source]","title":"statistics"},{"location":"generated/api/validation_report_api/#success","text":"Overall success of the validation step","title":"success"},{"location":"generated/api/validation_report_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/validation_report_api/#from_response_json","text":"ValidationReport . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/validation_report_api/#json","text":"ValidationReport . json () [source]","title":"json"},{"location":"generated/api/validation_report_api/#to_dict","text":"ValidationReport . to_dict () [source]","title":"to_dict"},{"location":"generated/api/validation_report_api/#to_ge_type","text":"ValidationReport . to_ge_type () [source]","title":"to_ge_type"},{"location":"generated/api/validation_report_api/#to_json_dict","text":"ValidationReport . to_json_dict ()","title":"to_json_dict"}]}