{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8522ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "258230d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve original send_request method\n",
    "from hsfs.client import base\n",
    "send_request_original = base.Client._send_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "91e6c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_send_request(response_instance_list):\n",
    "\n",
    "    def _send_request_wrap(\n",
    "            self,\n",
    "            method,\n",
    "            path_params,\n",
    "            query_params=None,\n",
    "            headers=None,\n",
    "            data=None,\n",
    "            stream=False,\n",
    "            files=None,\n",
    "    ):\n",
    "        global send_request_original\n",
    "        response_instance = RequestResponseInstance()\n",
    "        \n",
    "        response = send_request_original(self, method, path_params, query_params, headers, data, stream, files)\n",
    "        response_instance.response = response\n",
    "        response_instance.method = method\n",
    "        response_instance.path_params = path_params\n",
    "        response_instance.query_params = query_params\n",
    "        response_instance.headers = headers\n",
    "        response_instance.data = data\n",
    "        response_instance.stream = stream\n",
    "        response_instance.files = files\n",
    "        \n",
    "        response_instance_list.add(response_instance)\n",
    "        \n",
    "        return response\n",
    "\n",
    "    hsfs.client.base.Client._send_request = _send_request_wrap\n",
    "    \n",
    "def unwrap_send_request():\n",
    "    global send_request_original\n",
    "    base.Client._send_request = send_request_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "535ed8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestResponseInstanceList:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "        \n",
    "    def add(self, response_instance):\n",
    "        self.items.append(response_instance)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = {}\n",
    "        d[\"items\"] = list()\n",
    "        for i in self.items:\n",
    "            d[\"items\"].append(i.to_dict())\n",
    "        return d\n",
    "\n",
    "class RequestResponseInstance:\n",
    "    def __init__(self):\n",
    "        self.response = None\n",
    "        self.method = None\n",
    "        self.path_params = None\n",
    "        self.query_params = None\n",
    "        self.headers = None\n",
    "        self.data = None\n",
    "        self.stream = None\n",
    "        self.files = None\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = {}\n",
    "        d[\"response\"] = self.response\n",
    "        d[\"method\"] = self.method\n",
    "        d[\"path_params\"] = self.path_params\n",
    "        d[\"query_params\"] = self.query_params\n",
    "        d[\"headers\"] = self.headers\n",
    "        return d\n",
    "\n",
    "class ResponseGenerator:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def prepare(self):\n",
    "        pass\n",
    "\n",
    "    def call(self):\n",
    "        pass\n",
    "\n",
    "    def cleanup(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        global responses_dict\n",
    "        if self.name in responses_dict:\n",
    "            raise Exception(\"fixture was already determined. remove instance from responses_dict or rename generator to continue.\")\n",
    "        \n",
    "        response_instance_list = RequestResponseInstanceList()\n",
    "\n",
    "        self.prepare()\n",
    "        \n",
    "        wrap_send_request(response_instance_list)\n",
    "        self.call()\n",
    "        unwrap_send_request()\n",
    "        \n",
    "        self.cleanup()\n",
    "        \n",
    "        responses_dict[self.name] = response_instance_list.to_dict()\n",
    "        \n",
    "        return response_instance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5c776872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify generators...\n",
    "\n",
    "class FeatureGroupResponseGenerator(ResponseGenerator):\n",
    "\n",
    "    def prepare(self):\n",
    "\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "        data2 = [(1, \"asd\"),(2, \"asssd\"),(23, \"adssd\"),(1, \"adsasd\"),(7, \"asds\")]\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"intt\",IntegerType(),True),\n",
    "            StructField(\"stringt\",StringType(),True)\n",
    "        ])\n",
    "\n",
    "        df = spark.createDataFrame(data=data2,schema=schema)\n",
    "\n",
    "        from hsfs.feature import Feature\n",
    "        features = [\n",
    "            Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "            Feature(name=\"arrt\",type=\"array<int>\",online_type=\"varchar(1000)\")\n",
    "        ]\n",
    "\n",
    "        features = [\n",
    "            Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "            Feature(name=\"stringt\",type=\"string\",online_type=\"varchar(1000)\")\n",
    "        ]\n",
    "        self.fg = fs.create_feature_group(name=\"fg_test\",\n",
    "                                     features=features,\n",
    "                                     primary_key=[\"intt\"], # key can not contain null values\n",
    "                                     online_enabled=True,\n",
    "                                     time_travel_format=\"HUDI\")\n",
    "\n",
    "        \n",
    "        self.fg.save(df)\n",
    "\n",
    "    def call(self):\n",
    "        fs.get_feature_group(\"fg_test\", version=1)\n",
    "        \n",
    "    def cleanup(self):\n",
    "        self.fg.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "44bdb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify generators...\n",
    "\n",
    "class StorageConnectorResponseGenerator(ResponseGenerator):\n",
    "\n",
    "    def prepare(self):\n",
    "        pass\n",
    "\n",
    "    def call(self):\n",
    "        fs.get_storage_connector(\"test_project_featurestore\")\n",
    "        \n",
    "    def cleanup(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "509c0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify generators...\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class ExternalFeatureGroupResponseGenerator(ResponseGenerator):\n",
    "\n",
    "    def prepare(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "        \n",
    "        data2 = [(1, \"asd\", datetime.strptime(\"2022-03-24\",'%Y-%m-%d')),(2, \"asssd\", datetime.strptime(\"2022-03-20\",'%Y-%m-%d')),(23, \"adssd\", datetime.strptime(\"2022-03-11\",'%Y-%m-%d')),\n",
    "                 (1, \"adsasd\", datetime.strptime(\"2022-03-28\",'%Y-%m-%d')),(7, \"asds\", datetime.strptime(\"2022-03-1\",'%Y-%m-%d'))]\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"intt\",IntegerType(),True),\n",
    "            StructField(\"stringt\",StringType(),True),\n",
    "            StructField(\"datet\",DateType(),True)\n",
    "        ])\n",
    "\n",
    "        df = spark.createDataFrame(data=data2,schema=schema)\n",
    "\n",
    "        from hsfs.feature import Feature\n",
    "        features = [\n",
    "            Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "            Feature(name=\"stringt\",type=\"string\",online_type=\"varchar(1000)\"),\n",
    "            Feature(name=\"datet\",type=\"date\",online_type=\"date\")\n",
    "        ]\n",
    "        \n",
    "        from hsfs import statistics_config, expectation_suite\n",
    "        \n",
    "        self.external_fg = fs.create_external_feature_group(\n",
    "            name=\"external_fg_test1\",\n",
    "            storage_connector=fs.get_storage_connector(\"test_project_featurestore\"),\n",
    "            query=\"Select * from \",\n",
    "            data_format=\"hudi\",\n",
    "            path=\"test_path\",\n",
    "            options={},\n",
    "            version=1,\n",
    "            description=\"test description\",\n",
    "            primary_key=[\"intt\"],\n",
    "            features=features,\n",
    "            statistics_config=statistics_config.StatisticsConfig(),\n",
    "            event_time=\"datet\",\n",
    "            expectation_suite=expectation_suite.ExpectationSuite(expectation_suite_name=\"test_expectation_suite_name\", expectations=None, meta=\"{}\"),\n",
    "        )\n",
    "\n",
    "        self.external_fg.save()\n",
    "\n",
    "    def call(self):\n",
    "        fs.get_external_feature_group(\"external_fg_test\")\n",
    "        \n",
    "    def cleanup(self):\n",
    "        self.external_fg.delete()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "494ec8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify generators...\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class FeatureViewResponseGenerator(ResponseGenerator):\n",
    "\n",
    "    def prepare(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "        from hsfs.feature import Feature\n",
    "        from hsfs.client.exceptions import RestAPIError\n",
    "\n",
    "        # create fg 1\n",
    "        try:\n",
    "            fg_1 = fs.get_feature_group(\"fg_1_test\")\n",
    "        except RestAPIError:\n",
    "            data_1 = [(1, \"asd\"),(2, \"asssd\"),(23, \"adssd\"),(1, \"adsasd\"),(7, \"asds\")]\n",
    "\n",
    "            schema_1 = StructType([\n",
    "                StructField(\"intt\",IntegerType(),True),\n",
    "                StructField(\"stringt\",StringType(),True)\n",
    "            ])\n",
    "\n",
    "            df_1 = spark.createDataFrame(data=data_1,schema=schema_1)\n",
    "\n",
    "            features_1 = [\n",
    "                Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "                Feature(name=\"stringt\",type=\"string\",online_type=\"varchar(1000)\")\n",
    "            ]\n",
    "            fg_1 = fs.create_feature_group(name=\"fg_1_test\",\n",
    "                                         features=features_1,\n",
    "                                         primary_key=[\"intt\"], # key can not contain null values\n",
    "                                         online_enabled=True,\n",
    "                                         time_travel_format=\"HUDI\")\n",
    "\n",
    "            fg_1.save(df_1)\n",
    "        \n",
    "        # create fg 2\n",
    "\n",
    "        try:\n",
    "            fg_2 = fs.get_feature_group(\"fg_2_test\")\n",
    "        except RestAPIError:\n",
    "            data_2 = [(1, True),(2, True),(23, False),(1, True),(7, False)]\n",
    "\n",
    "            schema_2 = StructType([\n",
    "                StructField(\"intt\",IntegerType(),True),\n",
    "                StructField(\"boolt\",BooleanType(),True)\n",
    "            ])\n",
    "\n",
    "            df_2 = spark.createDataFrame(data=data_2,schema=schema_2)\n",
    "\n",
    "            features_2 = [\n",
    "                Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "                Feature(name=\"boolt\",type=\"boolean\",online_type=\"boolean\")\n",
    "            ]\n",
    "            fg_2 = fs.create_feature_group(name=\"fg_2_test\",\n",
    "                                         features=features_2,\n",
    "                                         primary_key=[\"intt\"], # key can not contain null values\n",
    "                                         online_enabled=True,\n",
    "                                         time_travel_format=\"HUDI\")\n",
    "\n",
    "            fg_2.save(df_2)\n",
    "        \n",
    "        # fv\n",
    "        query = fg_1.select_all().join(fg_2.select_all())\n",
    "        self.feature_view = fs.get_or_create_feature_view(\n",
    "            name='fv_test',\n",
    "            query=query,\n",
    "            version=1\n",
    "        )\n",
    "\n",
    "    def call(self):\n",
    "        fs.get_feature_view('fv_test')\n",
    "        \n",
    "    def cleanup(self):\n",
    "        self.feature_view.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38723945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify generators...\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class TrainingDatasetResponseGenerator(ResponseGenerator):\n",
    "\n",
    "    def prepare(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "        from hsfs.feature import Feature\n",
    "        from hsfs.client.exceptions import RestAPIError\n",
    "\n",
    "        # create fg 1\n",
    "        try:\n",
    "            fg_1 = fs.get_feature_group(\"fg_1_test\")\n",
    "        except RestAPIError:\n",
    "            data_1 = [(1, \"asd\"),(2, \"asssd\"),(23, \"adssd\"),(1, \"adsasd\"),(7, \"asds\")]\n",
    "\n",
    "            schema_1 = StructType([\n",
    "                StructField(\"intt\",IntegerType(),True),\n",
    "                StructField(\"stringt\",StringType(),True)\n",
    "            ])\n",
    "\n",
    "            df_1 = spark.createDataFrame(data=data_1,schema=schema_1)\n",
    "\n",
    "            features_1 = [\n",
    "                Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "                Feature(name=\"stringt\",type=\"string\",online_type=\"varchar(1000)\")\n",
    "            ]\n",
    "            fg_1 = fs.create_feature_group(name=\"fg_1_test\",\n",
    "                                         features=features_1,\n",
    "                                         primary_key=[\"intt\"], # key can not contain null values\n",
    "                                         online_enabled=True,\n",
    "                                         time_travel_format=\"HUDI\")\n",
    "\n",
    "            fg_1.save(df_1)\n",
    "        \n",
    "        # create fg 2\n",
    "\n",
    "        try:\n",
    "            fg_2 = fs.get_feature_group(\"fg_2_test\")\n",
    "        except RestAPIError:\n",
    "            data_2 = [(1, True),(2, True),(23, False),(1, True),(7, False)]\n",
    "\n",
    "            schema_2 = StructType([\n",
    "                StructField(\"intt\",IntegerType(),True),\n",
    "                StructField(\"boolt\",BooleanType(),True)\n",
    "            ])\n",
    "\n",
    "            df_2 = spark.createDataFrame(data=data_2,schema=schema_2)\n",
    "\n",
    "            features_2 = [\n",
    "                Feature(name=\"intt\",type=\"int\",online_type=\"int\"),\n",
    "                Feature(name=\"boolt\",type=\"boolean\",online_type=\"boolean\")\n",
    "            ]\n",
    "            fg_2 = fs.create_feature_group(name=\"fg_2_test\",\n",
    "                                         features=features_2,\n",
    "                                         primary_key=[\"intt\"], # key can not contain null values\n",
    "                                         online_enabled=True,\n",
    "                                         time_travel_format=\"HUDI\")\n",
    "\n",
    "            fg_2.save(df_2)\n",
    "        \n",
    "        # fv\n",
    "\n",
    "        query = fg_1.select_all().join(fg_2.select_all())\n",
    "            \n",
    "        self.td = out_fs.create_training_dataset(name=td_name, description=\"derived td description\", data_format=\"csv\", version=1, statistics_config=False)\n",
    "self.td.save(td_query)\n",
    "        \n",
    "\n",
    "    def call(self):\n",
    "        fs.get_training_dataset('fv_test')\n",
    "        \n",
    "    def cleanup(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76e194c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Metadata operation error: (url: https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/119/featurestores/67/trainingdatasets/fv_test). Server response: \n",
      "HTTP code: 404, HTTP reason: Not Found, error code: 270012, error msg: Training dataset wasn't found., user msg: training dataset name : fv_test\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 59, in run\n",
      "  File \"<stdin>\", line 83, in call\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/feature_store.py\", line 294, in get_training_dataset\n",
      "    return self._training_dataset_api.get(name, version)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/core/training_dataset_api.py\", line 57, in get\n",
      "    _client._send_request(\"GET\", path_params, query_params),\n",
      "  File \"<stdin>\", line 16, in _send_request_wrap\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/decorators.py\", line 35, in if_connected\n",
      "    return fn(inst, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/hsfs/client/base.py\", line 171, in _send_request\n",
      "    raise exceptions.RestAPIError(url, response)\n",
      "hsfs.client.exceptions.RestAPIError: Metadata operation error: (url: https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/119/featurestores/67/trainingdatasets/fv_test). Server response: \n",
      "HTTP code: 404, HTTP reason: Not Found, error code: 270012, error msg: Training dataset wasn't found., user msg: training dataset name : fv_test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses_dict = {}\n",
    "\n",
    "# run generators...\n",
    "#FeatureGroupResponseGenerator(\"get_feature_group\").run()\n",
    "#StorageConnectorResponseGenerator(\"get_storage_connector\").run()\n",
    "#ExternalFeatureGroupResponseGenerator(\"get_external_feature_group\").run()\n",
    "#FeatureViewResponseGenerator(\"get_feature_view\").run()\n",
    "TrainingDatasetResponseGenerator(\"get_training_dataset\").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc150255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "None"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(print(json.dumps(responses_dict, indent=4, separators=(',',': '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0683a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\nimport json\\nimport pydoop.hdfs as hdfs\\n\\nfilename = f'hdfs:///Projects/{fs.project_name}/Resources/backend_fixtures.json'\\nwith hdfs.open(filename, 'wt') as json_file:\\n    json.dump(responses_dict, json_file, \\n                        indent=4,  \\n                        separators=(',',': '))\\n\""
     ]
    }
   ],
   "source": [
    "# write responses captured with the generators to 'backend_fixtures.json'\n",
    "'''\n",
    "import json\n",
    "import pydoop.hdfs as hdfs\n",
    "\n",
    "filename = f'hdfs:///Projects/{fs.project_name}/Resources/backend_fixtures.json'\n",
    "with hdfs.open(filename, 'wt') as json_file:\n",
    "    json.dump(responses_dict, json_file, \n",
    "                        indent=4,  \n",
    "                        separators=(',',': '))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace27e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
