{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Feature Store # HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository. Documentation # Documentation is available at Hopsworks Feature Store Documentation . Issues # For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Hopsworks Feature Store"},{"location":"#hopsworks-feature-store","text":"HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Feature Store"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository.","title":"Getting Started On Hopsworks"},{"location":"#documentation","text":"Documentation is available at Hopsworks Feature Store Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[python,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ python,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Option 1: Build only current version of docs # Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve Option 2 (Preferred): Build multi-version doc with mike # Versioning on docs.hopsworks.ai # On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 . Build Instructions # For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ] Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[python,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ python,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","text":"Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve","title":"Option 1: Build only current version of docs"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","text":"","title":"Option 2 (Preferred): Build multi-version doc with mike"},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","text":"On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 .","title":"Versioning on docs.hopsworks.ai"},{"location":"CONTRIBUTING/#build-instructions","text":"For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ]","title":"Build Instructions"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"generated/api/connection_api/","text":"Connection # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # Hostname of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides. Arguments host Optional[str] : The hostname of the Hopsworks instance in the form of [UUID].cloud.hopsworks.ai , defaults to None . Do not use the url including https:// when connecting programatically. port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] cert_folder # [source] host # [source] hostname_verification # [source] port # [source] project # [source] region_name # [source] secrets_store # [source] trust_store_path # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide.","title":"Connection"},{"location":"generated/api/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/api/connection_api/#connection_1","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # Hostname of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides. Arguments host Optional[str] : The hostname of the Hopsworks instance in the form of [UUID].cloud.hopsworks.ai , defaults to None . Do not use the url including https:// when connecting programatically. port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/api/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/api/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/api/connection_api/#cert_folder","text":"[source]","title":"cert_folder"},{"location":"generated/api/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/api/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/api/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/api/connection_api/#project","text":"[source]","title":"project"},{"location":"generated/api/connection_api/#region_name","text":"[source]","title":"region_name"},{"location":"generated/api/connection_api/#secrets_store","text":"[source]","title":"secrets_store"},{"location":"generated/api/connection_api/#trust_store_path","text":"","title":"trust_store_path"},{"location":"generated/api/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/api/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/api/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source]","title":"connection"},{"location":"generated/api/connection_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/api/connection_api/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide.","title":"setup_databricks"},{"location":"generated/api/expectation_api/","text":"Expectation # {{expectation}} Properties # {{expectation_properties}} Methods # {{expectation_methods}} Creation # {{expectation_create}} Retrieval # {{expectation_getall}} {{expectation_get}}","title":"Expectation"},{"location":"generated/api/expectation_api/#expectation","text":"{{expectation}}","title":"Expectation"},{"location":"generated/api/expectation_api/#properties","text":"{{expectation_properties}}","title":"Properties"},{"location":"generated/api/expectation_api/#methods","text":"{{expectation_methods}}","title":"Methods"},{"location":"generated/api/expectation_api/#creation","text":"{{expectation_create}}","title":"Creation"},{"location":"generated/api/expectation_api/#retrieval","text":"{{expectation_getall}} {{expectation_get}}","title":"Retrieval"},{"location":"generated/api/external_feature_group_api/","text":"ExternalFeatureGroup # [source] ExternalFeatureGroup # hsfs . feature_group . ExternalFeatureGroup ( storage_connector , query = None , data_format = None , path = None , options = {}, name = None , version = None , description = None , primary_key = None , featurestore_id = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , statistics_config = None , event_time = None , expectation_suite = None , ) Creation # [source] create_external_feature_group # FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , ) Create a external feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. Retrieval # [source] get_external_feature_group # FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] created # [source] creator # [source] data_format # [source] description # [source] event_time # Event time feature in the feature group. [source] expectation_suite # Expectation Suite configuration object defining the settings for data validation of the feature group. [source] features # [source] id # [source] location # [source] name # [source] options # [source] path # [source] primary_key # List of features building the primary key. [source] query # [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] storage_connector # [source] version # Methods # [source] add_tag # ExternalFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # ExternalFeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] compute_statistics # ExternalFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # ExternalFeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_expectation_suite # ExternalFeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the featuregroup. Raises RestAPIException . [source] delete_tag # ExternalFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] filter # ExternalFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # ExternalFeatureGroup . from_response_json ( json_dict ) [source] get_all_validation_reports # ExternalFeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source] get_expectation_suite # ExternalFeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises RestAPIException . [source] get_feature # ExternalFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_latest_validation_report # ExternalFeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source] get_statistics # ExternalFeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Defaults to None . Defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # ExternalFeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # ExternalFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] json # ExternalFeatureGroup . json () [source] read # ExternalFeatureGroup . read ( dataframe_type = \"default\" ) Get the feature group as a DataFrame. Engine Support Spark only Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments dataframe_type : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . [source] save # ExternalFeatureGroup . save () [source] save_expectation_suite # ExternalFeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the featuregroup. run_validation : Set whether the expectation_suite will run on ingestion validation_ingestion_policy : Set the policy for ingestion to the featuregroup. \"STRICT\" only allows DataFrame passing validation to be inserted into featuregroup. \"ALWAYS\" always insert the DataFrame to the featuregroup, irrespective of overall validation result. Raises RestAPIException . [source] save_validation_report # ExternalFeatureGroup . save_validation_report ( validation_report , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same featuregroup. Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the featuregroup. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises RestAPIException . [source] select # ExternalFeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # ExternalFeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # ExternalFeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # ExternalFeatureGroup . show ( n ) Show the first n rows of the feature group. [source] to_dict # ExternalFeatureGroup . to_dict () [source] update_description # ExternalFeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # ExternalFeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # ExternalFeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # ExternalFeatureGroup . update_from_response_json ( json_dict ) [source] update_statistics_config # ExternalFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # ExternalFeatureGroup . validate ( save_report = False , validation_options = {}) Run validation based on the attached expectations Returns FeatureGroupValidation . The feature group validation metadata object.","title":"ExternalFeatureGroup"},{"location":"generated/api/external_feature_group_api/#externalfeaturegroup","text":"[source]","title":"ExternalFeatureGroup"},{"location":"generated/api/external_feature_group_api/#externalfeaturegroup_1","text":"hsfs . feature_group . ExternalFeatureGroup ( storage_connector , query = None , data_format = None , path = None , options = {}, name = None , version = None , description = None , primary_key = None , featurestore_id = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , statistics_config = None , event_time = None , expectation_suite = None , )","title":"ExternalFeatureGroup"},{"location":"generated/api/external_feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/external_feature_group_api/#create_external_feature_group","text":"FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , ) Create a external feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object.","title":"create_external_feature_group"},{"location":"generated/api/external_feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/external_feature_group_api/#get_external_feature_group","text":"FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_external_feature_group"},{"location":"generated/api/external_feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/external_feature_group_api/#created","text":"[source]","title":"created"},{"location":"generated/api/external_feature_group_api/#creator","text":"[source]","title":"creator"},{"location":"generated/api/external_feature_group_api/#data_format","text":"[source]","title":"data_format"},{"location":"generated/api/external_feature_group_api/#description","text":"[source]","title":"description"},{"location":"generated/api/external_feature_group_api/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/api/external_feature_group_api/#expectation_suite","text":"Expectation Suite configuration object defining the settings for data validation of the feature group. [source]","title":"expectation_suite"},{"location":"generated/api/external_feature_group_api/#features","text":"[source]","title":"features"},{"location":"generated/api/external_feature_group_api/#id","text":"[source]","title":"id"},{"location":"generated/api/external_feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/external_feature_group_api/#name","text":"[source]","title":"name"},{"location":"generated/api/external_feature_group_api/#options","text":"[source]","title":"options"},{"location":"generated/api/external_feature_group_api/#path","text":"[source]","title":"path"},{"location":"generated/api/external_feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/external_feature_group_api/#query","text":"[source]","title":"query"},{"location":"generated/api/external_feature_group_api/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/api/external_feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/api/external_feature_group_api/#storage_connector","text":"[source]","title":"storage_connector"},{"location":"generated/api/external_feature_group_api/#version","text":"","title":"version"},{"location":"generated/api/external_feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/external_feature_group_api/#add_tag","text":"ExternalFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/external_feature_group_api/#append_features","text":"ExternalFeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/external_feature_group_api/#compute_statistics","text":"ExternalFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/api/external_feature_group_api/#delete","text":"ExternalFeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/external_feature_group_api/#delete_expectation_suite","text":"ExternalFeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the featuregroup. Raises RestAPIException . [source]","title":"delete_expectation_suite"},{"location":"generated/api/external_feature_group_api/#delete_tag","text":"ExternalFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/external_feature_group_api/#filter","text":"ExternalFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/external_feature_group_api/#from_response_json","text":"ExternalFeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/external_feature_group_api/#get_all_validation_reports","text":"ExternalFeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source]","title":"get_all_validation_reports"},{"location":"generated/api/external_feature_group_api/#get_expectation_suite","text":"ExternalFeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises RestAPIException . [source]","title":"get_expectation_suite"},{"location":"generated/api/external_feature_group_api/#get_feature","text":"ExternalFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/api/external_feature_group_api/#get_latest_validation_report","text":"ExternalFeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source]","title":"get_latest_validation_report"},{"location":"generated/api/external_feature_group_api/#get_statistics","text":"ExternalFeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Defaults to None . Defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/api/external_feature_group_api/#get_tag","text":"ExternalFeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/external_feature_group_api/#get_tags","text":"ExternalFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/external_feature_group_api/#json","text":"ExternalFeatureGroup . json () [source]","title":"json"},{"location":"generated/api/external_feature_group_api/#read","text":"ExternalFeatureGroup . read ( dataframe_type = \"default\" ) Get the feature group as a DataFrame. Engine Support Spark only Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments dataframe_type : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . [source]","title":"read"},{"location":"generated/api/external_feature_group_api/#save","text":"ExternalFeatureGroup . save () [source]","title":"save"},{"location":"generated/api/external_feature_group_api/#save_expectation_suite","text":"ExternalFeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the featuregroup. run_validation : Set whether the expectation_suite will run on ingestion validation_ingestion_policy : Set the policy for ingestion to the featuregroup. \"STRICT\" only allows DataFrame passing validation to be inserted into featuregroup. \"ALWAYS\" always insert the DataFrame to the featuregroup, irrespective of overall validation result. Raises RestAPIException . [source]","title":"save_expectation_suite"},{"location":"generated/api/external_feature_group_api/#save_validation_report","text":"ExternalFeatureGroup . save_validation_report ( validation_report , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same featuregroup. Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the featuregroup. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises RestAPIException . [source]","title":"save_validation_report"},{"location":"generated/api/external_feature_group_api/#select","text":"ExternalFeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/external_feature_group_api/#select_all","text":"ExternalFeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/external_feature_group_api/#select_except","text":"ExternalFeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/external_feature_group_api/#show","text":"ExternalFeatureGroup . show ( n ) Show the first n rows of the feature group. [source]","title":"show"},{"location":"generated/api/external_feature_group_api/#to_dict","text":"ExternalFeatureGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/external_feature_group_api/#update_description","text":"ExternalFeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/external_feature_group_api/#update_feature_description","text":"ExternalFeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/api/external_feature_group_api/#update_features","text":"ExternalFeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/api/external_feature_group_api/#update_from_response_json","text":"ExternalFeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/external_feature_group_api/#update_statistics_config","text":"ExternalFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/api/external_feature_group_api/#validate","text":"ExternalFeatureGroup . validate ( save_report = False , validation_options = {}) Run validation based on the attached expectations Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/api/feature_api/","text":"Feature # [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] description # Description of the feature. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] contains # Feature . contains ( other ) [source] from_response_json # Feature . from_response_json ( json_dict ) [source] is_complex # Feature . is_complex () Returns true if the feature has a complex type. [source] json # Feature . json () [source] to_dict # Feature . to_dict ()","title":"Feature"},{"location":"generated/api/feature_api/#feature","text":"[source]","title":"Feature"},{"location":"generated/api/feature_api/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/api/feature_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_api/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/api/feature_api/#description","text":"Description of the feature. [source]","title":"description"},{"location":"generated/api/feature_api/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_api/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/api/feature_api/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/api/feature_api/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/api/feature_api/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/api/feature_api/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/api/feature_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_api/#contains","text":"Feature . contains ( other ) [source]","title":"contains"},{"location":"generated/api/feature_api/#from_response_json","text":"Feature . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_api/#is_complex","text":"Feature . is_complex () Returns true if the feature has a complex type. [source]","title":"is_complex"},{"location":"generated/api/feature_api/#json","text":"Feature . json () [source]","title":"json"},{"location":"generated/api/feature_api/#to_dict","text":"Feature . to_dict ()","title":"to_dict"},{"location":"generated/api/feature_group_api/","text":"FeatureGroup # [source] FeatureGroup # hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , online_topic_name = None , event_time = None , stream = False , expectation_suite = None , ) Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns FeatureGroup . The feature group metadata object. [source] get_or_create_feature_group # FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] event_time # Event time feature in the feature group. [source] expectation_suite # Expectation Suite configuration object defining the settings for data validation of the feature group. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] stream # Whether to enable real time stream writing capabilities. [source] time_travel_format # Setting of the feature group time travel format. [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] as_of # FeatureGroup . as_of ( wallclock_time = None , exclude_until = None ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: fg_a . select_all () . as_of ( ... , ... ) . join ( fg_b . select_all () . as_of ( ... , ... )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: fg_a . select_all () . as_of ( ... , ... ) # as_of is not applied . join ( fg_b . select_all () . as_of ( ... , ... )) # as_of is not applied . as_of ( ... , ... ) Warning This function only works for feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . exclude_until : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source] compute_statistics # FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_expectation_suite # FeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the featuregroup. Raises RestAPIException . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # FeatureGroup . from_response_json ( json_dict ) [source] get_all_validation_reports # FeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source] get_complex_features # FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source] get_expectation_suite # FeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises RestAPIException . [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_latest_validation_report # FeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Defaults to None . Defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, ) Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storag as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . If feature group doesn't exists the insert method will create the necessary metadata the first time it is invoked and writes the specified features dataframe as feature group to the online/offline feature store. Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. Returns FeatureGroup . Updated feature group metadata object. [source] insert_stream # FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Engine Support Spark only Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Data Validation Support insert_stream does not perform any data validation using Great Expectations even when a expectation suite is attached. Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options: Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source] json # FeatureGroup . json () [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. Deprecated `read_changes` method is deprecated . Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)` instead . This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time Union[str, int, datetime.datetime, datetime.date] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_wallclock_time Union[str, int, datetime.datetime, datetime.date] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source] save # FeatureGroup . save ( features , write_options = {}, validation_options = {}) Persist the metadata and materialize the feature group to the feature store. Deprecated save method is deprecated. Use the insert method instead. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source] save_expectation_suite # FeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the featuregroup. run_validation : Set whether the expectation_suite will run on ingestion validation_ingestion_policy : Set the policy for ingestion to the featuregroup. \"STRICT\" only allows DataFrame passing validation to be inserted into featuregroup. \"ALWAYS\" always insert the DataFrame to the featuregroup, irrespective of overall validation result. Raises RestAPIException . [source] save_validation_report # FeatureGroup . save_validation_report ( validation_report , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same featuregroup. Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the featuregroup. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises RestAPIException . [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] to_dict # FeatureGroup . to_dict () [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # FeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # FeatureGroup . update_from_response_json ( json_dict ) [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # FeatureGroup . validate ( dataframe = None , save_report = False , validation_options = {}) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The PySpark dataframe to run the data validation expectations against. expectation_suite : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. Returns FeatureGroupValidation , ValidationReport . The feature group validation metadata object, as well as the Validation Report produced by Great Expectations.","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup","text":"[source]","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup_1","text":"hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , online_topic_name = None , event_time = None , stream = False , expectation_suite = None , )","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_group_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_group_api/#get_or_create_feature_group","text":"FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object.","title":"get_or_create_feature_group"},{"location":"generated/api/feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_group_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/api/feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_group_api/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/api/feature_group_api/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/api/feature_group_api/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/api/feature_group_api/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/api/feature_group_api/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/api/feature_group_api/#expectation_suite","text":"Expectation Suite configuration object defining the settings for data validation of the feature group. [source]","title":"expectation_suite"},{"location":"generated/api/feature_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/feature_group_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/feature_group_api/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_group_api/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/api/feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/feature_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/feature_group_api/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/api/feature_group_api/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/api/feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/feature_group_api/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/api/feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/api/feature_group_api/#stream","text":"Whether to enable real time stream writing capabilities. [source]","title":"stream"},{"location":"generated/api/feature_group_api/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/api/feature_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_group_api/#add_tag","text":"FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/feature_group_api/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/feature_group_api/#as_of","text":"FeatureGroup . as_of ( wallclock_time = None , exclude_until = None ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: fg_a . select_all () . as_of ( ... , ... ) . join ( fg_b . select_all () . as_of ( ... , ... )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: fg_a . select_all () . as_of ( ... , ... ) # as_of is not applied . join ( fg_b . select_all () . as_of ( ... , ... )) # as_of is not applied . as_of ( ... , ... ) Warning This function only works for feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . exclude_until : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/api/feature_group_api/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/api/feature_group_api/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"commit_details"},{"location":"generated/api/feature_group_api/#compute_statistics","text":"FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/api/feature_group_api/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_group_api/#delete_expectation_suite","text":"FeatureGroup . delete_expectation_suite () Delete the expectation suite attached to the featuregroup. Raises RestAPIException . [source]","title":"delete_expectation_suite"},{"location":"generated/api/feature_group_api/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/feature_group_api/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/feature_group_api/#from_response_json","text":"FeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_group_api/#get_all_validation_reports","text":"FeatureGroup . get_all_validation_reports ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source]","title":"get_all_validation_reports"},{"location":"generated/api/feature_group_api/#get_complex_features","text":"FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source]","title":"get_complex_features"},{"location":"generated/api/feature_group_api/#get_expectation_suite","text":"FeatureGroup . get_expectation_suite ( ge_type = True ) Return the expectation suite attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ExpectationSuite . The expectation suite attached to the feature group. Raises RestAPIException . [source]","title":"get_expectation_suite"},{"location":"generated/api/feature_group_api/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/api/feature_group_api/#get_latest_validation_report","text":"FeatureGroup . get_latest_validation_report ( ge_type = True ) Return the latest validation report attached to the feature group if it exists. Arguments ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Returns ValidationReport . The latest validation report attached to the feature group. Raises RestAPIException . [source]","title":"get_latest_validation_report"},{"location":"generated/api/feature_group_api/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Defaults to None . Defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/api/feature_group_api/#get_tag","text":"FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/feature_group_api/#get_tags","text":"FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/feature_group_api/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {}, validation_options = {}, ) Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storag as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . If feature group doesn't exists the insert method will create the necessary metadata the first time it is invoked and writes the specified features dataframe as feature group to the online/offline feature store. Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/api/feature_group_api/#insert_stream","text":"FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Engine Support Spark only Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Data Validation Support insert_stream does not perform any data validation using Great Expectations even when a expectation suite is attached. Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options: Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source]","title":"insert_stream"},{"location":"generated/api/feature_group_api/#json","text":"FeatureGroup . json () [source]","title":"json"},{"location":"generated/api/feature_group_api/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/api/feature_group_api/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. Deprecated `read_changes` method is deprecated . Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)` instead . This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time Union[str, int, datetime.datetime, datetime.date] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_wallclock_time Union[str, int, datetime.datetime, datetime.date] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"read_changes"},{"location":"generated/api/feature_group_api/#save","text":"FeatureGroup . save ( features , write_options = {}, validation_options = {}) Persist the metadata and materialize the feature group to the feature store. Deprecated save method is deprecated. Use the insert method instead. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/api/feature_group_api/#save_expectation_suite","text":"FeatureGroup . save_expectation_suite ( expectation_suite , run_validation = True , validation_ingestion_policy = \"ALWAYS\" ) Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields. Arguments expectation_suite Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite] : The expectation suite to attach to the featuregroup. run_validation : Set whether the expectation_suite will run on ingestion validation_ingestion_policy : Set the policy for ingestion to the featuregroup. \"STRICT\" only allows DataFrame passing validation to be inserted into featuregroup. \"ALWAYS\" always insert the DataFrame to the featuregroup, irrespective of overall validation result. Raises RestAPIException . [source]","title":"save_expectation_suite"},{"location":"generated/api/feature_group_api/#save_validation_report","text":"FeatureGroup . save_validation_report ( validation_report , ge_type = True ) Save validation report to hopsworks platform along previous reports of the same featuregroup. Arguments validation_report Union[dict, hsfs.validation_report.ValidationReport, great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult] : The validation report to attach to the featuregroup. ge_type bool : If True returns a native Great Expectation type, Hopsworks custom type otherwise. Conversion can be performed via the to_ge_type() method on hopsworks type. Defaults to True . Raises RestAPIException . [source]","title":"save_validation_report"},{"location":"generated/api/feature_group_api/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/feature_group_api/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/feature_group_api/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/feature_group_api/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/api/feature_group_api/#to_dict","text":"FeatureGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_group_api/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/feature_group_api/#update_feature_description","text":"FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/api/feature_group_api/#update_features","text":"FeatureGroup . update_features ( features ) Update metadata of features in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/api/feature_group_api/#update_from_response_json","text":"FeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/feature_group_api/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/api/feature_group_api/#validate","text":"FeatureGroup . validate ( dataframe = None , save_report = False , validation_options = {}) Run validation based on the attached expectations. Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites. Arguments dataframe Optional[Union[pandas.DataFrame, pyspark.sql.DataFrame]] : The PySpark dataframe to run the data validation expectations against. expectation_suite : Optionally provide an Expectation Suite to override the one that is possibly attached to the feature group. This is useful for testing new Expectation suites. When an extra suite is provided, the results will never be persisted. Defaults to None . validation_options Optional[Dict[Any, Any]] : Additional validation options as key-value pairs, defaults to {} . key run_validation boolean value, set to False to skip validation temporarily on ingestion. key save_report boolean value, set to False to skip upload of the validation report to Hopsworks. key ge_validate_kwargs a dictionary containing kwargs for the validate method of Great Expectations. Returns FeatureGroupValidation , ValidationReport . The feature group validation metadata object, as well as the Validation Report produced by Great Expectations.","title":"validate"},{"location":"generated/api/feature_store_api/","text":"Feature Store # [source] FeatureStore # hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , num_feature_views = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , ) Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_external_feature_group # FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , ) Create a external feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns FeatureGroup . The feature group metadata object. [source] create_feature_view # FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , ) Create a external feature group metadata object. Deprecated create_on_demand_feature_group method is deprecated. Use the create_external_feature_group method instead. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source] from_response_json # FeatureStore . from_response_json ( json_dict ) [source] get_external_feature_group # FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_external_feature_groups # FeatureStore . get_external_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_view # FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name str : Name of the feature view to get. version Optional[int] : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_feature_views # FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Deprecated get_on_demand_feature_group method is deprecated. Use the get_external_feature_group method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_groups # FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Deprecated get_on_demand_feature_groups method is deprecated. Use the get_external_feature_groups method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_or_create_feature_group # FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. [source] get_or_create_feature_view # FeatureStore . get_or_create_feature_view ( name , query , version , description = \"\" , labels = [], transformation_functions = {} ) Get feature view metadata object or create a new one if it doesn't exist. This method doesn't update existing feature view metadata object. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version int : Version of the feature view to create. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns DataFrame : DataFrame depending on the chosen type.","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#feature-store","text":"[source]","title":"Feature Store"},{"location":"generated/api/feature_store_api/#featurestore","text":"hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , num_feature_views = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , )","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_store_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/api/feature_store_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_store_api/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/api/feature_store_api/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/api/feature_store_api/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/api/feature_store_api/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/api/feature_store_api/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/api/feature_store_api/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/api/feature_store_api/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/api/feature_store_api/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/api/feature_store_api/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/api/feature_store_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_store_api/#create_external_feature_group","text":"FeatureStore . create_external_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , ) Create a external feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. [source]","title":"create_external_feature_group"},{"location":"generated/api/feature_store_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , event_time = None , stream = False , expectation_suite = None , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_store_api/#create_feature_view","text":"FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source]","title":"create_feature_view"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , expectation_suite = None , ) Create a external feature group metadata object. Deprecated create_on_demand_feature_group method is deprecated. Use the create_external_feature_group method instead. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the external feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the external feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the external feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the external feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the external feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this external feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . Returns ExternalFeatureGroup . The external feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/api/feature_store_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"create_transformation_function"},{"location":"generated/api/feature_store_api/#from_response_json","text":"FeatureStore . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_store_api/#get_external_feature_group","text":"FeatureStore . get_external_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_external_feature_group"},{"location":"generated/api/feature_store_api/#get_external_feature_groups","text":"FeatureStore . get_external_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_external_feature_groups"},{"location":"generated/api/feature_store_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/api/feature_store_api/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/api/feature_store_api/#get_feature_view","text":"FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name str : Name of the feature view to get. version Optional[int] : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_view"},{"location":"generated/api/feature_store_api/#get_feature_views","text":"FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_views"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a external feature group entity from the feature store. Deprecated get_on_demand_feature_group method is deprecated. Use the get_external_feature_group method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. version Optional[int] : Version of the external feature group to retrieve, defaults to None and will return the version=1 . Returns ExternalFeatureGroup : The external feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_groups","text":"FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an external feature group entity from the feature store. Deprecated get_on_demand_feature_groups method is deprecated. Use the get_external_feature_groups method instead. Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the external feature group to get. Returns ExternalFeatureGroup : List of external feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_groups"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/api/feature_store_api/#get_or_create_feature_group","text":"FeatureStore . get_or_create_feature_group ( name , version , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , expectation_suite = None , event_time = None , stream = False , ) Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the insert() method with a DataFrame. Arguments name str : Name of the feature group to create. version int : Version of the feature group to retrieve or create. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. expectation_suite Optional[Union[hsfs.expectation_suite.ExpectationSuite, great_expectations.core.expectation_suite.ExpectationSuite]] : Optionally, attach an expectation suite to the feature group which dataframes should be validated against upon insertion. Defaults to None . event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. [source]","title":"get_or_create_feature_group"},{"location":"generated/api/feature_store_api/#get_or_create_feature_view","text":"FeatureStore . get_or_create_feature_view ( name , query , version , description = \"\" , labels = [], transformation_functions = {} ) Get feature view metadata object or create a new one if it doesn't exist. This method doesn't update existing feature view metadata object. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version int : Version of the feature view to create. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source]","title":"get_or_create_feature_view"},{"location":"generated/api/feature_store_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/feature_store_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/api/feature_store_api/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_datasets"},{"location":"generated/api/feature_store_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/feature_store_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source]","title":"get_transformation_functions"},{"location":"generated/api/feature_store_api/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns DataFrame : DataFrame depending on the chosen type.","title":"sql"},{"location":"generated/api/feature_view_api/","text":"Feature View # [source] FeatureView # hsfs . feature_view . FeatureView ( name , query , featurestore_id , id = None , version = None , description = \"\" , labels = [], transformation_functions = {}, ) Creation # [source] create_feature_view # FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. Retrieval # [source] get_feature_view # FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name str : Name of the feature view to get. version Optional[int] : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_feature_views # FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. Properties # [source] description # [source] featurestore_id # Feature store id. [source] id # Feature view id. [source] labels # The labels/prediction feature of the feature view. Can be a composite of multiple features. [source] name # Name of the feature view. [source] primary_keys # Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source] query # [source] schema # Feature view schema. [source] transformation_functions # Set transformation functions. [source] version # Version number of the feature view. Methods # [source] add_tag # FeatureView . add_tag ( name , value ) [source] add_training_dataset_tag # FeatureView . add_training_dataset_tag ( training_dataset_version , name , value ) [source] clean # FeatureView . clean ( feature_store_id , feature_view_name , feature_view_version ) Delete the feature view and all associated metadata. Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Arguments feature_store_id int : int. Id of feature store. feature_view_name str : str. Name of feature view. feature_view_version str : str. Version of feature view. Raises RestAPIError . [source] create_train_test_split # FeatureView . create_train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"csv\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, ) Create a training dataset and save data into location . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following ormats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"csv\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] create_train_validation_test_split # FeatureView . create_train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"csv\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, ) Create a training dataset and save data into location . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments validation_size Optional[float] : size of validation set. test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : tdatatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"csv\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] create_training_data # FeatureView . create_training_data ( start_time = \"\" , end_time = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"csv\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, ) Create a training dataset and save data into location . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"csv\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] delete # FeatureView . delete () Delete current feature view and all associated metadata. Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Raises RestAPIError . [source] delete_all_training_datasets # FeatureView . delete_all_training_datasets () [source] delete_tag # FeatureView . delete_tag ( name ) [source] delete_training_dataset # FeatureView . delete_training_dataset ( version ) [source] delete_training_dataset_tag # FeatureView . delete_training_dataset_tag ( training_dataset_version , name ) [source] from_response_json # FeatureView . from_response_json ( json_dict ) [source] get_batch_data # FeatureView . get_batch_data ( start_time = None , end_time = None , read_options = None ) Get a batch of data from an event time interval. Arguments start_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . read_options : User provided read options. Defaults to {} . [source] get_batch_query # FeatureView . get_batch_query ( start_time = None , end_time = None ) Get a query string of batch query. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Optional. Start time of the batch query. datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Optional. End time of the batch query. datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Returns str : batch query [source] get_feature_vector # FeatureView . get_feature_vector ( entry , passed_features = {}, external = None ) Returns assembled serving vector from online feature store. Arguments entry List[Dict[str, Any]] : dictionary of feature group primary key and values provided by serving application. passed_features Optional[Dict[str, Any]] : dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in the feature view query. [source] get_feature_vectors # FeatureView . get_feature_vectors ( entry , passed_features = {}, external = None ) Returns assembled serving vectors in batches from online feature store. Arguments entry List[Dict[str, Any]] : a list of dictionary of feature group primary key and values provided by serving application. passed_features Optional[List[Dict[str, Any]]] : a list of dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in the feature view query. [source] get_tag # FeatureView . get_tag ( name ) [source] get_tags # FeatureView . get_tags () [source] get_train_test_split # FeatureView . get_train_test_split ( training_dataset_version , read_options = None ) Get training data from storage or feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments version : training dataset version read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source] get_train_validation_test_split # FeatureView . get_train_validation_test_split ( training_dataset_version , read_options = None ) Get training data from storage or feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments version : training dataset version read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source] get_training_data # FeatureView . get_training_data ( training_dataset_version , read_options = None ) Get training data from storage or feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. External Storage Support Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client. Arguments version : training dataset version read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X, y): Tuple of dataframe of features and labels [source] get_training_dataset_tag # FeatureView . get_training_dataset_tag ( training_dataset_version , name ) [source] get_training_dataset_tags # FeatureView . get_training_dataset_tags ( training_dataset_version ) [source] init_batch_scoring # FeatureView . init_batch_scoring ( training_dataset_version = None ) Initialise and cache parametrized transformation functions. Arguments training_dataset_version Optional[int] : int, optional. Default to be None. Transformation statistics are fetched from training dataset and apply in serving vector. [source] init_serving # FeatureView . init_serving ( training_dataset_version = None , external = None ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments training_dataset_version Optional[int] : int, optional. Default to be 1. Transformation statistics are fetched from training dataset and apply in serving vector. batch : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. [source] json # FeatureView . json () [source] purge_all_training_data # FeatureView . purge_all_training_data () [source] purge_training_data # FeatureView . purge_training_data ( version ) [source] recreate_training_dataset # FeatureView . recreate_training_dataset ( version , write_options = None ) Recreate a training dataset. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments version int : training dataset version read_options : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source] to_dict # FeatureView . to_dict () [source] train_test_split # FeatureView . train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , ) Get training data from feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source] train_validation_test_split # FeatureView . train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , ) Get training data from feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments validation_size Optional[float] : size of validation set. Should be between 0 and 1. test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source] training_data # FeatureView . training_data ( start_time = None , end_time = None , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , ) Get training data from feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X, y): Tuple of dataframe of features and labels. If there are no labels, y returns None . [source] update # FeatureView . update () [source] update_from_response_json # FeatureView . update_from_response_json ( json_dict )","title":"FeatureView"},{"location":"generated/api/feature_view_api/#feature-view","text":"[source]","title":"Feature View"},{"location":"generated/api/feature_view_api/#featureview","text":"hsfs . feature_view . FeatureView ( name , query , featurestore_id , id = None , version = None , description = \"\" , labels = [], transformation_functions = {}, )","title":"FeatureView"},{"location":"generated/api/feature_view_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_view_api/#create_feature_view","text":"FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , labels = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Warning as_of argument in the Query will be ignored because feature view does not support time travel query. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . labels Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object.","title":"create_feature_view"},{"location":"generated/api/feature_view_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_view_api/#get_feature_view","text":"FeatureStore . get_feature_view ( name , version = None ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name str : Name of the feature view to get. version Optional[int] : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_view"},{"location":"generated/api/feature_view_api/#get_feature_views","text":"FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store.","title":"get_feature_views"},{"location":"generated/api/feature_view_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_view_api/#description","text":"[source]","title":"description"},{"location":"generated/api/feature_view_api/#featurestore_id","text":"Feature store id. [source]","title":"featurestore_id"},{"location":"generated/api/feature_view_api/#id","text":"Feature view id. [source]","title":"id"},{"location":"generated/api/feature_view_api/#labels","text":"The labels/prediction feature of the feature view. Can be a composite of multiple features. [source]","title":"labels"},{"location":"generated/api/feature_view_api/#name","text":"Name of the feature view. [source]","title":"name"},{"location":"generated/api/feature_view_api/#primary_keys","text":"Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source]","title":"primary_keys"},{"location":"generated/api/feature_view_api/#query","text":"[source]","title":"query"},{"location":"generated/api/feature_view_api/#schema","text":"Feature view schema. [source]","title":"schema"},{"location":"generated/api/feature_view_api/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/api/feature_view_api/#version","text":"Version number of the feature view.","title":"version"},{"location":"generated/api/feature_view_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_view_api/#add_tag","text":"FeatureView . add_tag ( name , value ) [source]","title":"add_tag"},{"location":"generated/api/feature_view_api/#add_training_dataset_tag","text":"FeatureView . add_training_dataset_tag ( training_dataset_version , name , value ) [source]","title":"add_training_dataset_tag"},{"location":"generated/api/feature_view_api/#clean","text":"FeatureView . clean ( feature_store_id , feature_view_name , feature_view_version ) Delete the feature view and all associated metadata. Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Arguments feature_store_id int : int. Id of feature store. feature_view_name str : str. Name of feature view. feature_view_version str : str. Version of feature view. Raises RestAPIError . [source]","title":"clean"},{"location":"generated/api/feature_view_api/#create_train_test_split","text":"FeatureView . create_train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"csv\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, ) Create a training dataset and save data into location . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following ormats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"csv\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"create_train_test_split"},{"location":"generated/api/feature_view_api/#create_train_validation_test_split","text":"FeatureView . create_train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"csv\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, ) Create a training dataset and save data into location . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments validation_size Optional[float] : size of validation set. test_size Optional[float] : size of test set. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : tdatatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"csv\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"create_train_validation_test_split"},{"location":"generated/api/feature_view_api/#create_training_data","text":"FeatureView . create_training_data ( start_time = \"\" , end_time = \"\" , storage_connector = None , location = \"\" , description = \"\" , extra_filter = None , data_format = \"csv\" , coalesce = False , seed = None , statistics_config = None , write_options = {}, ) Create a training dataset and save data into location . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"csv\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns (td_version, Job ): Tuple of training dataset version and job. When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"create_training_data"},{"location":"generated/api/feature_view_api/#delete","text":"FeatureView . delete () Delete current feature view and all associated metadata. Potentially dangerous operation This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_view_api/#delete_all_training_datasets","text":"FeatureView . delete_all_training_datasets () [source]","title":"delete_all_training_datasets"},{"location":"generated/api/feature_view_api/#delete_tag","text":"FeatureView . delete_tag ( name ) [source]","title":"delete_tag"},{"location":"generated/api/feature_view_api/#delete_training_dataset","text":"FeatureView . delete_training_dataset ( version ) [source]","title":"delete_training_dataset"},{"location":"generated/api/feature_view_api/#delete_training_dataset_tag","text":"FeatureView . delete_training_dataset_tag ( training_dataset_version , name ) [source]","title":"delete_training_dataset_tag"},{"location":"generated/api/feature_view_api/#from_response_json","text":"FeatureView . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_view_api/#get_batch_data","text":"FeatureView . get_batch_data ( start_time = None , end_time = None , read_options = None ) Get a batch of data from an event time interval. Arguments start_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . read_options : User provided read options. Defaults to {} . [source]","title":"get_batch_data"},{"location":"generated/api/feature_view_api/#get_batch_query","text":"FeatureView . get_batch_query ( start_time = None , end_time = None ) Get a query string of batch query. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Optional. Start time of the batch query. datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : Optional. End time of the batch query. datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Returns str : batch query [source]","title":"get_batch_query"},{"location":"generated/api/feature_view_api/#get_feature_vector","text":"FeatureView . get_feature_vector ( entry , passed_features = {}, external = None ) Returns assembled serving vector from online feature store. Arguments entry List[Dict[str, Any]] : dictionary of feature group primary key and values provided by serving application. passed_features Optional[Dict[str, Any]] : dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in the feature view query. [source]","title":"get_feature_vector"},{"location":"generated/api/feature_view_api/#get_feature_vectors","text":"FeatureView . get_feature_vectors ( entry , passed_features = {}, external = None ) Returns assembled serving vectors in batches from online feature store. Arguments entry List[Dict[str, Any]] : a list of dictionary of feature group primary key and values provided by serving application. passed_features Optional[List[Dict[str, Any]]] : a list of dictionary of feature values provided by the application at runtime. They can replace features values fetched from the feature store as well as providing feature values which are not available in the feature store. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in the feature view query. [source]","title":"get_feature_vectors"},{"location":"generated/api/feature_view_api/#get_tag","text":"FeatureView . get_tag ( name ) [source]","title":"get_tag"},{"location":"generated/api/feature_view_api/#get_tags","text":"FeatureView . get_tags () [source]","title":"get_tags"},{"location":"generated/api/feature_view_api/#get_train_test_split","text":"FeatureView . get_train_test_split ( training_dataset_version , read_options = None ) Get training data from storage or feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments version : training dataset version read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source]","title":"get_train_test_split"},{"location":"generated/api/feature_view_api/#get_train_validation_test_split","text":"FeatureView . get_train_validation_test_split ( training_dataset_version , read_options = None ) Get training data from storage or feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments version : training dataset version read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source]","title":"get_train_validation_test_split"},{"location":"generated/api/feature_view_api/#get_training_data","text":"FeatureView . get_training_data ( training_dataset_version , read_options = None ) Get training data from storage or feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. External Storage Support Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client. Arguments version : training dataset version read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X, y): Tuple of dataframe of features and labels [source]","title":"get_training_data"},{"location":"generated/api/feature_view_api/#get_training_dataset_tag","text":"FeatureView . get_training_dataset_tag ( training_dataset_version , name ) [source]","title":"get_training_dataset_tag"},{"location":"generated/api/feature_view_api/#get_training_dataset_tags","text":"FeatureView . get_training_dataset_tags ( training_dataset_version ) [source]","title":"get_training_dataset_tags"},{"location":"generated/api/feature_view_api/#init_batch_scoring","text":"FeatureView . init_batch_scoring ( training_dataset_version = None ) Initialise and cache parametrized transformation functions. Arguments training_dataset_version Optional[int] : int, optional. Default to be None. Transformation statistics are fetched from training dataset and apply in serving vector. [source]","title":"init_batch_scoring"},{"location":"generated/api/feature_view_api/#init_serving","text":"FeatureView . init_serving ( training_dataset_version = None , external = None ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments training_dataset_version Optional[int] : int, optional. Default to be 1. Transformation statistics are fetched from training dataset and apply in serving vector. batch : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. [source]","title":"init_serving"},{"location":"generated/api/feature_view_api/#json","text":"FeatureView . json () [source]","title":"json"},{"location":"generated/api/feature_view_api/#purge_all_training_data","text":"FeatureView . purge_all_training_data () [source]","title":"purge_all_training_data"},{"location":"generated/api/feature_view_api/#purge_training_data","text":"FeatureView . purge_training_data ( version ) [source]","title":"purge_training_data"},{"location":"generated/api/feature_view_api/#recreate_training_dataset","text":"FeatureView . recreate_training_dataset ( version , write_options = None ) Recreate a training dataset. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments version int : training dataset version read_options : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. [source]","title":"recreate_training_dataset"},{"location":"generated/api/feature_view_api/#to_dict","text":"FeatureView . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_view_api/#train_test_split","text":"FeatureView . train_test_split ( test_size = None , train_start = \"\" , train_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , ) Get training data from feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_test, y_train, y_test): Tuple of dataframe of features and labels [source]","title":"train_test_split"},{"location":"generated/api/feature_view_api/#train_validation_test_split","text":"FeatureView . train_validation_test_split ( validation_size = None , test_size = None , train_start = \"\" , train_end = \"\" , validation_start = \"\" , validation_end = \"\" , test_start = \"\" , test_end = \"\" , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , ) Get training data from feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments validation_size Optional[float] : size of validation set. Should be between 0 and 1. test_size Optional[float] : size of test set. Should be between 0 and 1. train_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . train_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . validation_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_start Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . test_end Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X_train, X_val, X_test, y_train, y_val, y_test): Tuple of dataframe of features and labels [source]","title":"train_validation_test_split"},{"location":"generated/api/feature_view_api/#training_data","text":"FeatureView . training_data ( start_time = None , end_time = None , description = \"\" , extra_filter = None , statistics_config = None , read_options = None , ) Get training data from feature groups. Info If a materialised training data has deleted. Use recreate_training_dataset() to recreate the training data. Arguments start_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . end_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. read_options Optional[Dict[Any, Any]] : Additional read options as key-value pairs, defaults to {} . When using the python engine, read_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. Returns (X, y): Tuple of dataframe of features and labels. If there are no labels, y returns None . [source]","title":"training_data"},{"location":"generated/api/feature_view_api/#update","text":"FeatureView . update () [source]","title":"update"},{"location":"generated/api/feature_view_api/#update_from_response_json","text":"FeatureView . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/job_configuration/","text":"[source] JobConfiguration # hsfs . core . job_configuration . JobConfiguration ( am_memory = 1024 , am_cores = 1 , executor_memory = 2048 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , )","title":"Job configuration"},{"location":"generated/api/job_configuration/#jobconfiguration","text":"hsfs . core . job_configuration . JobConfiguration ( am_memory = 1024 , am_cores = 1 , executor_memory = 2048 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , )","title":"JobConfiguration"},{"location":"generated/api/query_api/","text":"Query # Query objects are strictly generated by HSFS APIs called on Feature Group objects . Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here. Methods # [source] append_feature # Query . append_feature ( feature ) [source] as_of # Query . as_of ( wallclock_time = None , exclude_until = None ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: query1 . as_of ( ... , ... ) . join ( query2 . as_of ( ... , ... )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: query1 . as_of ( ... , ... ) # as_of is not applied . join ( query2 . as_of ( ... , ... )) # as_of is not applied . as_of ( ... , ... ) Warning This function only works for queries on feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . exclude_until : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source] filter # Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_cache_feature_group_only # Query . from_cache_feature_group_only () [source] from_response_json # Query . from_response_json ( json_dict ) [source] is_time_travel # Query . is_time_travel () [source] join # Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source] pull_changes # Query . pull_changes ( wallclock_start_time , wallclock_end_time ) Deprecated pull_changes method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead. [source] read # Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). External Feature Group Engine Support Spark only Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source] show # Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source] to_string # Query . to_string ( online = False ) Properties # [source] features # [source] left_feature_group_end_time # [source] left_feature_group_start_time #","title":"Query"},{"location":"generated/api/query_api/#query","text":"Query objects are strictly generated by HSFS APIs called on Feature Group objects . Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here.","title":"Query"},{"location":"generated/api/query_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/query_api/#append_feature","text":"Query . append_feature ( feature ) [source]","title":"append_feature"},{"location":"generated/api/query_api/#as_of","text":"Query . as_of ( wallclock_time = None , exclude_until = None ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Reading features at a specific point in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" ) . read () . show () Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:34:11\" , exclude_until = \"2020-10-19 07:34:11\" ) . read () . show () The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit. Reading only the changes from a single commit fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( \"2020-10-20 07:31:38\" , exclude_until = \"2020-10-20 07:31:37\" ) . read () . show () When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded. Reading the latest state of features, excluding commits before a specified point in time: fs = connection . get_feature_store (); query = fs . get_feature_group ( \"example_feature_group\" , 1 ) . select_all () query . as_of ( None , exclude_until = \"2020-10-20 07:31:38\" ) . read () . show () Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: query1 . as_of ( ... , ... ) . join ( query2 . as_of ( ... , ... )) If instead you apply another as_of selection after the join, all joined feature groups will be queried with this interval: query1 . as_of ( ... , ... ) # as_of is not applied . join ( query2 . as_of ( ... , ... )) # as_of is not applied . as_of ( ... , ... ) Warning This function only works for queries on feature groups with time_travel_format='HUDI'. Warning Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: hoodie.keep.min.commits and hoodie.keep.max.commits when calling the insert() method. Arguments wallclock_time : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . exclude_until : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/api/query_api/#filter","text":"Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/query_api/#from_cache_feature_group_only","text":"Query . from_cache_feature_group_only () [source]","title":"from_cache_feature_group_only"},{"location":"generated/api/query_api/#from_response_json","text":"Query . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/query_api/#is_time_travel","text":"Query . is_time_travel () [source]","title":"is_time_travel"},{"location":"generated/api/query_api/#join","text":"Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source]","title":"join"},{"location":"generated/api/query_api/#pull_changes","text":"Query . pull_changes ( wallclock_start_time , wallclock_end_time ) Deprecated pull_changes method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead. [source]","title":"pull_changes"},{"location":"generated/api/query_api/#read","text":"Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). External Feature Group Engine Support Spark only Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups. Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source]","title":"read"},{"location":"generated/api/query_api/#show","text":"Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source]","title":"show"},{"location":"generated/api/query_api/#to_string","text":"Query . to_string ( online = False )","title":"to_string"},{"location":"generated/api/query_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/query_api/#features","text":"[source]","title":"features"},{"location":"generated/api/query_api/#left_feature_group_end_time","text":"[source]","title":"left_feature_group_end_time"},{"location":"generated/api/query_api/#left_feature_group_start_time","text":"","title":"left_feature_group_start_time"},{"location":"generated/api/rule_api/","text":"Rule # {{rule}} Properties # {{rule_properties}}","title":"Rule"},{"location":"generated/api/rule_api/#rule","text":"{{rule}}","title":"Rule"},{"location":"generated/api/rule_api/#properties","text":"{{rule_properties}}","title":"Properties"},{"location":"generated/api/rule_definition_api/","text":"Rule Definition # {{ruledefinition}} Properties # {{ruledefinition_properties}} Retrieval # {{ruledefinition_getall}} {{ruledefinition_get}}","title":"Rule Definition"},{"location":"generated/api/rule_definition_api/#rule-definition","text":"{{ruledefinition}}","title":"Rule Definition"},{"location":"generated/api/rule_definition_api/#properties","text":"{{ruledefinition_properties}}","title":"Properties"},{"location":"generated/api/rule_definition_api/#retrieval","text":"{{ruledefinition_getall}} {{ruledefinition_get}}","title":"Retrieval"},{"location":"generated/api/statistics_config_api/","text":"StatisticsConfig # [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [] ) Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] exact_uniqueness # Enable exact uniqueness as an additional statistic to be computed for each feature. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig","text":"[source]","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [] )","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/statistics_config_api/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/api/statistics_config_api/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/api/statistics_config_api/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/api/statistics_config_api/#exact_uniqueness","text":"Enable exact uniqueness as an additional statistic to be computed for each feature. [source]","title":"exact_uniqueness"},{"location":"generated/api/statistics_config_api/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/api/storage_connector_api/","text":"Storage Connector # Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. HopsFS # Properties # [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # HopsFSConnector . refetch () Refetch storage connector. [source] spark_options # HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # HopsFSConnector . to_dict () [source] update_from_response_json # HopsFSConnector . update_from_response_json ( json_dict ) JDBC # Properties # [source] arguments # Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source] connection_string # JDBC connection string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # JdbcConnector . refetch () Refetch storage connector. [source] spark_options # JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # JdbcConnector . to_dict () [source] update_from_response_json # JdbcConnector . update_from_response_json ( json_dict ) S3 # Properties # [source] access_key # Access key. [source] bucket # Return the bucket for S3 connectors. [source] description # User provided description of the storage connector. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] session_token # Session token. Methods # [source] prepare_spark # S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # S3Connector . refetch () Refetch storage connector. [source] spark_options # S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # S3Connector . to_dict () [source] update_from_response_json # S3Connector . update_from_response_json ( json_dict ) Redshift # Properties # [source] arguments # Additional JDBC, REDSHIFT, or Snowflake arguments. [source] auto_create # Database username for redshift cluster. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] description # User provided description of the storage connector. [source] expiration # Cluster temporary credential expiration time. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] table_name # Table name for redshift cluster. Methods # [source] read # RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # RedshiftConnector . to_dict () [source] update_from_response_json # RedshiftConnector . update_from_response_json ( json_dict ) Azure Data Lake Storage # Properties # [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] container_name # Container name of the ADLS storage connector [source] description # User provided description of the storage connector. [source] directory_id # Directory ID of the ADLS storage connector [source] generation # Generation of the ADLS storage connector [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. ADLS) - return the path of the connector [source] service_credential # Service credential of the ADLS storage connector Methods # [source] prepare_spark # AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # AdlsConnector . refetch () Refetch storage connector. [source] spark_options # AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # AdlsConnector . to_dict () [source] update_from_response_json # AdlsConnector . update_from_response_json ( json_dict ) Snowflake # Properties # [source] account # Account of the Snowflake storage connector [source] application # Application of the Snowflake storage connector [source] database # Database of the Snowflake storage connector [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Additional options for the Snowflake storage connector [source] password # Password of the Snowflake storage connector [source] role # Role of the Snowflake storage connector [source] schema # Schema of the Snowflake storage connector [source] table # Table of the Snowflake storage connector [source] token # OAuth token of the Snowflake storage connector [source] url # URL of the Snowflake storage connector [source] user # User of the Snowflake storage connector [source] warehouse # Warehouse of the Snowflake storage connector Methods # [source] read # SnowflakeConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # SnowflakeConnector . refetch () Refetch storage connector. [source] snowflake_connector_options # SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source] spark_options # SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # SnowflakeConnector . to_dict () [source] update_from_response_json # SnowflakeConnector . update_from_response_json ( json_dict ) Google Cloud Storage # This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Properties # [source] algorithm # Encryption Algorithm [source] bucket # GCS Bucket [source] description # User provided description of the storage connector. [source] encryption_key # Encryption Key [source] encryption_key_hash # Encryption Key Hash [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] name # Name of the storage connector. [source] path # the path of the connector along with gs file system prefixed Methods # [source] prepare_spark # GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source] read # GcsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads GCS path into a dataframe using the storage connector. conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # GcsConnector . refetch () Refetch storage connector. [source] spark_options # GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # GcsConnector . to_dict () [source] update_from_response_json # GcsConnector . update_from_response_json ( json_dict ) BigQuery # The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Properties # [source] arguments # Additional spark options [source] dataset # BigQuery dataset (The dataset containing the table) [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] materialization_dataset # BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source] name # Name of the storage connector. [source] parent_project # BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source] query_project # BigQuery project (The Google Cloud Project ID of the table) [source] query_table # BigQuery table name Methods # [source] read # BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # BigQueryConnector . refetch () Refetch storage connector. [source] spark_options # BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source] to_dict # BigQueryConnector . to_dict () [source] update_from_response_json # BigQueryConnector . update_from_response_json ( json_dict ) Kafka # Properties # [source] boostrap_servers # Bootstrap servers string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Bootstrap servers string. [source] security_protocol # Bootstrap servers string. [source] ssl_endpoint_identification_algorithm # Bootstrap servers string. [source] ssl_keystore_location # Bootstrap servers string. [source] ssl_truststore_location # Bootstrap servers string. Methods # [source] read # KafkaConnector . read ( query = None , data_format = None , options = {}, path = None ) NOT SUPPORTED. [source] read_stream # KafkaConnector . read_stream ( topic , topic_pattern = False , message_format = \"avro\" , schema = None , options = {}, include_metadata = False , ) Reads a Kafka stream from a topic or multiple topics into a Dataframe. Engine Support Spark only Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Arguments topic str : Name or pattern of the topic(s) to subscribe to. topic_pattern bool : Flag to indicate if topic string is a pattern. Defaults to False . message_format str : The format of the messages to use for decoding. Can be \"avro\" or \"json\" . Defaults to \"avro\" . schema Optional[str] : Optional schema, to use for decoding, can be an Avro schema string for \"avro\" message format, or for JSON encoding a Spark StructType schema, or a DDL formatted string. Defaults to None . options dict : Additional options as key/value string pairs to be passed to Spark. Defaults to {} . include_metadata bool : Indicate whether to return additional metadata fields from messages in the stream. Otherwise only the decoded value fields are returned. Defaults to False . Raises ValueError : Malformed arguments. Returns StreamingDataframe : A Spark streaming dataframe. [source] refetch # KafkaConnector . refetch () Refetch storage connector. [source] spark_options # KafkaConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # KafkaConnector . to_dict () [source] update_from_response_json # KafkaConnector . update_from_response_json ( json_dict )","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#storage-connector","text":"","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/storage_connector_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/api/storage_connector_api/#hopsfs","text":"","title":"HopsFS"},{"location":"generated/api/storage_connector_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#description","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read","text":"HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch","text":"HopsFSConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options","text":"HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict","text":"HopsFSConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json","text":"HopsFSConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#jdbc","text":"","title":"JDBC"},{"location":"generated/api/storage_connector_api/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments","text":"Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/api/storage_connector_api/#description_1","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_1","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_1","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods_1","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_1","text":"JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_1","text":"JdbcConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_1","text":"JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_1","text":"JdbcConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_1","text":"JdbcConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#s3","text":"","title":"S3"},{"location":"generated/api/storage_connector_api/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/api/storage_connector_api/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_2","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_2","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_2","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/api/storage_connector_api/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/api/storage_connector_api/#session_token","text":"Session token.","title":"session_token"},{"location":"generated/api/storage_connector_api/#methods_2","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark","text":"S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_2","text":"S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_2","text":"S3Connector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_2","text":"S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_2","text":"S3Connector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_2","text":"S3Connector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#redshift","text":"","title":"Redshift"},{"location":"generated/api/storage_connector_api/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_1","text":"Additional JDBC, REDSHIFT, or Snowflake arguments. [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/api/storage_connector_api/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/api/storage_connector_api/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/api/storage_connector_api/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/api/storage_connector_api/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/api/storage_connector_api/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/api/storage_connector_api/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/api/storage_connector_api/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/api/storage_connector_api/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/api/storage_connector_api/#description_3","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/api/storage_connector_api/#iam_role_1","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_3","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_3","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/api/storage_connector_api/#methods_3","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_3","text":"RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_3","text":"RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_3","text":"RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_3","text":"RedshiftConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_3","text":"RedshiftConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#azure-data-lake-storage","text":"","title":"Azure Data Lake Storage"},{"location":"generated/api/storage_connector_api/#properties_4","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/api/storage_connector_api/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/api/storage_connector_api/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/api/storage_connector_api/#description_4","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/api/storage_connector_api/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/api/storage_connector_api/#id_4","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_4","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_1","text":"If the connector refers to a path (e.g. ADLS) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#service_credential","text":"Service credential of the ADLS storage connector","title":"service_credential"},{"location":"generated/api/storage_connector_api/#methods_4","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark_1","text":"AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_4","text":"AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_4","text":"AdlsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_4","text":"AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_4","text":"AdlsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_4","text":"AdlsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#snowflake","text":"","title":"Snowflake"},{"location":"generated/api/storage_connector_api/#properties_5","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account","text":"Account of the Snowflake storage connector [source]","title":"account"},{"location":"generated/api/storage_connector_api/#application","text":"Application of the Snowflake storage connector [source]","title":"application"},{"location":"generated/api/storage_connector_api/#database","text":"Database of the Snowflake storage connector [source]","title":"database"},{"location":"generated/api/storage_connector_api/#description_5","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_5","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_5","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#options","text":"Additional options for the Snowflake storage connector [source]","title":"options"},{"location":"generated/api/storage_connector_api/#password","text":"Password of the Snowflake storage connector [source]","title":"password"},{"location":"generated/api/storage_connector_api/#role","text":"Role of the Snowflake storage connector [source]","title":"role"},{"location":"generated/api/storage_connector_api/#schema","text":"Schema of the Snowflake storage connector [source]","title":"schema"},{"location":"generated/api/storage_connector_api/#table","text":"Table of the Snowflake storage connector [source]","title":"table"},{"location":"generated/api/storage_connector_api/#token","text":"OAuth token of the Snowflake storage connector [source]","title":"token"},{"location":"generated/api/storage_connector_api/#url","text":"URL of the Snowflake storage connector [source]","title":"url"},{"location":"generated/api/storage_connector_api/#user","text":"User of the Snowflake storage connector [source]","title":"user"},{"location":"generated/api/storage_connector_api/#warehouse","text":"Warehouse of the Snowflake storage connector","title":"warehouse"},{"location":"generated/api/storage_connector_api/#methods_5","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_5","text":"SnowflakeConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_5","text":"SnowflakeConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#snowflake_connector_options","text":"SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source]","title":"snowflake_connector_options"},{"location":"generated/api/storage_connector_api/#spark_options_5","text":"SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_5","text":"SnowflakeConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_5","text":"SnowflakeConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#google-cloud-storage","text":"This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop","title":"Google Cloud Storage"},{"location":"generated/api/storage_connector_api/#properties_6","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#algorithm","text":"Encryption Algorithm [source]","title":"algorithm"},{"location":"generated/api/storage_connector_api/#bucket_1","text":"GCS Bucket [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_6","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#encryption_key","text":"Encryption Key [source]","title":"encryption_key"},{"location":"generated/api/storage_connector_api/#encryption_key_hash","text":"Encryption Key Hash [source]","title":"encryption_key_hash"},{"location":"generated/api/storage_connector_api/#id_6","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#key_path","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/api/storage_connector_api/#name_6","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_2","text":"the path of the connector along with gs file system prefixed","title":"path"},{"location":"generated/api/storage_connector_api/#methods_6","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark_2","text":"GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_6","text":"GcsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads GCS path into a dataframe using the storage connector. conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_6","text":"GcsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_6","text":"GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_6","text":"GcsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_6","text":"GcsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#bigquery","text":"The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.","title":"BigQuery"},{"location":"generated/api/storage_connector_api/#properties_7","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_2","text":"Additional spark options [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#dataset","text":"BigQuery dataset (The dataset containing the table) [source]","title":"dataset"},{"location":"generated/api/storage_connector_api/#description_7","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_7","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#key_path_1","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/api/storage_connector_api/#materialization_dataset","text":"BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source]","title":"materialization_dataset"},{"location":"generated/api/storage_connector_api/#name_7","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#parent_project","text":"BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source]","title":"parent_project"},{"location":"generated/api/storage_connector_api/#query_project","text":"BigQuery project (The Google Cloud Project ID of the table) [source]","title":"query_project"},{"location":"generated/api/storage_connector_api/#query_table","text":"BigQuery table name","title":"query_table"},{"location":"generated/api/storage_connector_api/#methods_7","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_7","text":"BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_7","text":"BigQueryConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_7","text":"BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_7","text":"BigQueryConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_7","text":"BigQueryConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#kafka","text":"","title":"Kafka"},{"location":"generated/api/storage_connector_api/#properties_8","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#boostrap_servers","text":"Bootstrap servers string. [source]","title":"boostrap_servers"},{"location":"generated/api/storage_connector_api/#description_8","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_8","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_8","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#options_1","text":"Bootstrap servers string. [source]","title":"options"},{"location":"generated/api/storage_connector_api/#security_protocol","text":"Bootstrap servers string. [source]","title":"security_protocol"},{"location":"generated/api/storage_connector_api/#ssl_endpoint_identification_algorithm","text":"Bootstrap servers string. [source]","title":"ssl_endpoint_identification_algorithm"},{"location":"generated/api/storage_connector_api/#ssl_keystore_location","text":"Bootstrap servers string. [source]","title":"ssl_keystore_location"},{"location":"generated/api/storage_connector_api/#ssl_truststore_location","text":"Bootstrap servers string.","title":"ssl_truststore_location"},{"location":"generated/api/storage_connector_api/#methods_8","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_8","text":"KafkaConnector . read ( query = None , data_format = None , options = {}, path = None ) NOT SUPPORTED. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#read_stream","text":"KafkaConnector . read_stream ( topic , topic_pattern = False , message_format = \"avro\" , schema = None , options = {}, include_metadata = False , ) Reads a Kafka stream from a topic or multiple topics into a Dataframe. Engine Support Spark only Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming. Arguments topic str : Name or pattern of the topic(s) to subscribe to. topic_pattern bool : Flag to indicate if topic string is a pattern. Defaults to False . message_format str : The format of the messages to use for decoding. Can be \"avro\" or \"json\" . Defaults to \"avro\" . schema Optional[str] : Optional schema, to use for decoding, can be an Avro schema string for \"avro\" message format, or for JSON encoding a Spark StructType schema, or a DDL formatted string. Defaults to None . options dict : Additional options as key/value string pairs to be passed to Spark. Defaults to {} . include_metadata bool : Indicate whether to return additional metadata fields from messages in the stream. Otherwise only the decoded value fields are returned. Defaults to False . Raises ValueError : Malformed arguments. Returns StreamingDataframe : A Spark streaming dataframe. [source]","title":"read_stream"},{"location":"generated/api/storage_connector_api/#refetch_8","text":"KafkaConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_8","text":"KafkaConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_8","text":"KafkaConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_8","text":"KafkaConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/","text":"Training Dataset # [source] TrainingDataset # hsfs . training_dataset . TrainingDataset ( name , version , data_format , featurestore_id , location = \"\" , event_start_time = None , event_end_time = None , coalesce = False , description = None , storage_connector = None , splits = None , validation_size = None , test_size = None , train_start = None , train_end = None , validation_start = None , validation_end = None , test_start = None , test_end = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , train_split = None , time_split_size = None , extra_filter = None , ) Creation # [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] coalesce # If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source] data_format # File format of the training dataset. [source] description # [source] event_end_time # [source] event_start_time # [source] extra_filter # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] test_end # [source] test_size # [source] test_start # [source] train_end # [source] train_split # Set name of training dataset split that is used for training. [source] train_start # [source] training_dataset_type # [source] transformation_functions # Set transformation functions. [source] validation_end # [source] validation_size # [source] validation_start # [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete # TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] from_response_json # TrainingDataset . from_response_json ( json_dict ) [source] from_response_json_single # TrainingDataset . from_response_json_single ( json_dict ) [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_serving_vector # TrainingDataset . get_serving_vector ( entry , external = None ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_serving_vectors # TrainingDataset . get_serving_vectors ( entry , external = None ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] init_prepared_statement # TrainingDataset . init_prepared_statement ( batch = None , external = None ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] json # TrainingDataset . json () [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Engine Support Creating Training Datasets from Dataframes is only supported using Spark as Engine. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] serving_keys # TrainingDataset . serving_keys () Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] to_dict # TrainingDataset . to_dict () [source] update_from_response_json # TrainingDataset . update_from_response_json ( json_dict ) [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#training-dataset","text":"[source]","title":"Training Dataset"},{"location":"generated/api/training_dataset_api/#trainingdataset","text":"hsfs . training_dataset . TrainingDataset ( name , version , data_format , featurestore_id , location = \"\" , event_start_time = None , event_end_time = None , coalesce = False , description = None , storage_connector = None , splits = None , validation_size = None , test_size = None , train_start = None , train_end = None , validation_start = None , validation_end = None , test_start = None , test_end = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , train_split = None , time_split_size = None , extra_filter = None , )","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/training_dataset_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Deprecated TrainingDataset is deprecated, use FeatureView instead. From version 3.0 training datasets created with this API are not visibile in the API anymore. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/api/training_dataset_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/training_dataset_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Deprecated TrainingDataset is deprecated, use FeatureView instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version. It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/api/training_dataset_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/training_dataset_api/#coalesce","text":"If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source]","title":"coalesce"},{"location":"generated/api/training_dataset_api/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/api/training_dataset_api/#description","text":"[source]","title":"description"},{"location":"generated/api/training_dataset_api/#event_end_time","text":"[source]","title":"event_end_time"},{"location":"generated/api/training_dataset_api/#event_start_time","text":"[source]","title":"event_start_time"},{"location":"generated/api/training_dataset_api/#extra_filter","text":"[source]","title":"extra_filter"},{"location":"generated/api/training_dataset_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/training_dataset_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/training_dataset_api/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/api/training_dataset_api/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/api/training_dataset_api/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/api/training_dataset_api/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/api/training_dataset_api/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/api/training_dataset_api/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/api/training_dataset_api/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/api/training_dataset_api/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/api/training_dataset_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/api/training_dataset_api/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/api/training_dataset_api/#test_end","text":"[source]","title":"test_end"},{"location":"generated/api/training_dataset_api/#test_size","text":"[source]","title":"test_size"},{"location":"generated/api/training_dataset_api/#test_start","text":"[source]","title":"test_start"},{"location":"generated/api/training_dataset_api/#train_end","text":"[source]","title":"train_end"},{"location":"generated/api/training_dataset_api/#train_split","text":"Set name of training dataset split that is used for training. [source]","title":"train_split"},{"location":"generated/api/training_dataset_api/#train_start","text":"[source]","title":"train_start"},{"location":"generated/api/training_dataset_api/#training_dataset_type","text":"[source]","title":"training_dataset_type"},{"location":"generated/api/training_dataset_api/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/api/training_dataset_api/#validation_end","text":"[source]","title":"validation_end"},{"location":"generated/api/training_dataset_api/#validation_size","text":"[source]","title":"validation_size"},{"location":"generated/api/training_dataset_api/#validation_start","text":"[source]","title":"validation_start"},{"location":"generated/api/training_dataset_api/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/api/training_dataset_api/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/api/training_dataset_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/training_dataset_api/#add_tag","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/training_dataset_api/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/api/training_dataset_api/#delete","text":"TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/training_dataset_api/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/training_dataset_api/#from_response_json","text":"TrainingDataset . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/training_dataset_api/#from_response_json_single","text":"TrainingDataset . from_response_json_single ( json_dict ) [source]","title":"from_response_json_single"},{"location":"generated/api/training_dataset_api/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/api/training_dataset_api/#get_serving_vector","text":"TrainingDataset . get_serving_vector ( entry , external = None ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vector"},{"location":"generated/api/training_dataset_api/#get_serving_vectors","text":"TrainingDataset . get_serving_vectors ( entry , external = None ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vectors"},{"location":"generated/api/training_dataset_api/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[Union[str, int, datetime.datetime, datetime.date]] : datatime.datetime, datetime.date, unix timestamp in seconds (int), or string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , %Y%m%d%H%M%S , or %Y%m%d%H%M%S%f . Defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/api/training_dataset_api/#get_tag","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/training_dataset_api/#get_tags","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/training_dataset_api/#init_prepared_statement","text":"TrainingDataset . init_prepared_statement ( batch = None , external = None ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Defaults to True if connection to Hopsworks is established from external environment (e.g AWS Sagemaker or Google Colab), otherwise to False. [source]","title":"init_prepared_statement"},{"location":"generated/api/training_dataset_api/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/api/training_dataset_api/#json","text":"TrainingDataset . json () [source]","title":"json"},{"location":"generated/api/training_dataset_api/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/api/training_dataset_api/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Engine Support Creating Training Datasets from Dataframes is only supported using Spark as Engine. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/api/training_dataset_api/#serving_keys","text":"TrainingDataset . serving_keys () Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source]","title":"serving_keys"},{"location":"generated/api/training_dataset_api/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/api/training_dataset_api/#to_dict","text":"TrainingDataset . to_dict () [source]","title":"to_dict"},{"location":"generated/api/training_dataset_api/#update_from_response_json","text":"TrainingDataset . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/api/transformation_functions_api/","text":"Transformation Function # [source] TransformationFunction # hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ) Properties # [source] id # Training dataset id. [source] name # [source] output_type # [source] source_code_content # [source] transformation_fn # [source] transformer_code # [source] version # Methods # [source] delete # TransformationFunction . delete () Delete transformation function from backend. [source] save # TransformationFunction . save () Persist transformation function in backend. Creation # [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. Retrieval # [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"Transformation Functions"},{"location":"generated/api/transformation_functions_api/#transformation-function","text":"[source]","title":"Transformation Function"},{"location":"generated/api/transformation_functions_api/#transformationfunction","text":"hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , )","title":"TransformationFunction"},{"location":"generated/api/transformation_functions_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/transformation_functions_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/transformation_functions_api/#name","text":"[source]","title":"name"},{"location":"generated/api/transformation_functions_api/#output_type","text":"[source]","title":"output_type"},{"location":"generated/api/transformation_functions_api/#source_code_content","text":"[source]","title":"source_code_content"},{"location":"generated/api/transformation_functions_api/#transformation_fn","text":"[source]","title":"transformation_fn"},{"location":"generated/api/transformation_functions_api/#transformer_code","text":"[source]","title":"transformer_code"},{"location":"generated/api/transformation_functions_api/#version","text":"","title":"version"},{"location":"generated/api/transformation_functions_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/transformation_functions_api/#delete","text":"TransformationFunction . delete () Delete transformation function from backend. [source]","title":"delete"},{"location":"generated/api/transformation_functions_api/#save","text":"TransformationFunction . save () Persist transformation function in backend.","title":"save"},{"location":"generated/api/transformation_functions_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/transformation_functions_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, bytes, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, float, numpy.float64, datetime.datetime, numpy.datetime64, datetime.date, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object.","title":"create_transformation_function"},{"location":"generated/api/transformation_functions_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/transformation_functions_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/transformation_functions_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"get_transformation_functions"},{"location":"generated/api/validation_api/","text":"Validation # {{validation_result}} Properties # {{validation_result_properties}} Methods # {{expectation_methods}} Validate a dataframe # {{validate}} Retrieval # {{validation_result_get}}","title":"Validation"},{"location":"generated/api/validation_api/#validation","text":"{{validation_result}}","title":"Validation"},{"location":"generated/api/validation_api/#properties","text":"{{validation_result_properties}}","title":"Properties"},{"location":"generated/api/validation_api/#methods","text":"{{expectation_methods}}","title":"Methods"},{"location":"generated/api/validation_api/#validate-a-dataframe","text":"{{validate}}","title":"Validate a dataframe"},{"location":"generated/api/validation_api/#retrieval","text":"{{validation_result_get}}","title":"Retrieval"}]}