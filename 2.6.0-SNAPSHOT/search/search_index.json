{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Feature Store # HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) Feed the training dataset to a TensorFlow model: tf_data_object = training_dataset . tf_data ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = tf_data_object . tf_record_dataset ( batch_size = 32 , num_epochs = 5 , process = True ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository. Documentation # Documentation is available at Hopsworks Feature Store Documentation . Issues # For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Introduction"},{"location":"#hopsworks-feature-store","text":"HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Feature Store"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) Feed the training dataset to a TensorFlow model: tf_data_object = training_dataset . tf_data ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = tf_data_object . tf_record_dataset ( batch_size = 32 , num_epochs = 5 , process = True ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository.","title":"Getting Started On Hopsworks"},{"location":"#documentation","text":"Documentation is available at Hopsworks Feature Store Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[python,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ python,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Option 1: Build only current version of docs # Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve Option 2 (Preferred): Build multi-version doc with mike # Versioning on docs.hopsworks.ai # On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 . Build Instructions # For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ] Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[python,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ python,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","text":"Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve","title":"Option 1: Build only current version of docs"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","text":"","title":"Option 2 (Preferred): Build multi-version doc with mike"},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","text":"On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 .","title":"Versioning on docs.hopsworks.ai"},{"location":"CONTRIBUTING/#build-instructions","text":"For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ]","title":"Build Instructions"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"overview/","text":"Concept Overview # Project-Based Multi-tenancy # Hopsworks implements a dynamic role-based access control model through a project-based multi-tenant security model . Inspired by GDPR, in Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Every Project has an owner with full read-write privileges and zero or more members. An important aspect of Project based multi-tenancy is that assets can be shared between projects. The current assets that can be shared between projects are: files/directories in HopsFS, Hive databases, feature stores , and Kafka topics. Important Sharing assets does not mean that data is duplicated. The Hopsworks Feature Store # The Hopsworks Feature Store is a tool for curating and serving machine learning (ML)features. The Feature Store is a central and unified API between Data Engineers and Data Scientists. Benefits of the Feature Store Manage feature data to profive unified access to machine learning features from small teams to large enterprises. Enable discovery, documentation, sharing and insights into your features through rich metadata. Make feature data available in a performant and scalable way for model training and model inference. Allow point-in-time correct and consistent access to feature data (time travel). Feature Store Concepts # Entities in the Feature Store Entities within the Feature Store are organized hierarchically. On the most granular level are the features itself. Data Engineers ingest the feature data within their organization through the creation of feature groups . Data Scientists are then able to read selected features from the feature groups to create training datasets for model training, run batch inference with deployed models or perform inference from online models by scoring single feature vectors . Feature Vector A Feature Vector is a single row of feature values associated with a primary key. Feature Groups # Feature Groups are entities that contain both metadata about the grouped features, as well as information of the jobs used to ingest the data contained in a feature group and also the actual location of the data (HopsFS or externally, such as S3). Typically, feature groups represent a logical set of features coming from the same data source sharing a common primary key. Feature groups also contain the schema and type information of the features, for the user to know how to interpret the data. Feature groups can also be used to compute Statistics over features, or to define Data Validation Rules using the statistics and schema information. In order to enable online serving for features of a feature group, the feature group needs to be made available as an online feature group. Training Datasets # In order to be able to train machine learning models efficiently, the feature data needs to be materialized as a Training Dataset in the file format most suitable for the ML framework used. For example, when training models with TensorFlow the ideal file format is TensorFlow's tfrecord format. Training datasets can be created with features from any number of feature groups, as long as the feature groups can be joined in a meaningful way. Users are able to compute Statistics also for training datasets, which will make it easy to understand a dataset's characteristics also in the future. The Hopsworks Feature Store has support for writing training datasets either to the distributed file system of Hopsworks - HopsFS - or to external storage such as S3. Offline vs. Online Feature Store # The Feature Store is a dual database-system, to cover all machine learning use cases it consists of high throughput offline storage layer, and additionally a low-latency online storage. The offline storage is mainly used to generate large batches of feature data, for example to be exported as training datasets. Additionally, the offline storage can be used to score large amounts of data with a machine learning model in regular intervals, so called batch inference . The online storage on the other hand is required for online applications, where the goal is to retrieve a single feature vector with the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. An example for online inference would be an e-commerce business, which would like to predict the credit score of a client when he is about to checkout his shopping cart. A client-id will be sent to the online feature store to retrieve the historic features for this customer, which can then be enriched by real time features like the value of his shopping cart, and will then be passed to the machine learning model for inference. Offline vs. Online Feature Store There is no database fullfilling both requirements of very low latency and and high throughput. Therefore, the Hopsworks Feature Store builds on Apache Hive with Apache Hudi as offline storage layer and RonDB as online storage.","title":"Overview"},{"location":"overview/#concept-overview","text":"","title":"Concept Overview"},{"location":"overview/#project-based-multi-tenancy","text":"Hopsworks implements a dynamic role-based access control model through a project-based multi-tenant security model . Inspired by GDPR, in Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Every Project has an owner with full read-write privileges and zero or more members. An important aspect of Project based multi-tenancy is that assets can be shared between projects. The current assets that can be shared between projects are: files/directories in HopsFS, Hive databases, feature stores , and Kafka topics. Important Sharing assets does not mean that data is duplicated.","title":"Project-Based Multi-tenancy"},{"location":"overview/#the-hopsworks-feature-store","text":"The Hopsworks Feature Store is a tool for curating and serving machine learning (ML)features. The Feature Store is a central and unified API between Data Engineers and Data Scientists. Benefits of the Feature Store Manage feature data to profive unified access to machine learning features from small teams to large enterprises. Enable discovery, documentation, sharing and insights into your features through rich metadata. Make feature data available in a performant and scalable way for model training and model inference. Allow point-in-time correct and consistent access to feature data (time travel).","title":"The Hopsworks Feature Store"},{"location":"overview/#feature-store-concepts","text":"Entities in the Feature Store Entities within the Feature Store are organized hierarchically. On the most granular level are the features itself. Data Engineers ingest the feature data within their organization through the creation of feature groups . Data Scientists are then able to read selected features from the feature groups to create training datasets for model training, run batch inference with deployed models or perform inference from online models by scoring single feature vectors . Feature Vector A Feature Vector is a single row of feature values associated with a primary key.","title":"Feature Store Concepts"},{"location":"overview/#feature-groups","text":"Feature Groups are entities that contain both metadata about the grouped features, as well as information of the jobs used to ingest the data contained in a feature group and also the actual location of the data (HopsFS or externally, such as S3). Typically, feature groups represent a logical set of features coming from the same data source sharing a common primary key. Feature groups also contain the schema and type information of the features, for the user to know how to interpret the data. Feature groups can also be used to compute Statistics over features, or to define Data Validation Rules using the statistics and schema information. In order to enable online serving for features of a feature group, the feature group needs to be made available as an online feature group.","title":"Feature Groups"},{"location":"overview/#training-datasets","text":"In order to be able to train machine learning models efficiently, the feature data needs to be materialized as a Training Dataset in the file format most suitable for the ML framework used. For example, when training models with TensorFlow the ideal file format is TensorFlow's tfrecord format. Training datasets can be created with features from any number of feature groups, as long as the feature groups can be joined in a meaningful way. Users are able to compute Statistics also for training datasets, which will make it easy to understand a dataset's characteristics also in the future. The Hopsworks Feature Store has support for writing training datasets either to the distributed file system of Hopsworks - HopsFS - or to external storage such as S3.","title":"Training Datasets"},{"location":"overview/#offline-vs-online-feature-store","text":"The Feature Store is a dual database-system, to cover all machine learning use cases it consists of high throughput offline storage layer, and additionally a low-latency online storage. The offline storage is mainly used to generate large batches of feature data, for example to be exported as training datasets. Additionally, the offline storage can be used to score large amounts of data with a machine learning model in regular intervals, so called batch inference . The online storage on the other hand is required for online applications, where the goal is to retrieve a single feature vector with the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. An example for online inference would be an e-commerce business, which would like to predict the credit score of a client when he is about to checkout his shopping cart. A client-id will be sent to the online feature store to retrieve the historic features for this customer, which can then be enriched by real time features like the value of his shopping cart, and will then be passed to the machine learning model for inference. Offline vs. Online Feature Store There is no database fullfilling both requirements of very low latency and and high throughput. Therefore, the Hopsworks Feature Store builds on Apache Hive with Apache Hudi as offline storage layer and RonDB as online storage.","title":"Offline vs. Online Feature Store"},{"location":"quickstart/","text":"Quickstart Guide # The Hopsworks feature store is a centralized repository, within an organization, to manage machine learning features. A feature is a measurable property of a phenomenon. It could be a simple value such as the age of a customer, or it could be an aggregated value, such as the number of transactions made by a customer in the last 30 days. A feature is not restricted to an numeric value, it could be a string representing an address, or an image. The Hopsworks Feature Store A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models. In this Quickstart Guide we are going to focus on the left side of the picture above. In particular how data engeneers can create features and push them to the Hopsworks feature store so that they are available to the data scientists HSFS library # The Hopsworks feature feature store library is called hsfs ( H opswork s F eature S tore). The library is Apache V2 licensed and available here . The library is currently available for Python and JVM languages such as Scala and Java. If you want to connect to the Feature Store from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Feature Store. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Feature Store. In fact, the Feature Store itself is also represented by an object. Furthermore, these objects have methods to save data along with the entities in the feature store. This data can be materialized from Spark or Pandas DataFrames, or the HSFS - Query abstraction . Guide Notebooks # This guide is based on a series of notebooks , which is available in the Feature Store Demo Tour Project on Hopsworks. Connection, Project and Feature Store # The first step is to establish a connection with your Hopsworks Feature Store instance and retrieve the object that represents the Feature Store you'll be working with. By default connection.get_feature_store() returns the feature store of the project you are working with. However, it accepts also a project name as parameter to select a different feature store. Python Scala import hsfs # Create a connection connection = hsfs . connection () # Get the feature store handle for the project's feature store fs = connection . get_feature_store () import com . logicalclocks . hsfs . _ import scala . collection . JavaConverters . _ // Create a connection val connection = HopsworksConnection . builder (). build (); // Get the feature store handle for the project's feature store val fs = connection . getFeatureStore (); You can inspect the Feature Store's meta data by accessing its attributes: Python Scala print ( fs . name ) print ( fs . description ) println ( fs . getName ) println ( fs . getDescription ) Example Data # In order to use the example data, you need to unzip the archive.zip file which is located in /Jupyter/hsfs/ when you are running the Quickstart from the Feature Store Demo Tour project. To do so, head to the Data Sets tab in Hopsworks, open the /Jupyter/hsfs directory, mark the archive.zip -file and click the extract button. The Data Sets browser Of course you can also use your own data if you read it into a Spark DataFrame. Feature Groups # Assuming you have done some feature engineering on the raw data, having produced a DataFrame with Features, these can now be saved to the Feature Store. For examples of feature engineering on the provided Sales data, see the example notebook . Creation # Create a feature group named store_fg . The store is the primary key uniquely identifying all the remaining features in this feature group. As you can see, you have the possibility to make settings on the Feature Group, such as the version number, or the statistics which should be computed. The Feature Group Guide guides through the full configuration of Feature Groups. Python Scala store_fg_meta = fs . create_feature_group ( name = \"store_fg\" , version = 1 , primary_key = [ \"store\" ], description = \"Store related features\" , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True }) val storeFgMeta = ( fs . createFeatureGroup () . name ( \"store_fg\" ) . description ( \"Store related features\" ) . version ( 1 ) . primaryKeys ( Seq ( \"store\" ). asJava ) . statisticsEnabled ( True ) . histograms ( True ) . correlations ( True ) . build ()) Up to this point we have just created the metadata object representing the feature group. However, we haven't saved the feature group in the feature store yet. To do so, we can call the method save on the metadata object created in the cell above. Python Scala store_fg_meta . save ( store_dataframe ) storeFgMeta . save ( store_dataframe ) Retrieval # If there were feature groups previously created in your Feature Store, or you want to pick up where you left off before, you can retrieve and read feature groups in a similar fashion as creating them: Using the Feature Store object, you can retrieve handles to the entities, such as feature groups, in the Feature Store. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. This is necessary, in order to make the code reproducible, as version changes indicate breaking schema changes. Python Scala exogenous_fg_meta = fs . get_feature_group ( 'exogenous_fg' , version = 1 ) # Read the data, by default selecting all features exogenous_df = exogenous_fg_meta . read () # Select a subset of features and read into dataframe exogenous_df_subset = exogenous_fg_meta . select ([ \"store\" , \"fuel_price\" , \"is_holiday\" ]) . read () val exogenousFgMeta = fs . getFeatureGroup ( \"exogenous_fg\" , 1 ) // Read the data, by default selecting all features val exogenousDf = exogenousFgMeta . read () // Select a subset of features and read into dataframe val exogenousDfSubset = exogenousFgMeta . select ( Seq ( \"store\" , \"fuel_price\" , \"is_holiday\" ). asJava ). read () Joining # HSFS provides an API similar to Pandas to join feature groups together and to select features from different feature groups. The easies query you can write is by selecting all the features from a feature group and join them with all the features of another feature group. You can use the select_all() method of a feature group to select all its features. HSFS relies on the Hopsworks feature store to identify which features of the two feature groups to use as joining condition. If you don't specify anything, Hopsworks will use the largest matching subset of primary keys with the same name. In the example below, sales_fg has store , dept and date as composite primary key while exogenous_fg has only store and date . So Hopsworks will set as joining condition store and date . Python Scala sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () . join ( exogenous_fg . select_all ()) # print first 5 rows of the query query . show ( 5 ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) val query = salesFg . selectAll (). join ( exogenousFg . selectAll ()) // print first 5 rows of the query query . show ( 5 ) For a more complex joins, and details about overwriting the join keys and join type, the programming interface guide explains the Query interface as well as Training Datasets # Once a Data Scientist has found the features she needs for her model, she can create a training dataset to materialize the features in the desired file format. The Hopsworks Feature Store supports a variety of file formats, matching the Data Scientists' favourite Machine Learning Frameworks. Creation # You can either create a training dataset from a Query object or directly from a Spark or Pandas DataFrame. Spark and Pandas give you more flexibility, but it has drawbacks for reproducability at inference time, when the Feature Vector needs to be reconstructed. The idea of the Feature Store is to have ready-engineered features available for Data Scientists to be selected for training datasets. With this assumption, it should not be necessary to perform additional engineering, but instead joining, filtering and point in time querying should be enough to generate training datasets. Python Scala store_fg = fs . get_feature_group ( \"store_fg\" ) sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () \\ . join ( store_fg . select_all ()) \\ . join ( exogenous_fg . select ([ 'fuel_price' , 'unemployment' , 'cpi' ])) td = fs . create_training_dataset ( name = \"sales_model\" , description = \"Dataset to train the sales model\" , data_format = \"tfrecord\" , splits = { \"train\" : 0.7 , \"test\" : 0.2 , \"validate\" : 0.1 }, version = 1 ) td . save ( query ) val storeFg = fs . getFeatureGroup ( \"store_fg\" ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) query = ( salesFg . selectAll () . join ( storeFg . selectAll ()) . join ( exogenousFg . select ( Seq ( \"fuel_price\" , \"unemployment\" , \"cpi\" ). asJava ))) val td = ( fs . createTrainingDataset () . name ( \"sales_model\" ) . description ( \"Dataset to train the sales model\" ) . version ( 1 ) . dataFormat ( DataFormat . TFRECORD ) . splits ( Map ( \"train\" -> Double . box ( 0.7 ), \"test\" -> Double . box ( 0.2 ), \"validate\" -> Double . box ( 0.1 )) . build ()) td . save ( query ) Retrieval # If you want to use a previously created training dataset to train a machine learning model, you can get the training dataset similarly to how you get a feature group. Python Scala td = fs . get_training_dataset ( \"sales_model\" ) df = td . read ( split = \"train\" ) val td = fs . getTrainingDataset ( \"sales_model\" ) val df = td . read ( \"train\" ) Either you read the data into a DataFrame again, or you use the provided utility methods, to instantiate for example a tf.data.Dataset , which can directly be passed to a TensorFlow model. Python Scala train_input_feeder = training_dataset . feed ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = train_input_feeder . tf_record_dataset () This functionality is only available in the Python API.","title":"Quickstart"},{"location":"quickstart/#quickstart-guide","text":"The Hopsworks feature store is a centralized repository, within an organization, to manage machine learning features. A feature is a measurable property of a phenomenon. It could be a simple value such as the age of a customer, or it could be an aggregated value, such as the number of transactions made by a customer in the last 30 days. A feature is not restricted to an numeric value, it could be a string representing an address, or an image. The Hopsworks Feature Store A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models. In this Quickstart Guide we are going to focus on the left side of the picture above. In particular how data engeneers can create features and push them to the Hopsworks feature store so that they are available to the data scientists","title":"Quickstart Guide"},{"location":"quickstart/#hsfs-library","text":"The Hopsworks feature feature store library is called hsfs ( H opswork s F eature S tore). The library is Apache V2 licensed and available here . The library is currently available for Python and JVM languages such as Scala and Java. If you want to connect to the Feature Store from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Feature Store. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Feature Store. In fact, the Feature Store itself is also represented by an object. Furthermore, these objects have methods to save data along with the entities in the feature store. This data can be materialized from Spark or Pandas DataFrames, or the HSFS - Query abstraction .","title":"HSFS library"},{"location":"quickstart/#guide-notebooks","text":"This guide is based on a series of notebooks , which is available in the Feature Store Demo Tour Project on Hopsworks.","title":"Guide Notebooks"},{"location":"quickstart/#connection-project-and-feature-store","text":"The first step is to establish a connection with your Hopsworks Feature Store instance and retrieve the object that represents the Feature Store you'll be working with. By default connection.get_feature_store() returns the feature store of the project you are working with. However, it accepts also a project name as parameter to select a different feature store. Python Scala import hsfs # Create a connection connection = hsfs . connection () # Get the feature store handle for the project's feature store fs = connection . get_feature_store () import com . logicalclocks . hsfs . _ import scala . collection . JavaConverters . _ // Create a connection val connection = HopsworksConnection . builder (). build (); // Get the feature store handle for the project's feature store val fs = connection . getFeatureStore (); You can inspect the Feature Store's meta data by accessing its attributes: Python Scala print ( fs . name ) print ( fs . description ) println ( fs . getName ) println ( fs . getDescription )","title":"Connection, Project and Feature Store"},{"location":"quickstart/#example-data","text":"In order to use the example data, you need to unzip the archive.zip file which is located in /Jupyter/hsfs/ when you are running the Quickstart from the Feature Store Demo Tour project. To do so, head to the Data Sets tab in Hopsworks, open the /Jupyter/hsfs directory, mark the archive.zip -file and click the extract button. The Data Sets browser Of course you can also use your own data if you read it into a Spark DataFrame.","title":"Example Data"},{"location":"quickstart/#feature-groups","text":"Assuming you have done some feature engineering on the raw data, having produced a DataFrame with Features, these can now be saved to the Feature Store. For examples of feature engineering on the provided Sales data, see the example notebook .","title":"Feature Groups"},{"location":"quickstart/#creation","text":"Create a feature group named store_fg . The store is the primary key uniquely identifying all the remaining features in this feature group. As you can see, you have the possibility to make settings on the Feature Group, such as the version number, or the statistics which should be computed. The Feature Group Guide guides through the full configuration of Feature Groups. Python Scala store_fg_meta = fs . create_feature_group ( name = \"store_fg\" , version = 1 , primary_key = [ \"store\" ], description = \"Store related features\" , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True }) val storeFgMeta = ( fs . createFeatureGroup () . name ( \"store_fg\" ) . description ( \"Store related features\" ) . version ( 1 ) . primaryKeys ( Seq ( \"store\" ). asJava ) . statisticsEnabled ( True ) . histograms ( True ) . correlations ( True ) . build ()) Up to this point we have just created the metadata object representing the feature group. However, we haven't saved the feature group in the feature store yet. To do so, we can call the method save on the metadata object created in the cell above. Python Scala store_fg_meta . save ( store_dataframe ) storeFgMeta . save ( store_dataframe )","title":"Creation"},{"location":"quickstart/#retrieval","text":"If there were feature groups previously created in your Feature Store, or you want to pick up where you left off before, you can retrieve and read feature groups in a similar fashion as creating them: Using the Feature Store object, you can retrieve handles to the entities, such as feature groups, in the Feature Store. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. This is necessary, in order to make the code reproducible, as version changes indicate breaking schema changes. Python Scala exogenous_fg_meta = fs . get_feature_group ( 'exogenous_fg' , version = 1 ) # Read the data, by default selecting all features exogenous_df = exogenous_fg_meta . read () # Select a subset of features and read into dataframe exogenous_df_subset = exogenous_fg_meta . select ([ \"store\" , \"fuel_price\" , \"is_holiday\" ]) . read () val exogenousFgMeta = fs . getFeatureGroup ( \"exogenous_fg\" , 1 ) // Read the data, by default selecting all features val exogenousDf = exogenousFgMeta . read () // Select a subset of features and read into dataframe val exogenousDfSubset = exogenousFgMeta . select ( Seq ( \"store\" , \"fuel_price\" , \"is_holiday\" ). asJava ). read ()","title":"Retrieval"},{"location":"quickstart/#joining","text":"HSFS provides an API similar to Pandas to join feature groups together and to select features from different feature groups. The easies query you can write is by selecting all the features from a feature group and join them with all the features of another feature group. You can use the select_all() method of a feature group to select all its features. HSFS relies on the Hopsworks feature store to identify which features of the two feature groups to use as joining condition. If you don't specify anything, Hopsworks will use the largest matching subset of primary keys with the same name. In the example below, sales_fg has store , dept and date as composite primary key while exogenous_fg has only store and date . So Hopsworks will set as joining condition store and date . Python Scala sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () . join ( exogenous_fg . select_all ()) # print first 5 rows of the query query . show ( 5 ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) val query = salesFg . selectAll (). join ( exogenousFg . selectAll ()) // print first 5 rows of the query query . show ( 5 ) For a more complex joins, and details about overwriting the join keys and join type, the programming interface guide explains the Query interface as well as","title":"Joining"},{"location":"quickstart/#training-datasets","text":"Once a Data Scientist has found the features she needs for her model, she can create a training dataset to materialize the features in the desired file format. The Hopsworks Feature Store supports a variety of file formats, matching the Data Scientists' favourite Machine Learning Frameworks.","title":"Training Datasets"},{"location":"quickstart/#creation_1","text":"You can either create a training dataset from a Query object or directly from a Spark or Pandas DataFrame. Spark and Pandas give you more flexibility, but it has drawbacks for reproducability at inference time, when the Feature Vector needs to be reconstructed. The idea of the Feature Store is to have ready-engineered features available for Data Scientists to be selected for training datasets. With this assumption, it should not be necessary to perform additional engineering, but instead joining, filtering and point in time querying should be enough to generate training datasets. Python Scala store_fg = fs . get_feature_group ( \"store_fg\" ) sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () \\ . join ( store_fg . select_all ()) \\ . join ( exogenous_fg . select ([ 'fuel_price' , 'unemployment' , 'cpi' ])) td = fs . create_training_dataset ( name = \"sales_model\" , description = \"Dataset to train the sales model\" , data_format = \"tfrecord\" , splits = { \"train\" : 0.7 , \"test\" : 0.2 , \"validate\" : 0.1 }, version = 1 ) td . save ( query ) val storeFg = fs . getFeatureGroup ( \"store_fg\" ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) query = ( salesFg . selectAll () . join ( storeFg . selectAll ()) . join ( exogenousFg . select ( Seq ( \"fuel_price\" , \"unemployment\" , \"cpi\" ). asJava ))) val td = ( fs . createTrainingDataset () . name ( \"sales_model\" ) . description ( \"Dataset to train the sales model\" ) . version ( 1 ) . dataFormat ( DataFormat . TFRECORD ) . splits ( Map ( \"train\" -> Double . box ( 0.7 ), \"test\" -> Double . box ( 0.2 ), \"validate\" -> Double . box ( 0.1 )) . build ()) td . save ( query )","title":"Creation"},{"location":"quickstart/#retrieval_1","text":"If you want to use a previously created training dataset to train a machine learning model, you can get the training dataset similarly to how you get a feature group. Python Scala td = fs . get_training_dataset ( \"sales_model\" ) df = td . read ( split = \"train\" ) val td = fs . getTrainingDataset ( \"sales_model\" ) val df = td . read ( \"train\" ) Either you read the data into a DataFrame again, or you use the provided utility methods, to instantiate for example a tf.data.Dataset , which can directly be passed to a TensorFlow model. Python Scala train_input_feeder = training_dataset . feed ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = train_input_feeder . tf_record_dataset () This functionality is only available in the Python API.","title":"Retrieval"},{"location":"setup/","text":"Integrations # Hopsworks # If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide . Storage Connectors # Storage connectors encapsulate the configuration information needed for a Spark or Python execution engine to securely read and write to a specific storage. The storage connector guide explains step by step how to configure different data sources (such as S3, Azure Data Lake, Redshift, Snowflake, any JDBC data source) and how they can be used to ingest data and define external (on-demand) Feature Groups. Databricks # Connecting to the Feature Store from Databricks requires setting up a Feature Store API Key for Databricks and installing one of the HSFS client libraries on your Databricks cluster. The Databricks integration guide explains step by step how to connect to the Feature Store from Databricks. AWS Sagemaker # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API Key for SageMaker and installing the HSFS Python client library on SageMaker. The AWS SageMaker integration guide explains step by step how to connect to the Feature Store from SageMaker. Python (Local or KubeFlow) # Connecting to the Feature Store from any Python environment, such as your local environment or KubeFlow, requires setting up a Feature Store API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment. Spark Cluster # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Overview"},{"location":"setup/#integrations","text":"","title":"Integrations"},{"location":"setup/#hopsworks","text":"If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide .","title":"Hopsworks"},{"location":"setup/#storage-connectors","text":"Storage connectors encapsulate the configuration information needed for a Spark or Python execution engine to securely read and write to a specific storage. The storage connector guide explains step by step how to configure different data sources (such as S3, Azure Data Lake, Redshift, Snowflake, any JDBC data source) and how they can be used to ingest data and define external (on-demand) Feature Groups.","title":"Storage Connectors"},{"location":"setup/#databricks","text":"Connecting to the Feature Store from Databricks requires setting up a Feature Store API Key for Databricks and installing one of the HSFS client libraries on your Databricks cluster. The Databricks integration guide explains step by step how to connect to the Feature Store from Databricks.","title":"Databricks"},{"location":"setup/#aws-sagemaker","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API Key for SageMaker and installing the HSFS Python client library on SageMaker. The AWS SageMaker integration guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS Sagemaker"},{"location":"setup/#python-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment, such as your local environment or KubeFlow, requires setting up a Feature Store API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment.","title":"Python (Local or KubeFlow)"},{"location":"setup/#spark-cluster","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Cluster"},{"location":"generated/feature/","text":"Feature # Features are the most granular entity in the feature store and are logically grouped by feature groups . Features in the same feature groups are semantically related to the feature groups primary keys. The storage location of a single feature is determined by the feature group . Hence, enabling a feature group for online storage will make a feature available as an online feature. Features Taxonomy # Within a feature group there are different categories of features: Primary Keys : The columns that uniquely identify an entity in a feature group (e.g., the customer, the transaction, ...). A primary key may consist of multiple columns, that is, a composite primary key. Event Time : The column in a feature group that stores the time at which the event for the given record has happened (the observation time for the row). You need to provide the event time, if you want the feature group to be able to support point-in-time correct joins when used to creating training data. Partition Keys : The columns that define the storage layout of the feature group data in the offline feature store. The partition keys may be defined over one or more columns, the data is organized in partitions which are stored in separate subdirectories. Partitioning can help improve query efficiency for the offline feature store, by reducing the amount of data that needs to be read from disk for a given query. Features : These columns are the features that can be used to train models and perform inference on models. Feature Data Types # When a feature is stored in the both the online and offline feature stores, it will be stored in a data storage type native to each store. Offline data type : The data type of the feature when stored on the offline feature store Online data type : The data type of the feature when stored on the online feature store. The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled , its features will not have an online data type. Offline Data Types # The offline feature store is based on Apache Hudi and Hive Metastore, as such, any Hive Data Type can be leveraged. Potential offline types are: \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\" Online Data Types # The online storage is based on RonDB and hence, any MySQL Data Type can be leveraged. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(n)\" , \"BINARY\" , \"VARBINARY(n)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\" Complex online data types # Additionally to the online types above, Hopsworks allows users to store complex types (e.g. ARRAY ) on the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save() , insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the fs.sql(\"SELECT ...\", online=True) statement, it will return a binary blob. On the feature store UI, the online feature type for complex features will be reported as VARBINARY . Online restrictions for primary key data types: # When a feature is being used as a primary key, certain types are not allowed. Examples of such types are Float , Double , Date , Text , Blob and Complex Types (e.g. Array<>). Additionally the size of the sum of the primary key online data types storage requirements should not exceed 3KB. Type Mapping # The offline and online types for each feature are identified based on the types of the columns in the Spark or Pandas DataFrame, and those types are then mapped to the online and offline data types. In the case of a Spark DataFrame, the Spark types will be mapped to the corresponding Hive Metastore type and used as offline data type. If the feature group is online enabled, Hopsworks will then map the offline data type to the corresponding online data type. The mapping is based on the following rules: If the offline data type is also supported on the online feature store (e.g. INT, FLOAT, DATE, TIMESTAMP), the online data type will be the same as the offline data type If the offline data type is boolean , the online data type is going to be set as tinyint If the offline data type is string , the online data type is going to be set as varchar(100) If the offline data type is not supported by the online feature store and it is not one of the above exception, the online data type will be set as varbinary(100) to handle complex types. Pandas Conversion # When registering a Pandas DataFrame as a feature group, the following mapping rules are applied: Pandas Type Offline Feature Type int32 INT int64 BIGINT float32 FLOAT float64 DOUBLE datetime64[ns] TIMESTAMP object STRING Explicit Schema Definition # When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type inference. Users can explicitly define the feature group schema as follows: from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . create_feature_group ( name = \"fg_manual_schema\" , features = features , online_enabled = True ) fg . save ( df ) [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] description # Description of the feature. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] contains # Feature . contains ( other ) [source] is_complex # Feature . is_complex () Returns true if the feature has a complex type. [source] json # Feature . json ()","title":"Feature"},{"location":"generated/feature/#feature","text":"Features are the most granular entity in the feature store and are logically grouped by feature groups . Features in the same feature groups are semantically related to the feature groups primary keys. The storage location of a single feature is determined by the feature group . Hence, enabling a feature group for online storage will make a feature available as an online feature.","title":"Feature"},{"location":"generated/feature/#features-taxonomy","text":"Within a feature group there are different categories of features: Primary Keys : The columns that uniquely identify an entity in a feature group (e.g., the customer, the transaction, ...). A primary key may consist of multiple columns, that is, a composite primary key. Event Time : The column in a feature group that stores the time at which the event for the given record has happened (the observation time for the row). You need to provide the event time, if you want the feature group to be able to support point-in-time correct joins when used to creating training data. Partition Keys : The columns that define the storage layout of the feature group data in the offline feature store. The partition keys may be defined over one or more columns, the data is organized in partitions which are stored in separate subdirectories. Partitioning can help improve query efficiency for the offline feature store, by reducing the amount of data that needs to be read from disk for a given query. Features : These columns are the features that can be used to train models and perform inference on models.","title":"Features Taxonomy"},{"location":"generated/feature/#feature-data-types","text":"When a feature is stored in the both the online and offline feature stores, it will be stored in a data storage type native to each store. Offline data type : The data type of the feature when stored on the offline feature store Online data type : The data type of the feature when stored on the online feature store. The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled , its features will not have an online data type.","title":"Feature Data Types"},{"location":"generated/feature/#offline-data-types","text":"The offline feature store is based on Apache Hudi and Hive Metastore, as such, any Hive Data Type can be leveraged. Potential offline types are: \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\"","title":"Offline Data Types"},{"location":"generated/feature/#online-data-types","text":"The online storage is based on RonDB and hence, any MySQL Data Type can be leveraged. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(n)\" , \"BINARY\" , \"VARBINARY(n)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\"","title":"Online Data Types"},{"location":"generated/feature/#complex-online-data-types","text":"Additionally to the online types above, Hopsworks allows users to store complex types (e.g. ARRAY ) on the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save() , insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the fs.sql(\"SELECT ...\", online=True) statement, it will return a binary blob. On the feature store UI, the online feature type for complex features will be reported as VARBINARY .","title":"Complex online data types"},{"location":"generated/feature/#online-restrictions-for-primary-key-data-types","text":"When a feature is being used as a primary key, certain types are not allowed. Examples of such types are Float , Double , Date , Text , Blob and Complex Types (e.g. Array<>). Additionally the size of the sum of the primary key online data types storage requirements should not exceed 3KB.","title":"Online restrictions for primary key data types:"},{"location":"generated/feature/#type-mapping","text":"The offline and online types for each feature are identified based on the types of the columns in the Spark or Pandas DataFrame, and those types are then mapped to the online and offline data types. In the case of a Spark DataFrame, the Spark types will be mapped to the corresponding Hive Metastore type and used as offline data type. If the feature group is online enabled, Hopsworks will then map the offline data type to the corresponding online data type. The mapping is based on the following rules: If the offline data type is also supported on the online feature store (e.g. INT, FLOAT, DATE, TIMESTAMP), the online data type will be the same as the offline data type If the offline data type is boolean , the online data type is going to be set as tinyint If the offline data type is string , the online data type is going to be set as varchar(100) If the offline data type is not supported by the online feature store and it is not one of the above exception, the online data type will be set as varbinary(100) to handle complex types.","title":"Type Mapping"},{"location":"generated/feature/#pandas-conversion","text":"When registering a Pandas DataFrame as a feature group, the following mapping rules are applied: Pandas Type Offline Feature Type int32 INT int64 BIGINT float32 FLOAT float64 DOUBLE datetime64[ns] TIMESTAMP object STRING","title":"Pandas Conversion"},{"location":"generated/feature/#explicit-schema-definition","text":"When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type inference. Users can explicitly define the feature group schema as follows: from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . create_feature_group ( name = \"fg_manual_schema\" , features = features , online_enabled = True ) fg . save ( df ) [source]","title":"Explicit Schema Definition"},{"location":"generated/feature/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/feature/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/feature/#description","text":"Description of the feature. [source]","title":"description"},{"location":"generated/feature/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/feature/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/feature/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/feature/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/feature/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/feature/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/feature/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature/#contains","text":"Feature . contains ( other ) [source]","title":"contains"},{"location":"generated/feature/#is_complex","text":"Feature . is_complex () Returns true if the feature has a complex type. [source]","title":"is_complex"},{"location":"generated/feature/#json","text":"Feature . json ()","title":"json"},{"location":"generated/feature_group/","text":"Feature Group # A Feature Groups is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The Feature Group lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them. Generally, the features in a feature froup are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, feature groups provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in. Combine features from any number of feature groups Feature groups are logical groupings of features, usually based on the data source and ingestion job from which they originate. It is important to note that feature groups are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets. Versioning # Feature groups can be versioned. Data Engineers should use the version to indicate to a Data Scientist that the schema or the feature engineering logic of the features in this group has changed. Breaking feature group schema changes In order to guarantee reproducability, the schema of a feature group should be immutable, because deleting features could lead to failing model pipelines downstream. Hence, in order to modify a schema, a new version of a feature group has to be created. In contrary, appending features to feature groups is considered a non-breaking change, since the feature store makes all selections explicit and because the namespace within a feature group is flat, it is not possible to append a new feature with an already existing name to a feature group. Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] event_time # Event time feature in the feature group. [source] expectations_names # The names of expectations attached to this feature group. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] stream # whether real time stream writing capabilities are supported or not [source] time_travel_format # Setting of the feature group time travel format. [source] validation_type # Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] as_of # FeatureGroup . as_of ( wallclock_time ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source] attach_expectation # FeatureGroup . attach_expectation ( expectation ) Attach a feature group expectation. If feature group validation is not already enabled, it will be enabled and set to the stricter setting. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source] compute_statistics # FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] detach_expectation # FeatureGroup . detach_expectation ( expectation ) Remove an expectation from a feature group. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] get_complex_features # FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source] get_expectation # FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source] get_expectations # FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns FeatureGroup . Updated feature group metadata object. [source] insert_stream # FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source] save # FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # FeatureGroup . update_features ( features ) Update a single feature in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"Feature Group"},{"location":"generated/feature_group/#feature-group","text":"A Feature Groups is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The Feature Group lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them. Generally, the features in a feature froup are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, feature groups provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in. Combine features from any number of feature groups Feature groups are logical groupings of features, usually based on the data source and ingestion job from which they originate. It is important to note that feature groups are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets.","title":"Feature Group"},{"location":"generated/feature_group/#versioning","text":"Feature groups can be versioned. Data Engineers should use the version to indicate to a Data Scientist that the schema or the feature engineering logic of the features in this group has changed. Breaking feature group schema changes In order to guarantee reproducability, the schema of a feature group should be immutable, because deleting features could lead to failing model pipelines downstream. Hence, in order to modify a schema, a new version of a feature group has to be created. In contrary, appending features to feature groups is considered a non-breaking change, since the feature store makes all selections explicit and because the namespace within a feature group is flat, it is not possible to append a new feature with an already existing name to a feature group.","title":"Versioning"},{"location":"generated/feature_group/#creation","text":"[source]","title":"Creation"},{"location":"generated/feature_group/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object.","title":"create_feature_group"},{"location":"generated/feature_group/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_group/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/feature_group/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_group/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/feature_group/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/feature_group/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/feature_group/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/feature_group/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/feature_group/#expectations_names","text":"The names of expectations attached to this feature group. [source]","title":"expectations_names"},{"location":"generated/feature_group/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/feature_group/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/feature_group/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/feature_group/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/feature_group/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/feature_group/#location","text":"[source]","title":"location"},{"location":"generated/feature_group/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/feature_group/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/feature_group/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/feature_group/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/feature_group/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/feature_group/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/feature_group/#stream","text":"whether real time stream writing capabilities are supported or not [source]","title":"stream"},{"location":"generated/feature_group/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/feature_group/#validation_type","text":"Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source]","title":"validation_type"},{"location":"generated/feature_group/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/feature_group/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_group/#add_tag","text":"FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/feature_group/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/feature_group/#as_of","text":"FeatureGroup . as_of ( wallclock_time ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/feature_group/#attach_expectation","text":"FeatureGroup . attach_expectation ( expectation ) Attach a feature group expectation. If feature group validation is not already enabled, it will be enabled and set to the stricter setting. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"attach_expectation"},{"location":"generated/feature_group/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/feature_group/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"commit_details"},{"location":"generated/feature_group/#compute_statistics","text":"FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/feature_group/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/feature_group/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/feature_group/#detach_expectation","text":"FeatureGroup . detach_expectation ( expectation ) Remove an expectation from a feature group. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"detach_expectation"},{"location":"generated/feature_group/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/feature_group/#get_complex_features","text":"FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source]","title":"get_complex_features"},{"location":"generated/feature_group/#get_expectation","text":"FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"get_expectation"},{"location":"generated/feature_group/#get_expectations","text":"FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source]","title":"get_expectations"},{"location":"generated/feature_group/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/feature_group/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/feature_group/#get_tag","text":"FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/feature_group/#get_tags","text":"FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/feature_group/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source]","title":"get_validations"},{"location":"generated/feature_group/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/feature_group/#insert_stream","text":"FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source]","title":"insert_stream"},{"location":"generated/feature_group/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/feature_group/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"read_changes"},{"location":"generated/feature_group/#save","text":"FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/feature_group/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/feature_group/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/feature_group/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/feature_group/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/feature_group/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/feature_group/#update_feature_description","text":"FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/feature_group/#update_features","text":"FeatureGroup . update_features ( features ) Update a single feature in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/feature_group/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/feature_group/#validate","text":"FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/feature_store/","text":"Feature Store # Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object. [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. [source] create_feature_view # FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , label = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source] delete_expectation # FeatureStore . delete_expectation ( name ) Delete an expectation from the feature store. Arguments name str : Name of the training dataset to create. [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_view # FeatureStore . get_feature_view ( name , version ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. version : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_feature_views # FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_groups # FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns DataFrame : DataFrame depending on the chosen type.","title":"Feature Store"},{"location":"generated/feature_store/#feature-store","text":"","title":"Feature Store"},{"location":"generated/feature_store/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_store/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/feature_store/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_store/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/feature_store/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/feature_store/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/feature_store/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/feature_store/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/feature_store/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/feature_store/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/feature_store/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/feature_store/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/feature_store/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/feature_store/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_store/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object. [source]","title":"create_expectation"},{"location":"generated/feature_store/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/feature_store/#create_feature_view","text":"FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , label = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source]","title":"create_feature_view"},{"location":"generated/feature_store/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/feature_store/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/feature_store/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"create_transformation_function"},{"location":"generated/feature_store/#delete_expectation","text":"FeatureStore . delete_expectation ( name ) Delete an expectation from the feature store. Arguments name str : Name of the training dataset to create. [source]","title":"delete_expectation"},{"location":"generated/feature_store/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source]","title":"get_expectation"},{"location":"generated/feature_store/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source]","title":"get_expectations"},{"location":"generated/feature_store/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/feature_store/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/feature_store/#get_feature_view","text":"FeatureStore . get_feature_view ( name , version ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. version : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_view"},{"location":"generated/feature_store/#get_feature_views","text":"FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_views"},{"location":"generated/feature_store/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/feature_store/#get_on_demand_feature_groups","text":"FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_groups"},{"location":"generated/feature_store/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/feature_store/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/feature_store/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/feature_store/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_datasets"},{"location":"generated/feature_store/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/feature_store/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source]","title":"get_transformation_functions"},{"location":"generated/feature_store/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns DataFrame : DataFrame depending on the chosen type.","title":"sql"},{"location":"generated/feature_validation/","text":"Feature Validation with the Hopsworks Feature Store # Correct feature data is essential for developing accurate machine learning models. Raw data being ingested into the feature store maybe suffer from incorrect or corrupt values, may need to be validated against certain features depending on the domain. For example, a feature representing the customer's age should not be a negative number and should always have a value set. HSFS provides an API to define expectations on data being inserted into feature groups and also view results over time of these expectations in the form of feature validations. Feature validations can therefore be easily integrated with existing feature ingestion pipelines. HSFS utilizes the Deequ open source library and support is currently being added for working with the Great Expectations . Below we describe the different API components of the hsfs feature validation API and we walk you through Feature validation is part of the HSFS Java/Scala and Python API for working with Feature Groups. Users work with the abstractions: Rule definitions # Rule definitions is a set of pre-defined and immutable rules ( RuleDefiniton ) that are unique by name and are used for creating validation rules ( Rule ) and expectations ( Expectation ) applied on a dataframe that is ingested into a Feature Group. All rules are asserted based on a value provided by the users which can be set using the min and max rule properties. That means users must provide an exact value, or a range of values, that the rule's computed value needs to match in order for the rule to be asserted as successful. For example: - min(10) and max(10) for the HAS_MIN rule means the rule will be successful only if the minimum value of a feature is exactly 10 . - min(10) for the HAS_MIN rule means the rule will be successful if the minimum value of a feature is at least 10 . - max(10) for the HAS_MIN rule means the rule will be successful if the maximum value of a feature is at most 10 . Default value for min/max is 0.0 , except for compliance rules which is 1.0 . The following table describes all the supported rule definitions along with their properties (code examples are shown in the section below). - Name: The name of the rule. - Predicate: The type of the predicate type of value this rule expects. - Example: - IS_CONTAINED_IN expects the LEGAL_VALUES property to be set. - Compliance rules such as IS_LESS_THAN expect the feature property to be set. That means all the expectation's features will be evaluated against the provided feature. - Accepted type: The data type of value set for the rules predicate or min/max properties. - Examples: - HAS_MIN uses min() and max() with fractional values like min(2.5) max (5.0). - HAS_SIZE uses min() and max() with integral values like min(10) max(10) - HAS_DATATYPE expects String values to be provided as input to the ACCEPTED_TYPE predicate, ACCEPTED_TYPE=\"Boolean\" . - Feature type: The data type of the feature this rule is evaluated against. If a rule is to be applied to an incompatible feature type, an error will be thrown when the expectation is attached on the feature group. - Example, HAS_MIN can only be applied on numerical features but HAS_DISTINCTNESS can be applied on both numerical and categorical features. - Description: A short description of what the rule validates. Name Predicate Accepted type Feature type Description HAS_APPROX_COUNT_DISTINCT Fractional Assert on the approximate count distinct of a feature. HAS_APPROX_QUANTILE Fractional Numerical Assert on the approximate quantile of a feature. HAS_COMPLETENESS Fractional Assert on the uniqueness of a single or combined set of features. HAS_CORRELATION Fractional Numerical Assert on the pearson correlation between two features. HAS_DATATYPE ACCEPTED_TYPE String Assert on the fraction of rows that conform to the given data type. HAS_DISTINCTNESS Fractional Assert on the distinctness of a single or combined set of features. HAS_ENTROPY Fractional Assert on the entropy of a feature. HAS_MAX Fractional Numerical Assert on the max of a feature. HAS_MAX_LENGTH String Categorical Assert on the maximum length of the feature value. HAS_MEAN Fractional Numerical Assert on the mean of a feature. HAS_MIN Fractional Numerical Assert on the min of a feature. HAS_MIN_LENGTH String Categorical Assert on the minimum length of the feature value. HAS_MUTUAL_INFORMATION Fractional Assert on the mutual information between two features. HAS_NUMBER_OF_DISTINCT_VALUES Integral Assert on the number of distinct values of a feature. HAS_PATTERN String Categorical Assert on the average compliance of the feature to the regular expression. HAS_SIZE Integral Assert on the number of rows of the dataframe. HAS_STANDARD_DEVIATION Fractional Numerical Assert on the standard deviation of a feature. HAS_SUM Fractional Numerical Assert on the sum of a feature. HAS_UNIQUENESS Fractional Assert on the uniqueness of a feature, that is the fraction of unique values over the number of all its values. HAS_UNIQUE_VALUE_RATIO Fractional Assert on the unique value ratio of a feature, that is the fraction of unique values over the number of all distinct values. IS_CONTAINED_IN LEGAL_VALUES String Assert that every non-null value of feature is contained in a set of predefined values. IS_GREATER_THAN feature Fractional Assert on feature A values being greater than feature B. IS_GREATER_THAN_OR_EQUAL_TO feature Fractional Assert on feature A values being greater than or equal to those of feature B. IS_LESS_THAN feature Fractional Assert on feature A values being less that feature B. IS_LESS_THAN_OR_EQUAL_TO feature Fractional Assert on feature A values being less or equal to those of feature B. IS_NON_NEGATIVE Fractional Numerical Assert on feature containing non negative values. IS_POSITIVE Fractional Numerical Assert on a feature containing non negative values. Retrieval # [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. For example, to get all available rule definitions in hsfs: Python Scala import hsfs connection = hsfs . connection () rules = connection . get_rules () import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules () and to get a rule definition by name: Python Scala import hsfs connection = hsfs . connection () rules = connection . get_rules () import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules () Properties # [source] RuleDefinition # hsfs . ruledefinition . RuleDefinition ( name , accepted_type , predicate = None , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified. [source] accepted_type # The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source] description # [source] feature_type # The type of the feature, one of \"Numerical\", \"Categorical\". [source] name # Name of the rule definition. Unique across all features stores. [source] predicate # Predicate of the rule definition, one of \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\". Rules # Used as part of expectations that are applied on ingested features. Rule names correspond to the names of the rule definitions (see section above) and you can set the severity level and the actual values that the feature should respect. Defining expectation rules # In general, rule values can be an exact value or a range of values. For example, if you need a feature to be ingested if its minimum value is below zero, then you can set min(0) and max(0) but if you want the minimum to fall within a range of 0 and 1000 then you need to set min(0) and max(1000) . See section Expectations below for a detailed example. Rules that operate on tuples of features, for example HAS_CORRELEATION , are applied on the first two features as defined in the expectation (as ordered within the expectation). Examples # Python rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 10 )] # the minimum value of the feature needs to be at least 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , max = 10 )] # the minimum value of the feature needs to be at most 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 , max = 10 )] # the minimum value of the feature needs to be between 0 and 10 rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 )] # At least 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.5 )] # 10-50% of all instances of the feature need to of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"IS_CONTAINED_IN\" , level = \"ERROR\" , legal_values = [ \"a\" , \"b\" ], min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be contained in the legal_values list rules = [ Rule ( name = \"HAS_PATTERN\" , level = \"ERROR\" , pattern = \"a+\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to match the given pattern rules = [ Rule ( name = \"IS_LESS_THAN\" , level = \"ERROR\" , feature = \"graduation_date\" )] # All values of the expectation's features must be less than the graduation_date feature values (comparison is done on a per-row basis) rules = [ Rule ( name = \"IS_LESS_THAN\" , level = \"ERROR\" , feature = \"graduation_date\" , min = 0.5 , max = 0.5 )] # Same as above but only 50% of the expectation's feature values need to be less that the graduation_date feature values Scala Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). level ( Level . WARNING ). build () // the mininum value of the feature needs to be at least 10 Rule . createRule ( RuleName . HAS_MIN ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be at most 10 Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be between 0 and 10 Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). level ( Level . ERROR ). build () // At least 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). max ( 10 ). level ( Level . ERROR ). build () // At most 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . IS_CONTAINED_IN ). legalValues ( Seq ( \"a\" , \"b\" )). min ( 0.1 ). max ( 0.1 ). level ( Level . ERROR ). build () // # Exactly 10% of all instances of the feature need to be contained in the legal_values list Rule . createRule ( RuleName . HAS_PATTERN ). pattern ( \"a+\" ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to match the given pattern Rule . createRule ( RuleName . IS_LESS_THAN ). feature ( \"graduation_date\" ). level ( Level . ERROR ). build () // All values of the expectation's features must be less than the graduation_date feature values (comparison is done on a per-row basis) Rule . createRule ( RuleName . IS_LESS_THAN ). feature ( \"graduation_date\" ). min ( 0.5 ). max ( 0.5 ). level ( Level . ERROR ). build () // Same as above but only 50% of the expectation's feature values need to be less that the graduation_date feature values Properties # [source] Rule # hsfs . rule . Rule ( name , level , min = None , max = None , pattern = None , accepted_type = None , feature = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only. [source] accepted_type # Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source] feature # Feature to compare the expectation's features to, applied only to Compliance rules. [source] legal_values # List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source] level # Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source] max # The upper bound of the value range this feature should fall into. [source] min # The lower bound of the value range this feature should fall into. [source] name # Name of the rule as found in rule definitions. [source] pattern # Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule. Expectations # A set of rule instances that are applied on a set of features. Expectations are created at the feature store level and can be attached to multiple feature groups. If an expectation contains no features, it will be applied to all the features of the feature group when the validation is done An expectation contains a list of features it is applied to. If the feature group the expectation is attached to, does not contain all the expectations features, the expectation will not be met. Creation # Create an expectation with two rules for ensuring the min and max of the features are valid: Python Scala expectation_sales = fs . create_expectation ( \"sales\" , description = \"min and max sales limits\" , features = [ \"salary\" , \"commission\" ], rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 ), Rule ( name = \"HAS_MAX\" , level = \"ERROR\" , max = 1000000 )]) expectation_sales . save () // Create an expectation for the \"salary\" and \"commissio\" features so that their min value is \"10\" and their max is \"100\" val expectationSales = ( fs . createExpectation () . rules ( Seq ( Rule . createRule ( RuleName . HAS_MIN ). min ( 0 ). level ( Level . WARNING ). build (), //Set rule by name Rule . createRule ( ruleMax ). max ( 1000000 ). level ( Level . ERROR ). build ())) //Set rule by passing the RuleDefinition metadata . name ( \"sales\" ) . description ( \"min and max sales limits\" ) . features ( Seq ( \"salary\" , \"commission\" )) . build ()) expectationSales . save () Create an expectation with a rule to assert that no feature has a null value: Python Scala expectation_notnull = fs . create_expectation ( \"not_null\" , description = \"Assert no feature is null\" , rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"Null\" , max = 0 )]) expectation_notnull . save () val expectationNotNull = ( fs . createExpectation () . rules ( Seq ( Rule . createRule ( RuleName . HAS_DATATYPE ). max ( 0 ). acceptedType ( AcceptedType . Null ). level ( Level . ERROR ). build ())) . name ( \"not_null\" ) . description ( \"Assert no feature is null\" ) . build ()) expectationNotNull . save () [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object. Retrieval # [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. Properties # [source] Expectation # hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store. [source] description # Description of the expectation. [source] features # Optional list of features this expectation is applied to. If no features are provided, the expectation will be applied to all the feature group features. [source] name # Name of the expectation, unique per feature store (project). [source] rules # List of rules applied to the features of the expectation. Methods # [source] save # Expectation . save () Persist the expectation metadata object to the feature store. Validations # The results of expectations against the ingested dataframe are assigned a validation time and are persisted within the Feature Store. Users can then retrieve validation results by validation time and by commit time for time-travel enabled feature groups. Validation Type # Feature Validation is disabled by default, by having the validation_type feature group attribute set to NONE. The list of allowed validation types are: STRICT: Data validation is performed and feature group is updated only if validation status is \"Success\" WARNING: Data validation is performed and feature group is updated only if validation status is \"Warning\" or lower ALL: Data validation is performed and feature group is updated only if validation status is \"Failure\" or lower NONE: Data validation not performed on feature group For example, to update the validation type to all: Python Scala fg . validation_type = \"ALL\" import com . logicalclocks . hsfs . metadata . validation . _ fg . updateValidationType ( ValidationType . ALL ) Validate # You can also apply the expectations on a dataframe without inserting data, that can be helpful for debugging. [source] validate # FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object. Retrieval # [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. Properties # [source] ValidationResult # hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group. [source] features # Feature of the validation result on which the rule was applied. [source] message # Message describing the outcome of applying the rule against the feature. [source] rule # Feature of the validation result on which the rule was applied. [source] status # [source] value # The computed value of the feature according to the rule.","title":"Feature Validation"},{"location":"generated/feature_validation/#feature-validation-with-the-hopsworks-feature-store","text":"Correct feature data is essential for developing accurate machine learning models. Raw data being ingested into the feature store maybe suffer from incorrect or corrupt values, may need to be validated against certain features depending on the domain. For example, a feature representing the customer's age should not be a negative number and should always have a value set. HSFS provides an API to define expectations on data being inserted into feature groups and also view results over time of these expectations in the form of feature validations. Feature validations can therefore be easily integrated with existing feature ingestion pipelines. HSFS utilizes the Deequ open source library and support is currently being added for working with the Great Expectations . Below we describe the different API components of the hsfs feature validation API and we walk you through Feature validation is part of the HSFS Java/Scala and Python API for working with Feature Groups. Users work with the abstractions:","title":"Feature Validation with the Hopsworks Feature Store"},{"location":"generated/feature_validation/#rule-definitions","text":"Rule definitions is a set of pre-defined and immutable rules ( RuleDefiniton ) that are unique by name and are used for creating validation rules ( Rule ) and expectations ( Expectation ) applied on a dataframe that is ingested into a Feature Group. All rules are asserted based on a value provided by the users which can be set using the min and max rule properties. That means users must provide an exact value, or a range of values, that the rule's computed value needs to match in order for the rule to be asserted as successful. For example: - min(10) and max(10) for the HAS_MIN rule means the rule will be successful only if the minimum value of a feature is exactly 10 . - min(10) for the HAS_MIN rule means the rule will be successful if the minimum value of a feature is at least 10 . - max(10) for the HAS_MIN rule means the rule will be successful if the maximum value of a feature is at most 10 . Default value for min/max is 0.0 , except for compliance rules which is 1.0 . The following table describes all the supported rule definitions along with their properties (code examples are shown in the section below). - Name: The name of the rule. - Predicate: The type of the predicate type of value this rule expects. - Example: - IS_CONTAINED_IN expects the LEGAL_VALUES property to be set. - Compliance rules such as IS_LESS_THAN expect the feature property to be set. That means all the expectation's features will be evaluated against the provided feature. - Accepted type: The data type of value set for the rules predicate or min/max properties. - Examples: - HAS_MIN uses min() and max() with fractional values like min(2.5) max (5.0). - HAS_SIZE uses min() and max() with integral values like min(10) max(10) - HAS_DATATYPE expects String values to be provided as input to the ACCEPTED_TYPE predicate, ACCEPTED_TYPE=\"Boolean\" . - Feature type: The data type of the feature this rule is evaluated against. If a rule is to be applied to an incompatible feature type, an error will be thrown when the expectation is attached on the feature group. - Example, HAS_MIN can only be applied on numerical features but HAS_DISTINCTNESS can be applied on both numerical and categorical features. - Description: A short description of what the rule validates. Name Predicate Accepted type Feature type Description HAS_APPROX_COUNT_DISTINCT Fractional Assert on the approximate count distinct of a feature. HAS_APPROX_QUANTILE Fractional Numerical Assert on the approximate quantile of a feature. HAS_COMPLETENESS Fractional Assert on the uniqueness of a single or combined set of features. HAS_CORRELATION Fractional Numerical Assert on the pearson correlation between two features. HAS_DATATYPE ACCEPTED_TYPE String Assert on the fraction of rows that conform to the given data type. HAS_DISTINCTNESS Fractional Assert on the distinctness of a single or combined set of features. HAS_ENTROPY Fractional Assert on the entropy of a feature. HAS_MAX Fractional Numerical Assert on the max of a feature. HAS_MAX_LENGTH String Categorical Assert on the maximum length of the feature value. HAS_MEAN Fractional Numerical Assert on the mean of a feature. HAS_MIN Fractional Numerical Assert on the min of a feature. HAS_MIN_LENGTH String Categorical Assert on the minimum length of the feature value. HAS_MUTUAL_INFORMATION Fractional Assert on the mutual information between two features. HAS_NUMBER_OF_DISTINCT_VALUES Integral Assert on the number of distinct values of a feature. HAS_PATTERN String Categorical Assert on the average compliance of the feature to the regular expression. HAS_SIZE Integral Assert on the number of rows of the dataframe. HAS_STANDARD_DEVIATION Fractional Numerical Assert on the standard deviation of a feature. HAS_SUM Fractional Numerical Assert on the sum of a feature. HAS_UNIQUENESS Fractional Assert on the uniqueness of a feature, that is the fraction of unique values over the number of all its values. HAS_UNIQUE_VALUE_RATIO Fractional Assert on the unique value ratio of a feature, that is the fraction of unique values over the number of all distinct values. IS_CONTAINED_IN LEGAL_VALUES String Assert that every non-null value of feature is contained in a set of predefined values. IS_GREATER_THAN feature Fractional Assert on feature A values being greater than feature B. IS_GREATER_THAN_OR_EQUAL_TO feature Fractional Assert on feature A values being greater than or equal to those of feature B. IS_LESS_THAN feature Fractional Assert on feature A values being less that feature B. IS_LESS_THAN_OR_EQUAL_TO feature Fractional Assert on feature A values being less or equal to those of feature B. IS_NON_NEGATIVE Fractional Numerical Assert on feature containing non negative values. IS_POSITIVE Fractional Numerical Assert on a feature containing non negative values.","title":"Rule definitions"},{"location":"generated/feature_validation/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_validation/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/feature_validation/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. For example, to get all available rule definitions in hsfs: Python Scala import hsfs connection = hsfs . connection () rules = connection . get_rules () import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules () and to get a rule definition by name: Python Scala import hsfs connection = hsfs . connection () rules = connection . get_rules () import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules ()","title":"get_rule"},{"location":"generated/feature_validation/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#ruledefinition","text":"hsfs . ruledefinition . RuleDefinition ( name , accepted_type , predicate = None , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified. [source]","title":"RuleDefinition"},{"location":"generated/feature_validation/#accepted_type","text":"The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source]","title":"accepted_type"},{"location":"generated/feature_validation/#description","text":"[source]","title":"description"},{"location":"generated/feature_validation/#feature_type","text":"The type of the feature, one of \"Numerical\", \"Categorical\". [source]","title":"feature_type"},{"location":"generated/feature_validation/#name","text":"Name of the rule definition. Unique across all features stores. [source]","title":"name"},{"location":"generated/feature_validation/#predicate","text":"Predicate of the rule definition, one of \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\".","title":"predicate"},{"location":"generated/feature_validation/#rules","text":"Used as part of expectations that are applied on ingested features. Rule names correspond to the names of the rule definitions (see section above) and you can set the severity level and the actual values that the feature should respect.","title":"Rules"},{"location":"generated/feature_validation/#defining-expectation-rules","text":"In general, rule values can be an exact value or a range of values. For example, if you need a feature to be ingested if its minimum value is below zero, then you can set min(0) and max(0) but if you want the minimum to fall within a range of 0 and 1000 then you need to set min(0) and max(1000) . See section Expectations below for a detailed example. Rules that operate on tuples of features, for example HAS_CORRELEATION , are applied on the first two features as defined in the expectation (as ordered within the expectation).","title":"Defining expectation rules"},{"location":"generated/feature_validation/#examples","text":"Python rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 10 )] # the minimum value of the feature needs to be at least 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , max = 10 )] # the minimum value of the feature needs to be at most 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 , max = 10 )] # the minimum value of the feature needs to be between 0 and 10 rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 )] # At least 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.5 )] # 10-50% of all instances of the feature need to of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"IS_CONTAINED_IN\" , level = \"ERROR\" , legal_values = [ \"a\" , \"b\" ], min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be contained in the legal_values list rules = [ Rule ( name = \"HAS_PATTERN\" , level = \"ERROR\" , pattern = \"a+\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to match the given pattern rules = [ Rule ( name = \"IS_LESS_THAN\" , level = \"ERROR\" , feature = \"graduation_date\" )] # All values of the expectation's features must be less than the graduation_date feature values (comparison is done on a per-row basis) rules = [ Rule ( name = \"IS_LESS_THAN\" , level = \"ERROR\" , feature = \"graduation_date\" , min = 0.5 , max = 0.5 )] # Same as above but only 50% of the expectation's feature values need to be less that the graduation_date feature values Scala Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). level ( Level . WARNING ). build () // the mininum value of the feature needs to be at least 10 Rule . createRule ( RuleName . HAS_MIN ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be at most 10 Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be between 0 and 10 Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). level ( Level . ERROR ). build () // At least 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). max ( 10 ). level ( Level . ERROR ). build () // At most 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . IS_CONTAINED_IN ). legalValues ( Seq ( \"a\" , \"b\" )). min ( 0.1 ). max ( 0.1 ). level ( Level . ERROR ). build () // # Exactly 10% of all instances of the feature need to be contained in the legal_values list Rule . createRule ( RuleName . HAS_PATTERN ). pattern ( \"a+\" ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to match the given pattern Rule . createRule ( RuleName . IS_LESS_THAN ). feature ( \"graduation_date\" ). level ( Level . ERROR ). build () // All values of the expectation's features must be less than the graduation_date feature values (comparison is done on a per-row basis) Rule . createRule ( RuleName . IS_LESS_THAN ). feature ( \"graduation_date\" ). min ( 0.5 ). max ( 0.5 ). level ( Level . ERROR ). build () // Same as above but only 50% of the expectation's feature values need to be less that the graduation_date feature values","title":"Examples"},{"location":"generated/feature_validation/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#rule","text":"hsfs . rule . Rule ( name , level , min = None , max = None , pattern = None , accepted_type = None , feature = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only. [source]","title":"Rule"},{"location":"generated/feature_validation/#accepted_type_1","text":"Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source]","title":"accepted_type"},{"location":"generated/feature_validation/#feature","text":"Feature to compare the expectation's features to, applied only to Compliance rules. [source]","title":"feature"},{"location":"generated/feature_validation/#legal_values","text":"List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source]","title":"legal_values"},{"location":"generated/feature_validation/#level","text":"Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source]","title":"level"},{"location":"generated/feature_validation/#max","text":"The upper bound of the value range this feature should fall into. [source]","title":"max"},{"location":"generated/feature_validation/#min","text":"The lower bound of the value range this feature should fall into. [source]","title":"min"},{"location":"generated/feature_validation/#name_1","text":"Name of the rule as found in rule definitions. [source]","title":"name"},{"location":"generated/feature_validation/#pattern","text":"Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule.","title":"pattern"},{"location":"generated/feature_validation/#expectations","text":"A set of rule instances that are applied on a set of features. Expectations are created at the feature store level and can be attached to multiple feature groups. If an expectation contains no features, it will be applied to all the features of the feature group when the validation is done An expectation contains a list of features it is applied to. If the feature group the expectation is attached to, does not contain all the expectations features, the expectation will not be met.","title":"Expectations"},{"location":"generated/feature_validation/#creation","text":"Create an expectation with two rules for ensuring the min and max of the features are valid: Python Scala expectation_sales = fs . create_expectation ( \"sales\" , description = \"min and max sales limits\" , features = [ \"salary\" , \"commission\" ], rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 ), Rule ( name = \"HAS_MAX\" , level = \"ERROR\" , max = 1000000 )]) expectation_sales . save () // Create an expectation for the \"salary\" and \"commissio\" features so that their min value is \"10\" and their max is \"100\" val expectationSales = ( fs . createExpectation () . rules ( Seq ( Rule . createRule ( RuleName . HAS_MIN ). min ( 0 ). level ( Level . WARNING ). build (), //Set rule by name Rule . createRule ( ruleMax ). max ( 1000000 ). level ( Level . ERROR ). build ())) //Set rule by passing the RuleDefinition metadata . name ( \"sales\" ) . description ( \"min and max sales limits\" ) . features ( Seq ( \"salary\" , \"commission\" )) . build ()) expectationSales . save () Create an expectation with a rule to assert that no feature has a null value: Python Scala expectation_notnull = fs . create_expectation ( \"not_null\" , description = \"Assert no feature is null\" , rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"Null\" , max = 0 )]) expectation_notnull . save () val expectationNotNull = ( fs . createExpectation () . rules ( Seq ( Rule . createRule ( RuleName . HAS_DATATYPE ). max ( 0 ). acceptedType ( AcceptedType . Null ). level ( Level . ERROR ). build ())) . name ( \"not_null\" ) . description ( \"Assert no feature is null\" ) . build ()) expectationNotNull . save () [source]","title":"Creation"},{"location":"generated/feature_validation/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object.","title":"create_expectation"},{"location":"generated/feature_validation/#retrieval_1","text":"[source]","title":"Retrieval"},{"location":"generated/feature_validation/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source]","title":"get_expectation"},{"location":"generated/feature_validation/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store.","title":"get_expectations"},{"location":"generated/feature_validation/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#expectation","text":"hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store. [source]","title":"Expectation"},{"location":"generated/feature_validation/#description_1","text":"Description of the expectation. [source]","title":"description"},{"location":"generated/feature_validation/#features","text":"Optional list of features this expectation is applied to. If no features are provided, the expectation will be applied to all the feature group features. [source]","title":"features"},{"location":"generated/feature_validation/#name_2","text":"Name of the expectation, unique per feature store (project). [source]","title":"name"},{"location":"generated/feature_validation/#rules_1","text":"List of rules applied to the features of the expectation.","title":"rules"},{"location":"generated/feature_validation/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_validation/#save","text":"Expectation . save () Persist the expectation metadata object to the feature store.","title":"save"},{"location":"generated/feature_validation/#validations","text":"The results of expectations against the ingested dataframe are assigned a validation time and are persisted within the Feature Store. Users can then retrieve validation results by validation time and by commit time for time-travel enabled feature groups.","title":"Validations"},{"location":"generated/feature_validation/#validation-type","text":"Feature Validation is disabled by default, by having the validation_type feature group attribute set to NONE. The list of allowed validation types are: STRICT: Data validation is performed and feature group is updated only if validation status is \"Success\" WARNING: Data validation is performed and feature group is updated only if validation status is \"Warning\" or lower ALL: Data validation is performed and feature group is updated only if validation status is \"Failure\" or lower NONE: Data validation not performed on feature group For example, to update the validation type to all: Python Scala fg . validation_type = \"ALL\" import com . logicalclocks . hsfs . metadata . validation . _ fg . updateValidationType ( ValidationType . ALL )","title":"Validation Type"},{"location":"generated/feature_validation/#validate","text":"You can also apply the expectations on a dataframe without inserting data, that can be helpful for debugging. [source]","title":"Validate"},{"location":"generated/feature_validation/#validate_1","text":"FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/feature_validation/#retrieval_2","text":"[source]","title":"Retrieval"},{"location":"generated/feature_validation/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"get_validations"},{"location":"generated/feature_validation/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#validationresult","text":"hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group. [source]","title":"ValidationResult"},{"location":"generated/feature_validation/#features_1","text":"Feature of the validation result on which the rule was applied. [source]","title":"features"},{"location":"generated/feature_validation/#message","text":"Message describing the outcome of applying the rule against the feature. [source]","title":"message"},{"location":"generated/feature_validation/#rule_1","text":"Feature of the validation result on which the rule was applied. [source]","title":"rule"},{"location":"generated/feature_validation/#status","text":"[source]","title":"status"},{"location":"generated/feature_validation/#value","text":"The computed value of the feature according to the rule.","title":"value"},{"location":"generated/on_demand_feature_group/","text":"On-Demand (External) Feature Groups # On-demand (External) Feature Groups are Feature Groups for which the data is stored on an external storage system (e.g. Data Warehouse, S3, ADLS). From an API perspective, on-demand feature groups can be used in the same way as regular feature groups. Users can pick features from on-demand feature groups to create training datasets. On-demand feature groups can be also used as data source to create derived features, meaning features on which additional feature engineering is applied. On-demand feature groups rely on Storage Connectors to identify the location and to authenticate with the external storage. When the on-demand feature group is defined on top of an external database capabale of running SQL statements (i.e. when using the JDBC, Redshift or Snowflake connectors), the on-demand feature group needs to be defined as a SQL statement. SQL statements can contain feature engineering transformations, when reading the on-demand feature group, the SQL statement is pushed down to the storage for execution. Python Scala Define a SQL based on-demand feature group # Retrieve the storage connector defined before redshift_conn = fs . get_storage_connector ( \"telco_redshift_cluster\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_redshift\" , version = 1 , query = \"select * from telco\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = redshift_conn , statistics_config = True ) telco_on_dmd . save () Connecting from Hopsworks val redshiftConn = fs . getRedshiftConnector ( \"telco_redshift_cluster\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_redshift_scala\" ) . version ( 2 ) . query ( \"select * from telco\" ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( redshiftConn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save () When defining an on-demand feature group on top of a object store/external filesystem (i.e. when using the S3 or the ADLS connector) the underlying data is required to have a schema. The underlying data can be stored in ORC, Parquet, Delta, Hudi or Avro, and the schema for the feature group will be extracted by the files metadata. Python Scala Define a SQL based on-demand feature group # Retrieve the storage connector defined before s3_conn = fs . get_storage_connector ( \"telco_s3_bucket\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_s3\" , version = 1 , data_format = \"parquet\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = s3_conn , statistics_config = True ) telco_on_dmd . save () Connecting from Hopsworks val s3Conn = fs . getS3Connector ( \"telco_s3_bucket\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_s3\" ) . version ( 1 ) . dataFormat ( OnDemandDataFormat . PARQUET ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( s3Conn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save () Use cases # There are two use cases in which a user can benefit from on-demand feature groups: Existing feature engineering pipelines : in case users have recently migrated to Hopsworks Feature Store and they have existing feature engineering pipelines in production. Users can register the output of the existing pipelines as on-demand feature groups in Hopsworks, and immediately use their features to build training datasets. With on-demand feature groups, users do not have to modify the existing pipelines to write to the Hopsworks Feature Store. Data Ingestion : on-demand feature groups can be used as a data source. The benefit of using on-demand feature groups to ingest data from external sources is that the Hopsworks Feature Store keeps track of where the data is located and how to authenticate with the external storage system. In addition to that, the Hopsworks Feature Store tracks also the schema of the underlying data and will make sure that, if something changes in the underlying schema, the ingestion pipeline fails with a clear error. Limitations # Hopsworks Feature Store does not support time-travel capabilities for on-demand feature groups. Moreover, as the data resides on external systems, on-demand feature groups cannot be made available online for low latency serving. To make data from an on-demand feature group available online, users need to define an online enabled feature group and hava a job that periodically reads data from the on-demand feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on on-demand feature groups. Likewise it is not possibile to call the read() or show() methods on queries containing on-demand feature groups. Nevertheless, on-demand feature groups can be used from a Python engine to create training datasets. Creation # [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. Retrieval # [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] created # [source] creator # [source] data_format # [source] description # [source] event_time # Event time feature in the feature group. [source] expectations_names # The names of expectations attached to this feature group. [source] features # [source] id # [source] location # [source] name # [source] options # [source] path # [source] primary_key # List of features building the primary key. [source] query # [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] storage_connector # [source] validation_type # Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source] version # Methods # [source] add_tag # OnDemandFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # OnDemandFeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] attach_expectation # OnDemandFeatureGroup . attach_expectation ( expectation ) Attach a feature group expectation. If feature group validation is not already enabled, it will be enabled and set to the stricter setting. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] compute_statistics # OnDemandFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # OnDemandFeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # OnDemandFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] detach_expectation # OnDemandFeatureGroup . detach_expectation ( expectation ) Remove an expectation from a feature group. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] filter # OnDemandFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] get_expectation # OnDemandFeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source] get_expectations # OnDemandFeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source] get_feature # OnDemandFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # OnDemandFeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # OnDemandFeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # OnDemandFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_validations # OnDemandFeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source] read # OnDemandFeatureGroup . read ( dataframe_type = \"default\" ) Get the feature group as a DataFrame. [source] save # OnDemandFeatureGroup . save () [source] select # OnDemandFeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # OnDemandFeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # OnDemandFeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # OnDemandFeatureGroup . show ( n ) Show the first n rows of the feature group. [source] update_description # OnDemandFeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # OnDemandFeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # OnDemandFeatureGroup . update_features ( features ) Update a single feature in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # OnDemandFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # OnDemandFeatureGroup . validate () Run validation based on the attached expectations Returns FeatureGroupValidation . The feature group validation metadata object.","title":"On-demand (External) Feature Group"},{"location":"generated/on_demand_feature_group/#on-demand-external-feature-groups","text":"On-demand (External) Feature Groups are Feature Groups for which the data is stored on an external storage system (e.g. Data Warehouse, S3, ADLS). From an API perspective, on-demand feature groups can be used in the same way as regular feature groups. Users can pick features from on-demand feature groups to create training datasets. On-demand feature groups can be also used as data source to create derived features, meaning features on which additional feature engineering is applied. On-demand feature groups rely on Storage Connectors to identify the location and to authenticate with the external storage. When the on-demand feature group is defined on top of an external database capabale of running SQL statements (i.e. when using the JDBC, Redshift or Snowflake connectors), the on-demand feature group needs to be defined as a SQL statement. SQL statements can contain feature engineering transformations, when reading the on-demand feature group, the SQL statement is pushed down to the storage for execution. Python Scala Define a SQL based on-demand feature group # Retrieve the storage connector defined before redshift_conn = fs . get_storage_connector ( \"telco_redshift_cluster\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_redshift\" , version = 1 , query = \"select * from telco\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = redshift_conn , statistics_config = True ) telco_on_dmd . save () Connecting from Hopsworks val redshiftConn = fs . getRedshiftConnector ( \"telco_redshift_cluster\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_redshift_scala\" ) . version ( 2 ) . query ( \"select * from telco\" ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( redshiftConn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save () When defining an on-demand feature group on top of a object store/external filesystem (i.e. when using the S3 or the ADLS connector) the underlying data is required to have a schema. The underlying data can be stored in ORC, Parquet, Delta, Hudi or Avro, and the schema for the feature group will be extracted by the files metadata. Python Scala Define a SQL based on-demand feature group # Retrieve the storage connector defined before s3_conn = fs . get_storage_connector ( \"telco_s3_bucket\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_s3\" , version = 1 , data_format = \"parquet\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = s3_conn , statistics_config = True ) telco_on_dmd . save () Connecting from Hopsworks val s3Conn = fs . getS3Connector ( \"telco_s3_bucket\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_s3\" ) . version ( 1 ) . dataFormat ( OnDemandDataFormat . PARQUET ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( s3Conn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save ()","title":"On-Demand (External) Feature Groups"},{"location":"generated/on_demand_feature_group/#use-cases","text":"There are two use cases in which a user can benefit from on-demand feature groups: Existing feature engineering pipelines : in case users have recently migrated to Hopsworks Feature Store and they have existing feature engineering pipelines in production. Users can register the output of the existing pipelines as on-demand feature groups in Hopsworks, and immediately use their features to build training datasets. With on-demand feature groups, users do not have to modify the existing pipelines to write to the Hopsworks Feature Store. Data Ingestion : on-demand feature groups can be used as a data source. The benefit of using on-demand feature groups to ingest data from external sources is that the Hopsworks Feature Store keeps track of where the data is located and how to authenticate with the external storage system. In addition to that, the Hopsworks Feature Store tracks also the schema of the underlying data and will make sure that, if something changes in the underlying schema, the ingestion pipeline fails with a clear error.","title":"Use cases"},{"location":"generated/on_demand_feature_group/#limitations","text":"Hopsworks Feature Store does not support time-travel capabilities for on-demand feature groups. Moreover, as the data resides on external systems, on-demand feature groups cannot be made available online for low latency serving. To make data from an on-demand feature group available online, users need to define an online enabled feature group and hava a job that periodically reads data from the on-demand feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on on-demand feature groups. Likewise it is not possibile to call the read() or show() methods on queries containing on-demand feature groups. Nevertheless, on-demand feature groups can be used from a Python engine to create training datasets.","title":"Limitations"},{"location":"generated/on_demand_feature_group/#creation","text":"[source]","title":"Creation"},{"location":"generated/on_demand_feature_group/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object.","title":"create_on_demand_feature_group"},{"location":"generated/on_demand_feature_group/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/on_demand_feature_group/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_on_demand_feature_group"},{"location":"generated/on_demand_feature_group/#properties","text":"[source]","title":"Properties"},{"location":"generated/on_demand_feature_group/#created","text":"[source]","title":"created"},{"location":"generated/on_demand_feature_group/#creator","text":"[source]","title":"creator"},{"location":"generated/on_demand_feature_group/#data_format","text":"[source]","title":"data_format"},{"location":"generated/on_demand_feature_group/#description","text":"[source]","title":"description"},{"location":"generated/on_demand_feature_group/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/on_demand_feature_group/#expectations_names","text":"The names of expectations attached to this feature group. [source]","title":"expectations_names"},{"location":"generated/on_demand_feature_group/#features","text":"[source]","title":"features"},{"location":"generated/on_demand_feature_group/#id","text":"[source]","title":"id"},{"location":"generated/on_demand_feature_group/#location","text":"[source]","title":"location"},{"location":"generated/on_demand_feature_group/#name","text":"[source]","title":"name"},{"location":"generated/on_demand_feature_group/#options","text":"[source]","title":"options"},{"location":"generated/on_demand_feature_group/#path","text":"[source]","title":"path"},{"location":"generated/on_demand_feature_group/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/on_demand_feature_group/#query","text":"[source]","title":"query"},{"location":"generated/on_demand_feature_group/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/on_demand_feature_group/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/on_demand_feature_group/#storage_connector","text":"[source]","title":"storage_connector"},{"location":"generated/on_demand_feature_group/#validation_type","text":"Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source]","title":"validation_type"},{"location":"generated/on_demand_feature_group/#version","text":"","title":"version"},{"location":"generated/on_demand_feature_group/#methods","text":"[source]","title":"Methods"},{"location":"generated/on_demand_feature_group/#add_tag","text":"OnDemandFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/on_demand_feature_group/#append_features","text":"OnDemandFeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/on_demand_feature_group/#attach_expectation","text":"OnDemandFeatureGroup . attach_expectation ( expectation ) Attach a feature group expectation. If feature group validation is not already enabled, it will be enabled and set to the stricter setting. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"attach_expectation"},{"location":"generated/on_demand_feature_group/#compute_statistics","text":"OnDemandFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/on_demand_feature_group/#delete","text":"OnDemandFeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/on_demand_feature_group/#delete_tag","text":"OnDemandFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/on_demand_feature_group/#detach_expectation","text":"OnDemandFeatureGroup . detach_expectation ( expectation ) Remove an expectation from a feature group. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"detach_expectation"},{"location":"generated/on_demand_feature_group/#filter","text":"OnDemandFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/on_demand_feature_group/#get_expectation","text":"OnDemandFeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"get_expectation"},{"location":"generated/on_demand_feature_group/#get_expectations","text":"OnDemandFeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source]","title":"get_expectations"},{"location":"generated/on_demand_feature_group/#get_feature","text":"OnDemandFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/on_demand_feature_group/#get_statistics","text":"OnDemandFeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/on_demand_feature_group/#get_tag","text":"OnDemandFeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/on_demand_feature_group/#get_tags","text":"OnDemandFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/on_demand_feature_group/#get_validations","text":"OnDemandFeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source]","title":"get_validations"},{"location":"generated/on_demand_feature_group/#read","text":"OnDemandFeatureGroup . read ( dataframe_type = \"default\" ) Get the feature group as a DataFrame. [source]","title":"read"},{"location":"generated/on_demand_feature_group/#save","text":"OnDemandFeatureGroup . save () [source]","title":"save"},{"location":"generated/on_demand_feature_group/#select","text":"OnDemandFeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/on_demand_feature_group/#select_all","text":"OnDemandFeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/on_demand_feature_group/#select_except","text":"OnDemandFeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/on_demand_feature_group/#show","text":"OnDemandFeatureGroup . show ( n ) Show the first n rows of the feature group. [source]","title":"show"},{"location":"generated/on_demand_feature_group/#update_description","text":"OnDemandFeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/on_demand_feature_group/#update_feature_description","text":"OnDemandFeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/on_demand_feature_group/#update_features","text":"OnDemandFeatureGroup . update_features ( features ) Update a single feature in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/on_demand_feature_group/#update_statistics_config","text":"OnDemandFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/on_demand_feature_group/#validate","text":"OnDemandFeatureGroup . validate () Run validation based on the attached expectations Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/project/","text":"Project/Connection # In Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Feature Store. However, it is possible to share Feature Stores among projects. When working with the Feature Store from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Feature Stores simultaneously. A connection to a Hopsworks instance is represented by a Connection object . Its main purpose is to retrieve the API Key if you are connecting from an external environment and subsequently to retrieve the needed certificates to communicate with the Feature Store services. The handle can then be used to retrieve a reference to the Feature Store you want to operate on. Examples # Python Scala Connecting from Hopsworks import hsfs conn = hsfs . connection () fs = conn . get_feature_store () Connecting from Databricks In order to connect from Databricks, follow the integration guide . You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: Azure Use this method when working with Hopsworks on Azure. import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from AWS SageMaker In order to connect from SageMaker, follow the integration guide to setup the API Key. You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from Python environment To connect from a simple Python environment, you can provide the API Key as a file as shown in the SageMaker example above, or you provide the value directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_value = ( \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" \"xNnAzJ7RV6H\" ) ) fs = conn . get_feature_store () Connecting from Hopsworks import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val fs = connection . getFeatureStore (); Connecting from Databricks TBD Connecting from AWS SageMaker The Scala client version of hsfs is not supported on AWS SageMaker, please use the Python client. Sharing a Feature Store # Connections are on a project-level, however, it is possible to share feature stores among projects, so even if you have a connection to one project, you can retireve a handle to any feature store shared with that project. To share a feature store, you can follow these steps: Sharing a Feature Store Open the project of the feature store that you would like to share on Hopsworks. Go to the Data Set browser and right click the Featurestore.db entry. Click Share with , then select Project and choose the project you wish to share the feature store with. Select the permissions level that the project user members should have on the feature store and click Share . Open the project you just shared the feature store with. Go to the Data Sets browser and there you should see the shared feature store as [project_name_of_shared_feature_store]::Featurestore.db . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this feature store from the other project. Sharing a feature store between projects Accepting a shared feature store from a project Connection Handle # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"Project/Connection"},{"location":"generated/project/#projectconnection","text":"In Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Feature Store. However, it is possible to share Feature Stores among projects. When working with the Feature Store from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Feature Stores simultaneously. A connection to a Hopsworks instance is represented by a Connection object . Its main purpose is to retrieve the API Key if you are connecting from an external environment and subsequently to retrieve the needed certificates to communicate with the Feature Store services. The handle can then be used to retrieve a reference to the Feature Store you want to operate on.","title":"Project/Connection"},{"location":"generated/project/#examples","text":"Python Scala Connecting from Hopsworks import hsfs conn = hsfs . connection () fs = conn . get_feature_store () Connecting from Databricks In order to connect from Databricks, follow the integration guide . You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: Azure Use this method when working with Hopsworks on Azure. import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from AWS SageMaker In order to connect from SageMaker, follow the integration guide to setup the API Key. You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from Python environment To connect from a simple Python environment, you can provide the API Key as a file as shown in the SageMaker example above, or you provide the value directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_value = ( \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" \"xNnAzJ7RV6H\" ) ) fs = conn . get_feature_store () Connecting from Hopsworks import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val fs = connection . getFeatureStore (); Connecting from Databricks TBD Connecting from AWS SageMaker The Scala client version of hsfs is not supported on AWS SageMaker, please use the Python client.","title":"Examples"},{"location":"generated/project/#sharing-a-feature-store","text":"Connections are on a project-level, however, it is possible to share feature stores among projects, so even if you have a connection to one project, you can retireve a handle to any feature store shared with that project. To share a feature store, you can follow these steps: Sharing a Feature Store Open the project of the feature store that you would like to share on Hopsworks. Go to the Data Set browser and right click the Featurestore.db entry. Click Share with , then select Project and choose the project you wish to share the feature store with. Select the permissions level that the project user members should have on the feature store and click Share . Open the project you just shared the feature store with. Go to the Data Sets browser and there you should see the shared feature store as [project_name_of_shared_feature_store]::Featurestore.db . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this feature store from the other project. Sharing a feature store between projects Accepting a shared feature store from a project","title":"Sharing a Feature Store"},{"location":"generated/project/#connection-handle","text":"[source]","title":"Connection Handle"},{"location":"generated/project/#connection","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/project/#methods","text":"[source]","title":"Methods"},{"location":"generated/project/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/project/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/project/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/project/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rule"},{"location":"generated/project/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/project/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"setup_databricks"},{"location":"generated/query_vs_dataframe/","text":"Query vs DataFrame # HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters and point in time queries. To enable this functionality, we are introducing a new expressive Query abstraction with HSFS that provides these operations and guarantees reproducible creation of training datasets from features in the Feature Store. The new joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python Scala # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d ) # materialize query in the specified file format td . save ( feature_join ) # use materialized training dataset for training, possibly in a different environment td = fs . get_training_dataset ( \u201c rain_dataset \u201d , version = 1 ) # get TFRecordDataset to use in a TensorFlow model dataset = td . tf_data () . tf_record_dataset ( batch_size = 32 , num_epochs = 100 ) # reproduce query for online feature store and drop label for inference jdbc_querystring = td . get_query ( online = True , with_label = False ) # create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . build ()) # materialize query in the specified file format td . save ( featureJoin ) # use materialized training dataset for training , possibly in a different environment val td = fs . getTrainingDataset ( \u201c rain_dataset \u201d , 1 ) # reproduce query for online feature store and drop label for inference val jdbcQuerystring = td . getQuery ( true , false ) If a data scientist wants to modify a new feature that is not available in the Feature Store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the Feature Store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources. The Query Abstraction # Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation. Examples # Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python Scala rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" )) Join # Similarly joins return queries. The simplest join, is one of two feature groups without specifying a join key or type. By default Hopsworks will use the maximal matching subset of the primary key of the two feature groups as joining key, if not specified otherwise. Python Scala # Returns Query feature_join = rain_fg . join ( temperature_fg ) # Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joines feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the name of the features to join on. Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure. Filter # In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python Scala filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ))) Methods # [source] as_of # Query . as_of ( wallclock_time ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source] filter # Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_cache_feature_group_only # Query . from_cache_feature_group_only () [source] from_response_json # Query . from_response_json ( json_dict ) [source] join # Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source] json # Query . json () [source] pull_changes # Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source] read # Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source] show # Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source] to_dict # Query . to_dict () [source] to_string # Query . to_string ( online = False ) Properties # [source] left_feature_group_end_time # [source] left_feature_group_start_time #","title":"Query vs. Dataframe"},{"location":"generated/query_vs_dataframe/#query-vs-dataframe","text":"HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters and point in time queries. To enable this functionality, we are introducing a new expressive Query abstraction with HSFS that provides these operations and guarantees reproducible creation of training datasets from features in the Feature Store. The new joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python Scala # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d ) # materialize query in the specified file format td . save ( feature_join ) # use materialized training dataset for training, possibly in a different environment td = fs . get_training_dataset ( \u201c rain_dataset \u201d , version = 1 ) # get TFRecordDataset to use in a TensorFlow model dataset = td . tf_data () . tf_record_dataset ( batch_size = 32 , num_epochs = 100 ) # reproduce query for online feature store and drop label for inference jdbc_querystring = td . get_query ( online = True , with_label = False ) # create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . build ()) # materialize query in the specified file format td . save ( featureJoin ) # use materialized training dataset for training , possibly in a different environment val td = fs . getTrainingDataset ( \u201c rain_dataset \u201d , 1 ) # reproduce query for online feature store and drop label for inference val jdbcQuerystring = td . getQuery ( true , false ) If a data scientist wants to modify a new feature that is not available in the Feature Store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the Feature Store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.","title":"Query vs DataFrame"},{"location":"generated/query_vs_dataframe/#the-query-abstraction","text":"Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation.","title":"The Query Abstraction"},{"location":"generated/query_vs_dataframe/#examples","text":"Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python Scala rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" ))","title":"Examples"},{"location":"generated/query_vs_dataframe/#join","text":"Similarly joins return queries. The simplest join, is one of two feature groups without specifying a join key or type. By default Hopsworks will use the maximal matching subset of the primary key of the two feature groups as joining key, if not specified otherwise. Python Scala # Returns Query feature_join = rain_fg . join ( temperature_fg ) # Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joines feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the name of the features to join on. Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure.","title":"Join"},{"location":"generated/query_vs_dataframe/#filter","text":"In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python Scala filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )))","title":"Filter"},{"location":"generated/query_vs_dataframe/#methods","text":"[source]","title":"Methods"},{"location":"generated/query_vs_dataframe/#as_of","text":"Query . as_of ( wallclock_time ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/query_vs_dataframe/#filter_1","text":"Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/query_vs_dataframe/#from_cache_feature_group_only","text":"Query . from_cache_feature_group_only () [source]","title":"from_cache_feature_group_only"},{"location":"generated/query_vs_dataframe/#from_response_json","text":"Query . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/query_vs_dataframe/#join_1","text":"Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source]","title":"join"},{"location":"generated/query_vs_dataframe/#json","text":"Query . json () [source]","title":"json"},{"location":"generated/query_vs_dataframe/#pull_changes","text":"Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source]","title":"pull_changes"},{"location":"generated/query_vs_dataframe/#read","text":"Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source]","title":"read"},{"location":"generated/query_vs_dataframe/#show","text":"Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source]","title":"show"},{"location":"generated/query_vs_dataframe/#to_dict","text":"Query . to_dict () [source]","title":"to_dict"},{"location":"generated/query_vs_dataframe/#to_string","text":"Query . to_string ( online = False )","title":"to_string"},{"location":"generated/query_vs_dataframe/#properties","text":"[source]","title":"Properties"},{"location":"generated/query_vs_dataframe/#left_feature_group_end_time","text":"[source]","title":"left_feature_group_end_time"},{"location":"generated/query_vs_dataframe/#left_feature_group_start_time","text":"","title":"left_feature_group_start_time"},{"location":"generated/statistics/","text":"Statistics # HSFS provides functionality to compute statistics for training datasets and feature groups and save these along with their other metadata in the feature store . These statistics are meant to be helpful for Data Scientists to perform explorative data analysis and then recognize suitable features or training datasets for models. Statistics are configured on a training dataset or feature group level using a StatisticsConfig object. This object can be passed at creation time of the dataset or group or it can later on be updated through the API. [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [] ) For example, to enable all statistics (descriptive, histograms and correlations) for a training dataset: Python Scala from hsfs.statistics_config import StatisticsConfig td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d , statistics_config = StatisticsConfig ( true , true , true )) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . statisticsConfig ( new StatisticsConfig ( true , true , true )) . build ()) And similarly for feature groups. Default StatisticsConfig By default all training datasets and feature groups will be configured such that only descriptive statistics are computed. However, you can also enable histograms and correlations or limit the features for which statistics are computed. Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] exact_uniqueness # Enable exact uniqueness as an additional statistic to be computed for each feature. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"Statistics"},{"location":"generated/statistics/#statistics","text":"HSFS provides functionality to compute statistics for training datasets and feature groups and save these along with their other metadata in the feature store . These statistics are meant to be helpful for Data Scientists to perform explorative data analysis and then recognize suitable features or training datasets for models. Statistics are configured on a training dataset or feature group level using a StatisticsConfig object. This object can be passed at creation time of the dataset or group or it can later on be updated through the API. [source]","title":"Statistics"},{"location":"generated/statistics/#statisticsconfig","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [] ) For example, to enable all statistics (descriptive, histograms and correlations) for a training dataset: Python Scala from hsfs.statistics_config import StatisticsConfig td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d , statistics_config = StatisticsConfig ( true , true , true )) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . statisticsConfig ( new StatisticsConfig ( true , true , true )) . build ()) And similarly for feature groups. Default StatisticsConfig By default all training datasets and feature groups will be configured such that only descriptive statistics are computed. However, you can also enable histograms and correlations or limit the features for which statistics are computed.","title":"StatisticsConfig"},{"location":"generated/statistics/#properties","text":"[source]","title":"Properties"},{"location":"generated/statistics/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/statistics/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/statistics/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/statistics/#exact_uniqueness","text":"Enable exact uniqueness as an additional statistic to be computed for each feature. [source]","title":"exact_uniqueness"},{"location":"generated/statistics/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/storage_connector/","text":"Storage Connector # Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. HopsFS # Properties # [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # HopsFSConnector . refetch () Refetch storage connector. [source] spark_options # HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # HopsFSConnector . to_dict () [source] update_from_response_json # HopsFSConnector . update_from_response_json ( json_dict ) JDBC # Properties # [source] arguments # Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source] connection_string # JDBC connection string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # JdbcConnector . refetch () Refetch storage connector. [source] spark_options # JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # JdbcConnector . to_dict () [source] update_from_response_json # JdbcConnector . update_from_response_json ( json_dict ) S3 # Properties # [source] access_key # Access key. [source] bucket # Return the bucket for S3 connectors. [source] description # User provided description of the storage connector. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] session_token # Session token. Methods # [source] prepare_spark # S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # S3Connector . refetch () Refetch storage connector. [source] spark_options # S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # S3Connector . to_dict () [source] update_from_response_json # S3Connector . update_from_response_json ( json_dict ) Redshift # Properties # [source] arguments # Additional JDBC, REDSHIFT, or Snowflake arguments. [source] auto_create # Database username for redshift cluster. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] description # User provided description of the storage connector. [source] expiration # Cluster temporary credential expiration time. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] table_name # Table name for redshift cluster. Methods # [source] read # RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # RedshiftConnector . to_dict () [source] update_from_response_json # RedshiftConnector . update_from_response_json ( json_dict ) Azure Data Lake Storage # Properties # [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] container_name # Container name of the ADLS storage connector [source] description # User provided description of the storage connector. [source] directory_id # Directory ID of the ADLS storage connector [source] generation # Generation of the ADLS storage connector [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. ADLS) - return the path of the connector [source] service_credential # Service credential of the ADLS storage connector Methods # [source] prepare_spark # AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # AdlsConnector . refetch () Refetch storage connector. [source] spark_options # AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # AdlsConnector . to_dict () [source] update_from_response_json # AdlsConnector . update_from_response_json ( json_dict ) Snowflake # Properties # [source] account # Account of the Snowflake storage connector [source] application # Application of the Snowflake storage connector [source] database # Database of the Snowflake storage connector [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Additional options for the Snowflake storage connector [source] password # Password of the Snowflake storage connector [source] role # Role of the Snowflake storage connector [source] schema # Schema of the Snowflake storage connector [source] table # Table of the Snowflake storage connector [source] token # OAuth token of the Snowflake storage connector [source] url # URL of the Snowflake storage connector [source] user # User of the Snowflake storage connector [source] warehouse # Warehouse of the Snowflake storage connector Methods # [source] read # SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # SnowflakeConnector . refetch () Refetch storage connector. [source] snowflake_connector_options # SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source] spark_options # SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # SnowflakeConnector . to_dict () [source] update_from_response_json # SnowflakeConnector . update_from_response_json ( json_dict ) Google Cloud Storage # This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Properties # [source] algorithm # Encryption Algorithm [source] bucket # GCS Bucket [source] description # User provided description of the storage connector. [source] encryption_key # Encryption Key [source] encryption_key_hash # Encryption Key Hash [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] name # Name of the storage connector. [source] path # the path of the connector along with gs file system prefixed Methods # [source] prepare_spark # GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source] read # GcsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads GCS path into a dataframe using the storage connector. conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # GcsConnector . refetch () Refetch storage connector. [source] spark_options # GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # GcsConnector . to_dict () [source] update_from_response_json # GcsConnector . update_from_response_json ( json_dict ) BigQuery # The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Properties # [source] arguments # Additional spark options [source] dataset # BigQuery dataset (The dataset containing the table) [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] materialization_dataset # BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source] name # Name of the storage connector. [source] parent_project # BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source] query_project # BigQuery project (The Google Cloud Project ID of the table) [source] query_table # BigQuery table name Methods # [source] read # BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # BigQueryConnector . refetch () Refetch storage connector. [source] spark_options # BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source] to_dict # BigQueryConnector . to_dict () [source] update_from_response_json # BigQueryConnector . update_from_response_json ( json_dict )","title":"Storage Connector"},{"location":"generated/storage_connector/#storage-connector","text":"Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.","title":"Storage Connector"},{"location":"generated/storage_connector/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/storage_connector/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/storage_connector/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/storage_connector/#hopsfs","text":"","title":"HopsFS"},{"location":"generated/storage_connector/#properties","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#description","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name","text":"Name of the storage connector.","title":"name"},{"location":"generated/storage_connector/#methods","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read","text":"HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/storage_connector/#refetch","text":"HopsFSConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options","text":"HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict","text":"HopsFSConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json","text":"HopsFSConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#jdbc","text":"","title":"JDBC"},{"location":"generated/storage_connector/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#arguments","text":"Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source]","title":"arguments"},{"location":"generated/storage_connector/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/storage_connector/#description_1","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id_1","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_1","text":"Name of the storage connector.","title":"name"},{"location":"generated/storage_connector/#methods_1","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_1","text":"JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_1","text":"JdbcConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_1","text":"JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_1","text":"JdbcConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_1","text":"JdbcConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#s3","text":"","title":"S3"},{"location":"generated/storage_connector/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/storage_connector/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/storage_connector/#description_2","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/storage_connector/#id_2","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_2","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/storage_connector/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/storage_connector/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/storage_connector/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/storage_connector/#session_token","text":"Session token.","title":"session_token"},{"location":"generated/storage_connector/#methods_2","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#prepare_spark","text":"S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/storage_connector/#read_2","text":"S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_2","text":"S3Connector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_2","text":"S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_2","text":"S3Connector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_2","text":"S3Connector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#redshift","text":"","title":"Redshift"},{"location":"generated/storage_connector/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#arguments_1","text":"Additional JDBC, REDSHIFT, or Snowflake arguments. [source]","title":"arguments"},{"location":"generated/storage_connector/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/storage_connector/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/storage_connector/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/storage_connector/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/storage_connector/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/storage_connector/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/storage_connector/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/storage_connector/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/storage_connector/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/storage_connector/#description_3","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/storage_connector/#iam_role_1","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/storage_connector/#id_3","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_3","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/storage_connector/#methods_3","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_3","text":"RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_3","text":"RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_3","text":"RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_3","text":"RedshiftConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_3","text":"RedshiftConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#azure-data-lake-storage","text":"","title":"Azure Data Lake Storage"},{"location":"generated/storage_connector/#properties_4","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/storage_connector/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/storage_connector/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/storage_connector/#description_4","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/storage_connector/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/storage_connector/#id_4","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_4","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#path_1","text":"If the connector refers to a path (e.g. ADLS) - return the path of the connector [source]","title":"path"},{"location":"generated/storage_connector/#service_credential","text":"Service credential of the ADLS storage connector","title":"service_credential"},{"location":"generated/storage_connector/#methods_4","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#prepare_spark_1","text":"AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/storage_connector/#read_4","text":"AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_4","text":"AdlsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_4","text":"AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_4","text":"AdlsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_4","text":"AdlsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#snowflake","text":"","title":"Snowflake"},{"location":"generated/storage_connector/#properties_5","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#account","text":"Account of the Snowflake storage connector [source]","title":"account"},{"location":"generated/storage_connector/#application","text":"Application of the Snowflake storage connector [source]","title":"application"},{"location":"generated/storage_connector/#database","text":"Database of the Snowflake storage connector [source]","title":"database"},{"location":"generated/storage_connector/#description_5","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id_5","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_5","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#options","text":"Additional options for the Snowflake storage connector [source]","title":"options"},{"location":"generated/storage_connector/#password","text":"Password of the Snowflake storage connector [source]","title":"password"},{"location":"generated/storage_connector/#role","text":"Role of the Snowflake storage connector [source]","title":"role"},{"location":"generated/storage_connector/#schema","text":"Schema of the Snowflake storage connector [source]","title":"schema"},{"location":"generated/storage_connector/#table","text":"Table of the Snowflake storage connector [source]","title":"table"},{"location":"generated/storage_connector/#token","text":"OAuth token of the Snowflake storage connector [source]","title":"token"},{"location":"generated/storage_connector/#url","text":"URL of the Snowflake storage connector [source]","title":"url"},{"location":"generated/storage_connector/#user","text":"User of the Snowflake storage connector [source]","title":"user"},{"location":"generated/storage_connector/#warehouse","text":"Warehouse of the Snowflake storage connector","title":"warehouse"},{"location":"generated/storage_connector/#methods_5","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_5","text":"SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_5","text":"SnowflakeConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#snowflake_connector_options","text":"SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source]","title":"snowflake_connector_options"},{"location":"generated/storage_connector/#spark_options_5","text":"SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_5","text":"SnowflakeConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_5","text":"SnowflakeConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#google-cloud-storage","text":"This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop","title":"Google Cloud Storage"},{"location":"generated/storage_connector/#properties_6","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#algorithm","text":"Encryption Algorithm [source]","title":"algorithm"},{"location":"generated/storage_connector/#bucket_1","text":"GCS Bucket [source]","title":"bucket"},{"location":"generated/storage_connector/#description_6","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#encryption_key","text":"Encryption Key [source]","title":"encryption_key"},{"location":"generated/storage_connector/#encryption_key_hash","text":"Encryption Key Hash [source]","title":"encryption_key_hash"},{"location":"generated/storage_connector/#id_6","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#key_path","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/storage_connector/#name_6","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#path_2","text":"the path of the connector along with gs file system prefixed","title":"path"},{"location":"generated/storage_connector/#methods_6","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#prepare_spark_2","text":"GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/storage_connector/#read_6","text":"GcsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads GCS path into a dataframe using the storage connector. conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_6","text":"GcsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_6","text":"GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_6","text":"GcsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_6","text":"GcsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#bigquery","text":"The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.","title":"BigQuery"},{"location":"generated/storage_connector/#properties_7","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#arguments_2","text":"Additional spark options [source]","title":"arguments"},{"location":"generated/storage_connector/#dataset","text":"BigQuery dataset (The dataset containing the table) [source]","title":"dataset"},{"location":"generated/storage_connector/#description_7","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id_7","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#key_path_1","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/storage_connector/#materialization_dataset","text":"BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source]","title":"materialization_dataset"},{"location":"generated/storage_connector/#name_7","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#parent_project","text":"BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source]","title":"parent_project"},{"location":"generated/storage_connector/#query_project","text":"BigQuery project (The Google Cloud Project ID of the table) [source]","title":"query_project"},{"location":"generated/storage_connector/#query_table","text":"BigQuery table name","title":"query_table"},{"location":"generated/storage_connector/#methods_7","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_7","text":"BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_7","text":"BigQueryConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_7","text":"BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_7","text":"BigQueryConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_7","text":"BigQueryConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/tags/","text":"Tags # The feature store enables users to attach tags to artifacts, such as feature groups or training datasets. Tags are aditional metadata attached to your artifacts and thus they can be used for an enhanced full text search. Adding tags to an artifact provides users with a more dynamic metadata content that can be used for both storage as well as enhancing artifact discoverability. Note : By default Hopsworks makes all metadata searchable, users can opt out for particular featurestores if they want to keep them private. A tag is a { key : value } association, providing additional information about the data, such as for example geographic origin. This is useful in an organization as it adds more context to your data making it easier to share and discover data and artifacts. Note : Tagging is only available in the enterprise version. Tag Schemas # The first step is to define the schemas of tags that can later be attached. These schemas follow the https://json-schema.org as reference. The schemas define legal jsons and these can be primitives, objects or arrays. The schemas themselves are also defined as jsons. Allowed primitive types are: string boolean integer number (float) A tag of primitive type - string would look like: { \"type\" : \"string\" } and this would allow a json value of: string tag value We can also define arbitrarily complex json schemas, such as: { \"type\" : \"object\", \"properties\" : { \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } } }, \"required\" : [\"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a value that follows this schema would be: { \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"] } Properties section of a tag is a dictionary that defines field names and types. Json schema are pretty lenient, all that the properties section tells us, is that if a field appears, it should be of the appropriate type. If the json object contains the field first_name , this field cannot be of type boolean , it has to be of type string . What we emphasize here, is that the properties section does not impose that fields declared are mandatory, or that the json object cannot contain other fields that were not defined in the schemas. Required section enforces the mandatory fields. In our case above first_name , last_name , age are declared as mandatory, while hobbies is left as an optional field. Additional Properties section enforces the strictness of the schema. If we set this to false the json objects of this schema can only use fields that are declared (mandatoriy or optional) by the schema. No undeclared fields will be allowed. Type object is the default type for schemas, so you can ommit it if you want to keep the schema short. Advanced tag usage # We can use additional properties of schemas as defined by https://json-schema.org to enhance our previous person schema: Add a $schema section to allow us to use more advanced features of the json schemas defined in later drafts. The default schema draft is 4 and we will use 7 here (latest). Add an id field that is of type string but has to follow a particular regex pattern. We will also make this field mandatory. Set some rules on age , for example age should be an Integer between 0 and 150. Add an address field that is itself an object. { \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\" : \"object\", \"properties\" : { \"id\" : { \"type\" : \"string\", \"pattern\" : \"^[A-Z]{2}[0-9]{4}$\" }, \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\", \"minimum\" : 0, \"maximum\" : 150 }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } }, \"address\" : { \"street\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" } } }, \"required\" : [\"id\", \"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a valid value for this new schema would be: { \"id\" : \"AB1234\", \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"], \"address\" : { \"street\" : \"Vasagatan nr. 12\", \"city\" : \"Stockholm\" } } Basic tag usage # Our new Feature Store UI is aimed to ease the general use of tags by users and we thus currently support only basic tags usage there. Basic tag schemas allow only one level depth fields. So types of fields are limited to primitives or array of primitives. Basic schemas also only allow the required and additionalProperties sections. Creating schemas # Schemas are defined at a cluster level, so they are available to all projects. They can only be defined by a user with admin rights. Attach tags using the UI # Tags can be attached using the feature store UI or programmatically using the API. This API will be described in the rest of this notebook. Search with Tags # Once tags are attached, the feature groups are now searchable also by their tags, both keys and values. Advanced tag usage # Our old UI allows for the full capabilities of the json schemas as defined by https://json-schema.org . This includes allowing to define tags of primitive type as well as arbitrarily complex json objects. Creating advanced schemas # If you want to create advanced schemas, you can do this in our old UI by providing the raw json value of the schema. Attach advanced tag values using the UI # Our old UI also provides you with a way to attach tag values that follow these advanced semantics by providing their raw json values. Examples # You can try our tags example in notebooks populated by our feature store tour under notebooks hsfs/tags . You can also check our example on https://examples.hopsworks.ai/featurestore/hsfs/tags/feature_store_tags/ . API # Both feature groups and training datasets contain the following add, get, get_all, delete operations for working with tags. Feature Groups # Attach tags # Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source] add_tag # FeatureGroupBase . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. Get tag by key # [source] get_tag # FeatureGroupBase . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. Get all tags # [source] get_tags # FeatureGroupBase . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. Delete tag # [source] delete_tag # FeatureGroupBase . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. Training Datasets # Attach tags # Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. Get tag by key # [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. Get all tags # [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. Delete tag # [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag.","title":"Feature Store Tags"},{"location":"generated/tags/#tags","text":"The feature store enables users to attach tags to artifacts, such as feature groups or training datasets. Tags are aditional metadata attached to your artifacts and thus they can be used for an enhanced full text search. Adding tags to an artifact provides users with a more dynamic metadata content that can be used for both storage as well as enhancing artifact discoverability. Note : By default Hopsworks makes all metadata searchable, users can opt out for particular featurestores if they want to keep them private. A tag is a { key : value } association, providing additional information about the data, such as for example geographic origin. This is useful in an organization as it adds more context to your data making it easier to share and discover data and artifacts. Note : Tagging is only available in the enterprise version.","title":"Tags"},{"location":"generated/tags/#tag-schemas","text":"The first step is to define the schemas of tags that can later be attached. These schemas follow the https://json-schema.org as reference. The schemas define legal jsons and these can be primitives, objects or arrays. The schemas themselves are also defined as jsons. Allowed primitive types are: string boolean integer number (float) A tag of primitive type - string would look like: { \"type\" : \"string\" } and this would allow a json value of: string tag value We can also define arbitrarily complex json schemas, such as: { \"type\" : \"object\", \"properties\" : { \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } } }, \"required\" : [\"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a value that follows this schema would be: { \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"] } Properties section of a tag is a dictionary that defines field names and types. Json schema are pretty lenient, all that the properties section tells us, is that if a field appears, it should be of the appropriate type. If the json object contains the field first_name , this field cannot be of type boolean , it has to be of type string . What we emphasize here, is that the properties section does not impose that fields declared are mandatory, or that the json object cannot contain other fields that were not defined in the schemas. Required section enforces the mandatory fields. In our case above first_name , last_name , age are declared as mandatory, while hobbies is left as an optional field. Additional Properties section enforces the strictness of the schema. If we set this to false the json objects of this schema can only use fields that are declared (mandatoriy or optional) by the schema. No undeclared fields will be allowed. Type object is the default type for schemas, so you can ommit it if you want to keep the schema short.","title":"Tag Schemas"},{"location":"generated/tags/#advanced-tag-usage","text":"We can use additional properties of schemas as defined by https://json-schema.org to enhance our previous person schema: Add a $schema section to allow us to use more advanced features of the json schemas defined in later drafts. The default schema draft is 4 and we will use 7 here (latest). Add an id field that is of type string but has to follow a particular regex pattern. We will also make this field mandatory. Set some rules on age , for example age should be an Integer between 0 and 150. Add an address field that is itself an object. { \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\" : \"object\", \"properties\" : { \"id\" : { \"type\" : \"string\", \"pattern\" : \"^[A-Z]{2}[0-9]{4}$\" }, \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\", \"minimum\" : 0, \"maximum\" : 150 }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } }, \"address\" : { \"street\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" } } }, \"required\" : [\"id\", \"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a valid value for this new schema would be: { \"id\" : \"AB1234\", \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"], \"address\" : { \"street\" : \"Vasagatan nr. 12\", \"city\" : \"Stockholm\" } }","title":"Advanced tag usage"},{"location":"generated/tags/#basic-tag-usage","text":"Our new Feature Store UI is aimed to ease the general use of tags by users and we thus currently support only basic tags usage there. Basic tag schemas allow only one level depth fields. So types of fields are limited to primitives or array of primitives. Basic schemas also only allow the required and additionalProperties sections.","title":"Basic tag usage"},{"location":"generated/tags/#creating-schemas","text":"Schemas are defined at a cluster level, so they are available to all projects. They can only be defined by a user with admin rights.","title":"Creating schemas"},{"location":"generated/tags/#attach-tags-using-the-ui","text":"Tags can be attached using the feature store UI or programmatically using the API. This API will be described in the rest of this notebook.","title":"Attach tags using the UI"},{"location":"generated/tags/#search-with-tags","text":"Once tags are attached, the feature groups are now searchable also by their tags, both keys and values.","title":"Search with Tags"},{"location":"generated/tags/#advanced-tag-usage_1","text":"Our old UI allows for the full capabilities of the json schemas as defined by https://json-schema.org . This includes allowing to define tags of primitive type as well as arbitrarily complex json objects.","title":"Advanced tag usage"},{"location":"generated/tags/#creating-advanced-schemas","text":"If you want to create advanced schemas, you can do this in our old UI by providing the raw json value of the schema.","title":"Creating advanced schemas"},{"location":"generated/tags/#attach-advanced-tag-values-using-the-ui","text":"Our old UI also provides you with a way to attach tag values that follow these advanced semantics by providing their raw json values.","title":"Attach advanced tag values using the UI"},{"location":"generated/tags/#examples","text":"You can try our tags example in notebooks populated by our feature store tour under notebooks hsfs/tags . You can also check our example on https://examples.hopsworks.ai/featurestore/hsfs/tags/feature_store_tags/ .","title":"Examples"},{"location":"generated/tags/#api","text":"Both feature groups and training datasets contain the following add, get, get_all, delete operations for working with tags.","title":"API"},{"location":"generated/tags/#feature-groups","text":"","title":"Feature Groups"},{"location":"generated/tags/#attach-tags","text":"Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source]","title":"Attach tags"},{"location":"generated/tags/#add_tag","text":"FeatureGroupBase . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"add_tag"},{"location":"generated/tags/#get-tag-by-key","text":"[source]","title":"Get tag by key"},{"location":"generated/tags/#get_tag","text":"FeatureGroupBase . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag.","title":"get_tag"},{"location":"generated/tags/#get-all-tags","text":"[source]","title":"Get all tags"},{"location":"generated/tags/#get_tags","text":"FeatureGroupBase . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags.","title":"get_tags"},{"location":"generated/tags/#delete-tag","text":"[source]","title":"Delete tag"},{"location":"generated/tags/#delete_tag","text":"FeatureGroupBase . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag.","title":"delete_tag"},{"location":"generated/tags/#training-datasets","text":"","title":"Training Datasets"},{"location":"generated/tags/#attach-tags_1","text":"Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source]","title":"Attach tags"},{"location":"generated/tags/#add_tag_1","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"add_tag"},{"location":"generated/tags/#get-tag-by-key_1","text":"[source]","title":"Get tag by key"},{"location":"generated/tags/#get_tag_1","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag.","title":"get_tag"},{"location":"generated/tags/#get-all-tags_1","text":"[source]","title":"Get all tags"},{"location":"generated/tags/#get_tags_1","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags.","title":"get_tags"},{"location":"generated/tags/#delete-tag_1","text":"[source]","title":"Delete tag"},{"location":"generated/tags/#delete_tag_1","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag.","title":"delete_tag"},{"location":"generated/training_dataset/","text":"Training Dataset # The training dataset abstraction in Hopsworks Feature Store allows users to group a set of features (potentially from multiple different feature groups) with labels for training a model to do a particular prediction task. The training dataset is a versioned and managed dataset and is stored in HopsFS as tfrecords , parquet , csv , or tsv files. Versioning # Training Dataset can be versioned. Data Scientist should use the version to indicate to the model, as well as to the schema or the feature engineering logic of the features associated to this training dataset. Creation # To create training dataset, the user supplies a Pandas, Numpy or Spark dataframe with features and labels together with metadata. Once the training dataset has been created, the dataset is discoverable in the feature registry and users can use it to train models. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] coalesce # If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source] data_format # File format of the training dataset. [source] description # [source] event_end_time # [source] event_start_time # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] train_split # Set name of training dataset split that is used for training. [source] training_dataset_type # [source] transformation_functions # Set transformation functions. [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete # TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] from_response_json_single # TrainingDataset . from_response_json_single ( json_dict ) [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_serving_vector # TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_serving_vectors # TrainingDataset . get_serving_vectors ( entry , external = False ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] init_prepared_statement # TrainingDataset . init_prepared_statement ( batch = None , external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] tf_data # TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , deterministic = False , file_pattern = \"*.tfrecord*\" , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . deterministic Optional[bool] : Controls the order in which the transformation produces elements. If set to False, the transformation is allowed to yield elements out of order to trade determinism for performance. Defaults to False . file_pattern Optional[str] : Returns a list of files that match the given pattern Defaults to *.tfrecord* . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError . TFData engine # [source] tf_record_dataset # TFDataEngine . tf_record_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False , serialized_ndarray_fname = [], ) Reads tfrecord files and returns ParallelMapDataset or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object ParallelMapDataset can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 3 ) td . tf_data ( target_name = \"id\" ) . tf_record_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set True then one hot encode labels, defaults to False . num_classes Optional[int] : If above True then provide number of target classes, defaults to None . process Optional[bool] : If set True api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname Optional[list] : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . ParallelMapDataset . If process is set to False . [source] tf_csv_dataset # TFDataEngine . tf_csv_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False ) Reads csv files and returns CsvDatasetV2 or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object CsvDatasetV2 can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 1 ) td . tf_data ( target_name = \"id\" ) . tf_csv_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set true then one hot encode labels, defaults to False . num_classes Optional[int] : If above true then provide number of target classes, defaults to None . process Optional[bool] : If set true api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . CsvDatasetV2 . If process is set to False .","title":"Training Dataset"},{"location":"generated/training_dataset/#training-dataset","text":"The training dataset abstraction in Hopsworks Feature Store allows users to group a set of features (potentially from multiple different feature groups) with labels for training a model to do a particular prediction task. The training dataset is a versioned and managed dataset and is stored in HopsFS as tfrecords , parquet , csv , or tsv files.","title":"Training Dataset"},{"location":"generated/training_dataset/#versioning","text":"Training Dataset can be versioned. Data Scientist should use the version to indicate to the model, as well as to the schema or the feature engineering logic of the features associated to this training dataset.","title":"Versioning"},{"location":"generated/training_dataset/#creation","text":"To create training dataset, the user supplies a Pandas, Numpy or Spark dataframe with features and labels together with metadata. Once the training dataset has been created, the dataset is discoverable in the feature registry and users can use it to train models. [source]","title":"Creation"},{"location":"generated/training_dataset/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/training_dataset/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/training_dataset/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/training_dataset/#properties","text":"[source]","title":"Properties"},{"location":"generated/training_dataset/#coalesce","text":"If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source]","title":"coalesce"},{"location":"generated/training_dataset/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/training_dataset/#description","text":"[source]","title":"description"},{"location":"generated/training_dataset/#event_end_time","text":"[source]","title":"event_end_time"},{"location":"generated/training_dataset/#event_start_time","text":"[source]","title":"event_start_time"},{"location":"generated/training_dataset/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/training_dataset/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/training_dataset/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/training_dataset/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/training_dataset/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/training_dataset/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/training_dataset/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/training_dataset/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/training_dataset/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/training_dataset/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/training_dataset/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/training_dataset/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/training_dataset/#train_split","text":"Set name of training dataset split that is used for training. [source]","title":"train_split"},{"location":"generated/training_dataset/#training_dataset_type","text":"[source]","title":"training_dataset_type"},{"location":"generated/training_dataset/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/training_dataset/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/training_dataset/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/training_dataset/#methods","text":"[source]","title":"Methods"},{"location":"generated/training_dataset/#add_tag","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/training_dataset/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/training_dataset/#delete","text":"TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/training_dataset/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/training_dataset/#from_response_json_single","text":"TrainingDataset . from_response_json_single ( json_dict ) [source]","title":"from_response_json_single"},{"location":"generated/training_dataset/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/training_dataset/#get_serving_vector","text":"TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vector"},{"location":"generated/training_dataset/#get_serving_vectors","text":"TrainingDataset . get_serving_vectors ( entry , external = False ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vectors"},{"location":"generated/training_dataset/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/training_dataset/#get_tag","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/training_dataset/#get_tags","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/training_dataset/#init_prepared_statement","text":"TrainingDataset . init_prepared_statement ( batch = None , external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source]","title":"init_prepared_statement"},{"location":"generated/training_dataset/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/training_dataset/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/training_dataset/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/training_dataset/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/training_dataset/#tf_data","text":"TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , deterministic = False , file_pattern = \"*.tfrecord*\" , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . deterministic Optional[bool] : Controls the order in which the transformation produces elements. If set to False, the transformation is allowed to yield elements out of order to trade determinism for performance. Defaults to False . file_pattern Optional[str] : Returns a list of files that match the given pattern Defaults to *.tfrecord* . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source]","title":"tf_data"},{"location":"generated/training_dataset/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/training_dataset/#tfdata-engine","text":"[source]","title":"TFData engine"},{"location":"generated/training_dataset/#tf_record_dataset","text":"TFDataEngine . tf_record_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False , serialized_ndarray_fname = [], ) Reads tfrecord files and returns ParallelMapDataset or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object ParallelMapDataset can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 3 ) td . tf_data ( target_name = \"id\" ) . tf_record_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set True then one hot encode labels, defaults to False . num_classes Optional[int] : If above True then provide number of target classes, defaults to None . process Optional[bool] : If set True api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname Optional[list] : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . ParallelMapDataset . If process is set to False . [source]","title":"tf_record_dataset"},{"location":"generated/training_dataset/#tf_csv_dataset","text":"TFDataEngine . tf_csv_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False ) Reads csv files and returns CsvDatasetV2 or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object CsvDatasetV2 can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 1 ) td . tf_data ( target_name = \"id\" ) . tf_csv_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set true then one hot encode labels, defaults to False . num_classes Optional[int] : If above true then provide number of target classes, defaults to None . process Optional[bool] : If set true api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . CsvDatasetV2 . If process is set to False .","title":"tf_csv_dataset"},{"location":"generated/transformation_functions/","text":"Transformation Functions # HSFS provides functionality to attach transformation functions to training datasets . User defined, custom transformation functions need to be registered in the Feature Store to make them accessible for training dataset creation. To register them in the Feature Store they either have to be part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators Don't decorate transformation functions with Pyspark @udf or @pandas_udf , as well as don't use any Pyspark dependencies. HSFS will decorate transformation function only if it is used inside Pyspark application. Hopsworks also ships Built-in transformation functions such as min_max_scaler , standard_scaler , robust_scaler and label_encoder . They can be registered by calling register_builtin_transformation_functions method on the feature store handle. Examples # Let's assume that we have already installed Python library transformation_fn_template containing transformation function plus_one . Python Register transformation function plus_one in the Hopsworks feature store. from custom_functions import transformations plus_one_meta = fs . create_transformation_function ( transformation_function = transformations . plus_one , output_type = int , version = 1 ) plus_one_meta . save () To retrieve all transformation functions from the feature store use get_transformation_functions which will return list of TransformationFunction objects. A specific transformation function can be retrieved by get_transformation_function method where the user can provide a name and a version of the transformation function. If only function name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 fs . get_transformation_function ( name = \"plus_one\" ) # get built-in transformation function min max scaler fs . get_transformation_function ( name = \"min_max_scaler\" ) # get transformation function by name and version. fs . get_transformation_function ( name = \"plus_one\" , version = 2 ) To attach transformation function to training dataset provide transformation functions as dict, where key is feature name and value is online transformation function name. Also training dataset must be created from the Query object. Once attached transformation function will be applied on whenever save , insert and get_serving_vector methods are called on training dataset object. Python Attaching transformation functions to the training dataset plus_one_meta = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) fs . create_training_dataset ( name = \"td_demo\" , description = \"Dataset to train the demo model\" , data_format = \"csv\" , transformation_functions = { \"feature_name\" : plus_one_meta } statistics_config = None , version = 1 ) td . save ( join_query ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Python Attaching built-in transformation functions to the training dataset min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) fs . create_training_dataset ( name = \"td_demo\" , description = \"Dataset to train the demo model\" , data_format = \"csv\" , transformation_functions = { \"feature_name\" : min_max_scaler , \"feature_name\" : standard_scaler , \"feature_name\" : robust_scaler , \"feature_name\" : label_encoder }, statistics_config = None , version = 1 ) td . save ( join_query ) Scala support Creating and attaching Transformation functions to training datasets are not supported for hsfs scala client. If training dataset with transformation function was created using python client and later insert or getServingVector methods are called on this training dataset from scala client hsfs will throw an exception. Transformation Function # [source] TransformationFunction # hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ) Properties # [source] id # Training dataset id. [source] name # [source] output_type # [source] source_code_content # [source] transformation_fn # [source] transformer_code # [source] version # Methods # [source] delete # TransformationFunction . delete () Delete transformation function from backend. [source] save # TransformationFunction . save () Persist transformation function in backend. Creation # [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. Retrieval # [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"Transformation Functions"},{"location":"generated/transformation_functions/#transformation-functions","text":"HSFS provides functionality to attach transformation functions to training datasets . User defined, custom transformation functions need to be registered in the Feature Store to make them accessible for training dataset creation. To register them in the Feature Store they either have to be part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators Don't decorate transformation functions with Pyspark @udf or @pandas_udf , as well as don't use any Pyspark dependencies. HSFS will decorate transformation function only if it is used inside Pyspark application. Hopsworks also ships Built-in transformation functions such as min_max_scaler , standard_scaler , robust_scaler and label_encoder . They can be registered by calling register_builtin_transformation_functions method on the feature store handle.","title":"Transformation Functions"},{"location":"generated/transformation_functions/#examples","text":"Let's assume that we have already installed Python library transformation_fn_template containing transformation function plus_one . Python Register transformation function plus_one in the Hopsworks feature store. from custom_functions import transformations plus_one_meta = fs . create_transformation_function ( transformation_function = transformations . plus_one , output_type = int , version = 1 ) plus_one_meta . save () To retrieve all transformation functions from the feature store use get_transformation_functions which will return list of TransformationFunction objects. A specific transformation function can be retrieved by get_transformation_function method where the user can provide a name and a version of the transformation function. If only function name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 fs . get_transformation_function ( name = \"plus_one\" ) # get built-in transformation function min max scaler fs . get_transformation_function ( name = \"min_max_scaler\" ) # get transformation function by name and version. fs . get_transformation_function ( name = \"plus_one\" , version = 2 ) To attach transformation function to training dataset provide transformation functions as dict, where key is feature name and value is online transformation function name. Also training dataset must be created from the Query object. Once attached transformation function will be applied on whenever save , insert and get_serving_vector methods are called on training dataset object. Python Attaching transformation functions to the training dataset plus_one_meta = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) fs . create_training_dataset ( name = \"td_demo\" , description = \"Dataset to train the demo model\" , data_format = \"csv\" , transformation_functions = { \"feature_name\" : plus_one_meta } statistics_config = None , version = 1 ) td . save ( join_query ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Python Attaching built-in transformation functions to the training dataset min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) fs . create_training_dataset ( name = \"td_demo\" , description = \"Dataset to train the demo model\" , data_format = \"csv\" , transformation_functions = { \"feature_name\" : min_max_scaler , \"feature_name\" : standard_scaler , \"feature_name\" : robust_scaler , \"feature_name\" : label_encoder }, statistics_config = None , version = 1 ) td . save ( join_query ) Scala support Creating and attaching Transformation functions to training datasets are not supported for hsfs scala client. If training dataset with transformation function was created using python client and later insert or getServingVector methods are called on this training dataset from scala client hsfs will throw an exception.","title":"Examples"},{"location":"generated/transformation_functions/#transformation-function","text":"[source]","title":"Transformation Function"},{"location":"generated/transformation_functions/#transformationfunction","text":"hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , )","title":"TransformationFunction"},{"location":"generated/transformation_functions/#properties","text":"[source]","title":"Properties"},{"location":"generated/transformation_functions/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/transformation_functions/#name","text":"[source]","title":"name"},{"location":"generated/transformation_functions/#output_type","text":"[source]","title":"output_type"},{"location":"generated/transformation_functions/#source_code_content","text":"[source]","title":"source_code_content"},{"location":"generated/transformation_functions/#transformation_fn","text":"[source]","title":"transformation_fn"},{"location":"generated/transformation_functions/#transformer_code","text":"[source]","title":"transformer_code"},{"location":"generated/transformation_functions/#version","text":"","title":"version"},{"location":"generated/transformation_functions/#methods","text":"[source]","title":"Methods"},{"location":"generated/transformation_functions/#delete","text":"TransformationFunction . delete () Delete transformation function from backend. [source]","title":"delete"},{"location":"generated/transformation_functions/#save","text":"TransformationFunction . save () Persist transformation function in backend.","title":"save"},{"location":"generated/transformation_functions/#creation","text":"[source]","title":"Creation"},{"location":"generated/transformation_functions/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object.","title":"create_transformation_function"},{"location":"generated/transformation_functions/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/transformation_functions/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/transformation_functions/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"get_transformation_functions"},{"location":"generated/versioning/","text":"Versioning # The concept of versioning in Hopsworks works on two dimensions: metadata versioning (e.g. schemas) and data versioning. Metadata versioning # Every entity in the Hopsworks feature store has a version number. As an example, every feature group is uniquely identified within the platform based on the feature store (Project) it belongs to, its name and version. The version allows users to identify breaking changes in the schema or computation of features. For example, if a user decides to remove a feature or change the way a feature is computed, that is considered a breaking change and requires a version increase. Increasing the version number will allow existing pipelines and models to keep using the old version of the feature(s) up until the pipeline is adapted to the new feature group version or the model is re-trained. This allow users to progressively rollout and test new features. Creating a new version # When creating a feature group or training dataset using the create_feature_group() or create_training_dataset() methods of the FeatureStore object, it is possible to provide a version number using the version parameter. The version parameter is not mandatory. If not provided and no feature group (or training dataset) with that name exists, then the version is going to be set to 1. If the version parameter is not provided, and a feature group (or training dataset) already exist, then the version number will be increased by one. Appending features to an existing version # For feature groups, it is possible to append new features to existing feature groups. This is not considered a breaking change. To append new features users can either use the UI or the following append_features method: [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. Appended features can also define a default value as placeholder for the feature data that is already present in the feature group. When setting a default value, the value is going to be attached as metadata, the existing data does not need to be rewritten. The default value is going to be used by the query APIs . Retrieving a specific version # When retrieving a feature group from the feature store, the get_feature_group() has an optional version parameter. If the version is not provided, the version defaults to 1. This is done explicitly to guarantee a safe behavior for pipelines and models that use the feature group. Setting the default version to 1 will make sure that, even when the user does not specify the version number, Hopsworks can still guarantee a degree of safety for pipelines and models. This is because, even if new breaking changes are introduced (and so new versions are created), existing pipelines will still use the same version they have been build or trained with. Retrieving all the versions # It is also possible to retrieve the metadata of all the versions of a feature group or training dataset based on its name using the following methods: [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. Data versioning # Data versioning captures the different commits of data that are inserted into a feature group. The data, as it belongs to the same schema version, is homogeneous in terms of schema structure and feature definition. Hopsworks provides data versioning capabilities only for the offline feature groups. The online version of the feature groups only store the most recent values for any given primary key. Data versioning is also not critical for training datasets, which are point in time snapshots of a set of features. Data stored on the offline feature store is stored as Apache Hudi files. Apache Hudi provides the Upsert and Time Travel capabilities that powers the Hopsworks offline feature store. Using Apache Hudi, users on Hopsworks are able to track what data was inserted at which commit. Information regarding the commits made on a feature group, the amount of new rows written, updated and deleted, is available in the Activity UI of a feature group. It is also possible to retrieve the same information programmatically, using the commit_details() method of a feature group object: [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format Users can also use the APIs to read the feature group data at a specific point in time using the as_of method of the query object: [source] as_of # Query . as_of ( wallclock_time ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. Data versioning is critical for reproducibility and debugging. As an example, if a data scientist is debugging why a new model is performing poorly compared to the same model trained six months ago, they can leverage the time travel capabilities of the Hopsworks feature store to build a training dataset with the data as it was six months ago. From there, using statistics and data validation , further debugging can be made to determine what is the root cause of the new model degraded performances.","title":"Versioning"},{"location":"generated/versioning/#versioning","text":"The concept of versioning in Hopsworks works on two dimensions: metadata versioning (e.g. schemas) and data versioning.","title":"Versioning"},{"location":"generated/versioning/#metadata-versioning","text":"Every entity in the Hopsworks feature store has a version number. As an example, every feature group is uniquely identified within the platform based on the feature store (Project) it belongs to, its name and version. The version allows users to identify breaking changes in the schema or computation of features. For example, if a user decides to remove a feature or change the way a feature is computed, that is considered a breaking change and requires a version increase. Increasing the version number will allow existing pipelines and models to keep using the old version of the feature(s) up until the pipeline is adapted to the new feature group version or the model is re-trained. This allow users to progressively rollout and test new features.","title":"Metadata versioning"},{"location":"generated/versioning/#creating-a-new-version","text":"When creating a feature group or training dataset using the create_feature_group() or create_training_dataset() methods of the FeatureStore object, it is possible to provide a version number using the version parameter. The version parameter is not mandatory. If not provided and no feature group (or training dataset) with that name exists, then the version is going to be set to 1. If the version parameter is not provided, and a feature group (or training dataset) already exist, then the version number will be increased by one.","title":"Creating a new version"},{"location":"generated/versioning/#appending-features-to-an-existing-version","text":"For feature groups, it is possible to append new features to existing feature groups. This is not considered a breaking change. To append new features users can either use the UI or the following append_features method: [source]","title":"Appending features to an existing version"},{"location":"generated/versioning/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. Appended features can also define a default value as placeholder for the feature data that is already present in the feature group. When setting a default value, the value is going to be attached as metadata, the existing data does not need to be rewritten. The default value is going to be used by the query APIs .","title":"append_features"},{"location":"generated/versioning/#retrieving-a-specific-version","text":"When retrieving a feature group from the feature store, the get_feature_group() has an optional version parameter. If the version is not provided, the version defaults to 1. This is done explicitly to guarantee a safe behavior for pipelines and models that use the feature group. Setting the default version to 1 will make sure that, even when the user does not specify the version number, Hopsworks can still guarantee a degree of safety for pipelines and models. This is because, even if new breaking changes are introduced (and so new versions are created), existing pipelines will still use the same version they have been build or trained with.","title":"Retrieving a specific version"},{"location":"generated/versioning/#retrieving-all-the-versions","text":"It is also possible to retrieve the metadata of all the versions of a feature group or training dataset based on its name using the following methods: [source]","title":"Retrieving all the versions"},{"location":"generated/versioning/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/versioning/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_datasets"},{"location":"generated/versioning/#data-versioning","text":"Data versioning captures the different commits of data that are inserted into a feature group. The data, as it belongs to the same schema version, is homogeneous in terms of schema structure and feature definition. Hopsworks provides data versioning capabilities only for the offline feature groups. The online version of the feature groups only store the most recent values for any given primary key. Data versioning is also not critical for training datasets, which are point in time snapshots of a set of features. Data stored on the offline feature store is stored as Apache Hudi files. Apache Hudi provides the Upsert and Time Travel capabilities that powers the Hopsworks offline feature store. Using Apache Hudi, users on Hopsworks are able to track what data was inserted at which commit. Information regarding the commits made on a feature group, the amount of new rows written, updated and deleted, is available in the Activity UI of a feature group. It is also possible to retrieve the same information programmatically, using the commit_details() method of a feature group object: [source]","title":"Data versioning"},{"location":"generated/versioning/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format Users can also use the APIs to read the feature group data at a specific point in time using the as_of method of the query object: [source]","title":"commit_details"},{"location":"generated/versioning/#as_of","text":"Query . as_of ( wallclock_time ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. Data versioning is critical for reproducibility and debugging. As an example, if a data scientist is debugging why a new model is performing poorly compared to the same model trained six months ago, they can leverage the time travel capabilities of the Hopsworks feature store to build a training dataset with the data as it was six months ago. From there, using statistics and data validation , further debugging can be made to determine what is the root cause of the new model degraded performances.","title":"as_of"},{"location":"generated/api/connection_api/","text":"Connection # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] cert_folder # [source] host # [source] hostname_verification # [source] port # [source] project # [source] region_name # [source] secrets_store # [source] trust_store_path # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"Connection"},{"location":"generated/api/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/api/connection_api/#connection_1","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"python\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"/tmp\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/api/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/api/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/api/connection_api/#cert_folder","text":"[source]","title":"cert_folder"},{"location":"generated/api/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/api/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/api/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/api/connection_api/#project","text":"[source]","title":"project"},{"location":"generated/api/connection_api/#region_name","text":"[source]","title":"region_name"},{"location":"generated/api/connection_api/#secrets_store","text":"[source]","title":"secrets_store"},{"location":"generated/api/connection_api/#trust_store_path","text":"","title":"trust_store_path"},{"location":"generated/api/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/api/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/api/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source]","title":"connection"},{"location":"generated/api/connection_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/api/connection_api/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rule"},{"location":"generated/api/connection_api/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/api/connection_api/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"/tmp\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"setup_databricks"},{"location":"generated/api/expectation_api/","text":"Expectation # [source] Expectation # hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store. Properties # [source] description # Description of the expectation. [source] features # Optional list of features this expectation is applied to. If no features are provided, the expectation will be applied to all the feature group features. [source] name # Name of the expectation, unique per feature store (project). [source] rules # List of rules applied to the features of the expectation. Methods # [source] save # Expectation . save () Persist the expectation metadata object to the feature store. Creation # [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object. Retrieval # [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store.","title":"Expectation"},{"location":"generated/api/expectation_api/#expectation","text":"[source]","title":"Expectation"},{"location":"generated/api/expectation_api/#expectation_1","text":"hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store.","title":"Expectation"},{"location":"generated/api/expectation_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/expectation_api/#description","text":"Description of the expectation. [source]","title":"description"},{"location":"generated/api/expectation_api/#features","text":"Optional list of features this expectation is applied to. If no features are provided, the expectation will be applied to all the feature group features. [source]","title":"features"},{"location":"generated/api/expectation_api/#name","text":"Name of the expectation, unique per feature store (project). [source]","title":"name"},{"location":"generated/api/expectation_api/#rules","text":"List of rules applied to the features of the expectation.","title":"rules"},{"location":"generated/api/expectation_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/expectation_api/#save","text":"Expectation . save () Persist the expectation metadata object to the feature store.","title":"save"},{"location":"generated/api/expectation_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/expectation_api/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object.","title":"create_expectation"},{"location":"generated/api/expectation_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/expectation_api/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source]","title":"get_expectations"},{"location":"generated/api/expectation_api/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store.","title":"get_expectation"},{"location":"generated/api/feature_api/","text":"Feature # [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] description # Description of the feature. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] contains # Feature . contains ( other ) [source] from_response_json # Feature . from_response_json ( json_dict ) [source] is_complex # Feature . is_complex () Returns true if the feature has a complex type. [source] json # Feature . json () [source] to_dict # Feature . to_dict ()","title":"Feature"},{"location":"generated/api/feature_api/#feature","text":"[source]","title":"Feature"},{"location":"generated/api/feature_api/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/api/feature_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_api/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/api/feature_api/#description","text":"Description of the feature. [source]","title":"description"},{"location":"generated/api/feature_api/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_api/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/api/feature_api/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/api/feature_api/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/api/feature_api/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/api/feature_api/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/api/feature_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_api/#contains","text":"Feature . contains ( other ) [source]","title":"contains"},{"location":"generated/api/feature_api/#from_response_json","text":"Feature . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_api/#is_complex","text":"Feature . is_complex () Returns true if the feature has a complex type. [source]","title":"is_complex"},{"location":"generated/api/feature_api/#json","text":"Feature . json () [source]","title":"json"},{"location":"generated/api/feature_api/#to_dict","text":"Feature . to_dict ()","title":"to_dict"},{"location":"generated/api/feature_group_api/","text":"FeatureGroup # [source] FeatureGroup # hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , validation_type = \"NONE\" , expectations = None , online_topic_name = None , event_time = None , stream = False , ) Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] event_time # Event time feature in the feature group. [source] expectations_names # The names of expectations attached to this feature group. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] stream # whether real time stream writing capabilities are supported or not [source] time_travel_format # Setting of the feature group time travel format. [source] validation_type # Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source] as_of # FeatureGroup . as_of ( wallclock_time ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source] attach_expectation # FeatureGroup . attach_expectation ( expectation ) Attach a feature group expectation. If feature group validation is not already enabled, it will be enabled and set to the stricter setting. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source] compute_statistics # FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] detach_expectation # FeatureGroup . detach_expectation ( expectation ) Remove an expectation from a feature group. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # FeatureGroup . from_response_json ( json_dict ) [source] get_complex_features # FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source] get_expectation # FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source] get_expectations # FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns FeatureGroup . Updated feature group metadata object. [source] insert_stream # FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source] json # FeatureGroup . json () [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source] save # FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] to_dict # FeatureGroup . to_dict () [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_feature_description # FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source] update_features # FeatureGroup . update_features ( features ) Update a single feature in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # FeatureGroup . update_from_response_json ( json_dict ) [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup","text":"[source]","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup_1","text":"hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , validation_type = \"NONE\" , expectations = None , online_topic_name = None , event_time = None , stream = False , )","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_group_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object.","title":"create_feature_group"},{"location":"generated/api/feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_group_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/api/feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_group_api/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/api/feature_group_api/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/api/feature_group_api/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/api/feature_group_api/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/api/feature_group_api/#event_time","text":"Event time feature in the feature group. [source]","title":"event_time"},{"location":"generated/api/feature_group_api/#expectations_names","text":"The names of expectations attached to this feature group. [source]","title":"expectations_names"},{"location":"generated/api/feature_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/feature_group_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/feature_group_api/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_group_api/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/api/feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/feature_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/feature_group_api/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/api/feature_group_api/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/api/feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/feature_group_api/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/api/feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/api/feature_group_api/#stream","text":"whether real time stream writing capabilities are supported or not [source]","title":"stream"},{"location":"generated/api/feature_group_api/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/api/feature_group_api/#validation_type","text":"Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source]","title":"validation_type"},{"location":"generated/api/feature_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_group_api/#add_tag","text":"FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/feature_group_api/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. Safe append This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup . The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/feature_group_api/#as_of","text":"FeatureGroup . as_of ( wallclock_time ) Get Query object to retrieve all features of the group at a point in the past. This method selects all features in the feature group and returns a Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/api/feature_group_api/#attach_expectation","text":"FeatureGroup . attach_expectation ( expectation ) Attach a feature group expectation. If feature group validation is not already enabled, it will be enabled and set to the stricter setting. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"attach_expectation"},{"location":"generated/api/feature_group_api/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/api/feature_group_api/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"commit_details"},{"location":"generated/api/feature_group_api/#compute_statistics","text":"FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/api/feature_group_api/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_group_api/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/feature_group_api/#detach_expectation","text":"FeatureGroup . detach_expectation ( expectation ) Remove an expectation from a feature group. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"detach_expectation"},{"location":"generated/api/feature_group_api/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/feature_group_api/#from_response_json","text":"FeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_group_api/#get_complex_features","text":"FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source]","title":"get_complex_features"},{"location":"generated/api/feature_group_api/#get_expectation","text":"FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"get_expectation"},{"location":"generated/api/feature_group_api/#get_expectations","text":"FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source]","title":"get_expectations"},{"location":"generated/api/feature_group_api/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/api/feature_group_api/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/api/feature_group_api/#get_tag","text":"FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/feature_group_api/#get_tags","text":"FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/feature_group_api/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source]","title":"get_validations"},{"location":"generated/api/feature_group_api/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/api/feature_group_api/#insert_stream","text":"FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , checkpoint_dir = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to False . timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . checkpoint_dir Optional[str] : Checkpoint directory location. This will be used to as a reference to from where to resume the streaming job. If None then hsfs will construct as \"insert_stream_\" + online_topic_name. Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source]","title":"insert_stream"},{"location":"generated/api/feature_group_api/#json","text":"FeatureGroup . json () [source]","title":"json"},{"location":"generated/api/feature_group_api/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/api/feature_group_api/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"read_changes"},{"location":"generated/api/feature_group_api/#save","text":"FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key start_offline_backfill and value True or False to configure whether or not to start the backfill job to write data to the offline storage. By default the backfill job gets started immediately. key internal_kafka and value True or False in case you established connectivity from you Python environment to the internal advertised listeners of the Hopsworks Kafka Cluster. Defaults to False and will use external listeners when connecting from outside of Hopsworks. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/api/feature_group_api/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/feature_group_api/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/feature_group_api/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be excluded from the selection. Defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/feature_group_api/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/api/feature_group_api/#to_dict","text":"FeatureGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_group_api/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature group. Safe update This method updates the feature group description safely. In case of failure your local metadata object will keep the old description. Arguments description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/feature_group_api/#update_feature_description","text":"FeatureGroup . update_feature_description ( feature_name , description ) Update the description of a single feature in this feature group. Safe update This method updates the feature description safely. In case of failure your local metadata object will keep the old description. Arguments feature_name str : Name of the feature to be updated. description str : New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_feature_description"},{"location":"generated/api/feature_group_api/#update_features","text":"FeatureGroup . update_features ( features ) Update a single feature in this feature group. Currently it's only supported to update the description of a feature. Unsafe update Note that if you use an existing Feature object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails. Arguments features Union[hsfs.feature.Feature, List[hsfs.feature.Feature]] : Feature or list of features. A feature object or list thereof to be updated. Returns FeatureGroup . The updated feature group object. [source]","title":"update_features"},{"location":"generated/api/feature_group_api/#update_from_response_json","text":"FeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/feature_group_api/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/api/feature_group_api/#validate","text":"FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/api/feature_store_api/","text":"Feature Store # [source] FeatureStore # hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , ) Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object. [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. [source] create_feature_view # FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , label = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source] delete_expectation # FeatureStore . delete_expectation ( name ) Delete an expectation from the feature store. Arguments name str : Name of the training dataset to create. [source] from_response_json # FeatureStore . from_response_json ( json_dict ) [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_view # FeatureStore . get_feature_view ( name , version ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. version : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_feature_views # FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_groups # FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns DataFrame : DataFrame depending on the chosen type.","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#feature-store","text":"[source]","title":"Feature Store"},{"location":"generated/api/feature_store_api/#featurestore","text":"hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , )","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_store_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/api/feature_store_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_store_api/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/api/feature_store_api/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/api/feature_store_api/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/api/feature_store_api/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/api/feature_store_api/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/api/feature_store_api/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/api/feature_store_api/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/api/feature_store_api/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/api/feature_store_api/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/api/feature_store_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_store_api/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the expectation to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the features. Returns: Expectation : The expectation metadata object. [source]","title":"create_expectation"},{"location":"generated/api/feature_store_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], event_time = None , stream = False , ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . stream Optional[bool] : Optionally, Define whether the feature group should support real time stream writing capabilities. Stream enabled Feature Groups have unified single API for writing streaming features transparently to both online and offline store. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_store_api/#create_feature_view","text":"FeatureStore . create_feature_view ( name , query , version = None , description = \"\" , label = [], transformation_functions = {} ) Create a feature view metadata object and saved it to Hopsworks. Arguments name str : Name of the feature view to create. query hsfs.constructor.query.Query : Feature store Query . version Optional[int] : Version of the feature view to create, defaults to None and will create the feature view with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature view to improve discoverability for Data Scientists, defaults to empty string \"\" . label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the feature view. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the vector and at inference time. Defaults to {} , no transformations. Returns: FeatureView : The feature view metadata object. [source]","title":"create_feature_view"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , primary_key = [], features = [], statistics_config = None , event_time = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the feature group won't have any primary key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation, \"histograms\" to compute feature value frequencies and \"exact_uniqueness\" to compute uniqueness, distinctness and entropy. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. event_time Optional[str] : Optionally, provide the name of the feature containing the event time for the features in this feature group. If event_time is set the feature group can be used for point-in-time joins. Defaults to None . validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/api/feature_store_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"create_transformation_function"},{"location":"generated/api/feature_store_api/#delete_expectation","text":"FeatureStore . delete_expectation ( name ) Delete an expectation from the feature store. Arguments name str : Name of the training dataset to create. [source]","title":"delete_expectation"},{"location":"generated/api/feature_store_api/#from_response_json","text":"FeatureStore . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_store_api/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source]","title":"get_expectation"},{"location":"generated/api/feature_store_api/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source]","title":"get_expectations"},{"location":"generated/api/feature_store_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/api/feature_store_api/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/api/feature_store_api/#get_feature_view","text":"FeatureStore . get_feature_view ( name , version ) Get a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. version : Version of the feature view to retrieve, defaults to None and will return the version=1 . Returns FeatureView : The feature view metadata object. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_view"},{"location":"generated/api/feature_store_api/#get_feature_views","text":"FeatureStore . get_feature_views ( name ) Get a list of all versions of a feature view entity from the feature store. Getting a feature view from the Feature Store means getting its metadata. Arguments name : Name of the feature view to get. Returns FeatureView : List of feature view metadata objects. Raises RestAPIError : If unable to retrieve feature view from the feature store. [source]","title":"get_feature_views"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_groups","text":"FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_groups"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/api/feature_store_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/feature_store_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/api/feature_store_api/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_datasets"},{"location":"generated/api/feature_store_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/feature_store_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source]","title":"get_transformation_functions"},{"location":"generated/api/feature_store_api/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Arguments query str : The SQL query to execute. dataframe_type Optional[str] : The type of the returned dataframe. Defaults to \"default\". which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online Optional[bool] : Set to true to execute the query against the online feature store. Defaults to False. read_options Optional[dict] : Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns DataFrame : DataFrame depending on the chosen type.","title":"sql"},{"location":"generated/api/job_configuration/","text":"[source] JobConfiguration # hsfs . core . job_configuration . JobConfiguration ( am_memory = 1024 , am_cores = 1 , executor_memory = 2048 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , )","title":"Job configuration"},{"location":"generated/api/job_configuration/#jobconfiguration","text":"hsfs . core . job_configuration . JobConfiguration ( am_memory = 1024 , am_cores = 1 , executor_memory = 2048 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , )","title":"JobConfiguration"},{"location":"generated/api/query_api/","text":"Query # Query objects are strictly generated by HSFS APIs called on Feature Group objects . Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here. Methods # [source] as_of # Query . as_of ( wallclock_time ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source] filter # Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_cache_feature_group_only # Query . from_cache_feature_group_only () [source] from_response_json # Query . from_response_json ( json_dict ) [source] join # Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source] pull_changes # Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source] read # Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source] show # Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source] to_string # Query . to_string ( online = False ) Properties # [source] left_feature_group_end_time # [source] left_feature_group_start_time #","title":"Query"},{"location":"generated/api/query_api/#query","text":"Query objects are strictly generated by HSFS APIs called on Feature Group objects . Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here.","title":"Query"},{"location":"generated/api/query_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/query_api/#as_of","text":"Query . as_of ( wallclock_time ) Perform time travel on the given Query. This method returns a new Query object at the specified point in time. This can then either be read into a Dataframe or used further to perform joins or construct a training dataset. Arguments wallclock_time : Datetime string. The String should be formatted in one of the following formats %Y%m%d , %Y%m%d%H , %Y%m%d%H%M , or %Y%m%d%H%M%S . Returns Query . The query object with the applied time travel condition. [source]","title":"as_of"},{"location":"generated/api/query_api/#filter","text":"Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/query_api/#from_cache_feature_group_only","text":"Query . from_cache_feature_group_only () [source]","title":"from_cache_feature_group_only"},{"location":"generated/api/query_api/#from_response_json","text":"Query . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/query_api/#join","text":"Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source]","title":"join"},{"location":"generated/api/query_api/#pull_changes","text":"Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source]","title":"pull_changes"},{"location":"generated/api/query_api/#read","text":"Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source]","title":"read"},{"location":"generated/api/query_api/#show","text":"Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source]","title":"show"},{"location":"generated/api/query_api/#to_string","text":"Query . to_string ( online = False )","title":"to_string"},{"location":"generated/api/query_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/query_api/#left_feature_group_end_time","text":"[source]","title":"left_feature_group_end_time"},{"location":"generated/api/query_api/#left_feature_group_start_time","text":"","title":"left_feature_group_start_time"},{"location":"generated/api/rule_api/","text":"Rule # [source] Rule # hsfs . rule . Rule ( name , level , min = None , max = None , pattern = None , accepted_type = None , feature = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only. Properties # [source] accepted_type # Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source] feature # Feature to compare the expectation's features to, applied only to Compliance rules. [source] legal_values # List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source] level # Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source] max # The upper bound of the value range this feature should fall into. [source] min # The lower bound of the value range this feature should fall into. [source] name # Name of the rule as found in rule definitions. [source] pattern # Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule.","title":"Rule"},{"location":"generated/api/rule_api/#rule","text":"[source]","title":"Rule"},{"location":"generated/api/rule_api/#rule_1","text":"hsfs . rule . Rule ( name , level , min = None , max = None , pattern = None , accepted_type = None , feature = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only.","title":"Rule"},{"location":"generated/api/rule_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/rule_api/#accepted_type","text":"Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source]","title":"accepted_type"},{"location":"generated/api/rule_api/#feature","text":"Feature to compare the expectation's features to, applied only to Compliance rules. [source]","title":"feature"},{"location":"generated/api/rule_api/#legal_values","text":"List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source]","title":"legal_values"},{"location":"generated/api/rule_api/#level","text":"Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source]","title":"level"},{"location":"generated/api/rule_api/#max","text":"The upper bound of the value range this feature should fall into. [source]","title":"max"},{"location":"generated/api/rule_api/#min","text":"The lower bound of the value range this feature should fall into. [source]","title":"min"},{"location":"generated/api/rule_api/#name","text":"Name of the rule as found in rule definitions. [source]","title":"name"},{"location":"generated/api/rule_api/#pattern","text":"Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule.","title":"pattern"},{"location":"generated/api/rule_definition_api/","text":"Rule Definition # [source] RuleDefinition # hsfs . ruledefinition . RuleDefinition ( name , accepted_type , predicate = None , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified. Properties # [source] accepted_type # The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source] description # [source] feature_type # The type of the feature, one of \"Numerical\", \"Categorical\". [source] name # Name of the rule definition. Unique across all features stores. [source] predicate # Predicate of the rule definition, one of \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\". Retrieval # [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation.","title":"Rule Definiton"},{"location":"generated/api/rule_definition_api/#rule-definition","text":"[source]","title":"Rule Definition"},{"location":"generated/api/rule_definition_api/#ruledefinition","text":"hsfs . ruledefinition . RuleDefinition ( name , accepted_type , predicate = None , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified.","title":"RuleDefinition"},{"location":"generated/api/rule_definition_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/rule_definition_api/#accepted_type","text":"The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source]","title":"accepted_type"},{"location":"generated/api/rule_definition_api/#description","text":"[source]","title":"description"},{"location":"generated/api/rule_definition_api/#feature_type","text":"The type of the feature, one of \"Numerical\", \"Categorical\". [source]","title":"feature_type"},{"location":"generated/api/rule_definition_api/#name","text":"Name of the rule definition. Unique across all features stores. [source]","title":"name"},{"location":"generated/api/rule_definition_api/#predicate","text":"Predicate of the rule definition, one of \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\".","title":"predicate"},{"location":"generated/api/rule_definition_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/rule_definition_api/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/api/rule_definition_api/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation.","title":"get_rule"},{"location":"generated/api/statistics_config_api/","text":"StatisticsConfig # [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [] ) Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] exact_uniqueness # Enable exact uniqueness as an additional statistic to be computed for each feature. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig","text":"[source]","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , exact_uniqueness = False , columns = [] )","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/statistics_config_api/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/api/statistics_config_api/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/api/statistics_config_api/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/api/statistics_config_api/#exact_uniqueness","text":"Enable exact uniqueness as an additional statistic to be computed for each feature. [source]","title":"exact_uniqueness"},{"location":"generated/api/statistics_config_api/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/api/storage_connector_api/","text":"Storage Connector # Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. HopsFS # Properties # [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # HopsFSConnector . refetch () Refetch storage connector. [source] spark_options # HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # HopsFSConnector . to_dict () [source] update_from_response_json # HopsFSConnector . update_from_response_json ( json_dict ) JDBC # Properties # [source] arguments # Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source] connection_string # JDBC connection string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # JdbcConnector . refetch () Refetch storage connector. [source] spark_options # JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # JdbcConnector . to_dict () [source] update_from_response_json # JdbcConnector . update_from_response_json ( json_dict ) S3 # Properties # [source] access_key # Access key. [source] bucket # Return the bucket for S3 connectors. [source] description # User provided description of the storage connector. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] session_token # Session token. Methods # [source] prepare_spark # S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # S3Connector . refetch () Refetch storage connector. [source] spark_options # S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # S3Connector . to_dict () [source] update_from_response_json # S3Connector . update_from_response_json ( json_dict ) Redshift # Properties # [source] arguments # Additional JDBC, REDSHIFT, or Snowflake arguments. [source] auto_create # Database username for redshift cluster. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] description # User provided description of the storage connector. [source] expiration # Cluster temporary credential expiration time. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] table_name # Table name for redshift cluster. Methods # [source] read # RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # RedshiftConnector . to_dict () [source] update_from_response_json # RedshiftConnector . update_from_response_json ( json_dict ) Azure Data Lake Storage # Properties # [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] container_name # Container name of the ADLS storage connector [source] description # User provided description of the storage connector. [source] directory_id # Directory ID of the ADLS storage connector [source] generation # Generation of the ADLS storage connector [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. ADLS) - return the path of the connector [source] service_credential # Service credential of the ADLS storage connector Methods # [source] prepare_spark # AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # AdlsConnector . refetch () Refetch storage connector. [source] spark_options # AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # AdlsConnector . to_dict () [source] update_from_response_json # AdlsConnector . update_from_response_json ( json_dict ) Snowflake # Properties # [source] account # Account of the Snowflake storage connector [source] application # Application of the Snowflake storage connector [source] database # Database of the Snowflake storage connector [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Additional options for the Snowflake storage connector [source] password # Password of the Snowflake storage connector [source] role # Role of the Snowflake storage connector [source] schema # Schema of the Snowflake storage connector [source] table # Table of the Snowflake storage connector [source] token # OAuth token of the Snowflake storage connector [source] url # URL of the Snowflake storage connector [source] user # User of the Snowflake storage connector [source] warehouse # Warehouse of the Snowflake storage connector Methods # [source] read # SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # SnowflakeConnector . refetch () Refetch storage connector. [source] snowflake_connector_options # SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source] spark_options # SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # SnowflakeConnector . to_dict () [source] update_from_response_json # SnowflakeConnector . update_from_response_json ( json_dict ) Google Cloud Storage # This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Properties # [source] algorithm # Encryption Algorithm [source] bucket # GCS Bucket [source] description # User provided description of the storage connector. [source] encryption_key # Encryption Key [source] encryption_key_hash # Encryption Key Hash [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] name # Name of the storage connector. [source] path # the path of the connector along with gs file system prefixed Methods # [source] prepare_spark # GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source] read # GcsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads GCS path into a dataframe using the storage connector. conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # GcsConnector . refetch () Refetch storage connector. [source] spark_options # GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # GcsConnector . to_dict () [source] update_from_response_json # GcsConnector . update_from_response_json ( json_dict ) BigQuery # The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Properties # [source] arguments # Additional spark options [source] dataset # BigQuery dataset (The dataset containing the table) [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] key_path # JSON keyfile for service account [source] materialization_dataset # BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source] name # Name of the storage connector. [source] parent_project # BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source] query_project # BigQuery project (The Google Cloud Project ID of the table) [source] query_table # BigQuery table name Methods # [source] read # BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source] refetch # BigQueryConnector . refetch () Refetch storage connector. [source] spark_options # BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source] to_dict # BigQueryConnector . to_dict () [source] update_from_response_json # BigQueryConnector . update_from_response_json ( json_dict )","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#storage-connector","text":"","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/storage_connector_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/api/storage_connector_api/#hopsfs","text":"","title":"HopsFS"},{"location":"generated/api/storage_connector_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#description","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read","text":"HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch","text":"HopsFSConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options","text":"HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict","text":"HopsFSConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json","text":"HopsFSConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#jdbc","text":"","title":"JDBC"},{"location":"generated/api/storage_connector_api/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments","text":"Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/api/storage_connector_api/#description_1","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_1","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_1","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods_1","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_1","text":"JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_1","text":"JdbcConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_1","text":"JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_1","text":"JdbcConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_1","text":"JdbcConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#s3","text":"","title":"S3"},{"location":"generated/api/storage_connector_api/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/api/storage_connector_api/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_2","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_2","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_2","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/api/storage_connector_api/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/api/storage_connector_api/#session_token","text":"Session token.","title":"session_token"},{"location":"generated/api/storage_connector_api/#methods_2","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark","text":"S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_2","text":"S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_2","text":"S3Connector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_2","text":"S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_2","text":"S3Connector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_2","text":"S3Connector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#redshift","text":"","title":"Redshift"},{"location":"generated/api/storage_connector_api/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_1","text":"Additional JDBC, REDSHIFT, or Snowflake arguments. [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/api/storage_connector_api/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/api/storage_connector_api/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/api/storage_connector_api/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/api/storage_connector_api/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/api/storage_connector_api/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/api/storage_connector_api/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/api/storage_connector_api/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/api/storage_connector_api/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/api/storage_connector_api/#description_3","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/api/storage_connector_api/#iam_role_1","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_3","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_3","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/api/storage_connector_api/#methods_3","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_3","text":"RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_3","text":"RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_3","text":"RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_3","text":"RedshiftConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_3","text":"RedshiftConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#azure-data-lake-storage","text":"","title":"Azure Data Lake Storage"},{"location":"generated/api/storage_connector_api/#properties_4","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/api/storage_connector_api/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/api/storage_connector_api/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/api/storage_connector_api/#description_4","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/api/storage_connector_api/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/api/storage_connector_api/#id_4","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_4","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_1","text":"If the connector refers to a path (e.g. ADLS) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#service_credential","text":"Service credential of the ADLS storage connector","title":"service_credential"},{"location":"generated/api/storage_connector_api/#methods_4","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark_1","text":"AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_4","text":"AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_4","text":"AdlsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_4","text":"AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_4","text":"AdlsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_4","text":"AdlsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#snowflake","text":"","title":"Snowflake"},{"location":"generated/api/storage_connector_api/#properties_5","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account","text":"Account of the Snowflake storage connector [source]","title":"account"},{"location":"generated/api/storage_connector_api/#application","text":"Application of the Snowflake storage connector [source]","title":"application"},{"location":"generated/api/storage_connector_api/#database","text":"Database of the Snowflake storage connector [source]","title":"database"},{"location":"generated/api/storage_connector_api/#description_5","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_5","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_5","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#options","text":"Additional options for the Snowflake storage connector [source]","title":"options"},{"location":"generated/api/storage_connector_api/#password","text":"Password of the Snowflake storage connector [source]","title":"password"},{"location":"generated/api/storage_connector_api/#role","text":"Role of the Snowflake storage connector [source]","title":"role"},{"location":"generated/api/storage_connector_api/#schema","text":"Schema of the Snowflake storage connector [source]","title":"schema"},{"location":"generated/api/storage_connector_api/#table","text":"Table of the Snowflake storage connector [source]","title":"table"},{"location":"generated/api/storage_connector_api/#token","text":"OAuth token of the Snowflake storage connector [source]","title":"token"},{"location":"generated/api/storage_connector_api/#url","text":"URL of the Snowflake storage connector [source]","title":"url"},{"location":"generated/api/storage_connector_api/#user","text":"User of the Snowflake storage connector [source]","title":"user"},{"location":"generated/api/storage_connector_api/#warehouse","text":"Warehouse of the Snowflake storage connector","title":"warehouse"},{"location":"generated/api/storage_connector_api/#methods_5","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_5","text":"SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_5","text":"SnowflakeConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#snowflake_connector_options","text":"SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source]","title":"snowflake_connector_options"},{"location":"generated/api/storage_connector_api/#spark_options_5","text":"SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_5","text":"SnowflakeConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_5","text":"SnowflakeConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#google-cloud-storage","text":"This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The connector also supports the optional encryption method Customer Supplied Encryption Key by Google. The encryption details are stored as Secrets in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop","title":"Google Cloud Storage"},{"location":"generated/api/storage_connector_api/#properties_6","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#algorithm","text":"Encryption Algorithm [source]","title":"algorithm"},{"location":"generated/api/storage_connector_api/#bucket_1","text":"GCS Bucket [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_6","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#encryption_key","text":"Encryption Key [source]","title":"encryption_key"},{"location":"generated/api/storage_connector_api/#encryption_key_hash","text":"Encryption Key Hash [source]","title":"encryption_key_hash"},{"location":"generated/api/storage_connector_api/#id_6","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#key_path","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/api/storage_connector_api/#name_6","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_2","text":"the path of the connector along with gs file system prefixed","title":"path"},{"location":"generated/api/storage_connector_api/#methods_6","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark_2","text":"GcsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"gs://bucket/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"gs://bucket/path\" )) Arguments path Optional[str] : Path to prepare for reading from Google cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_6","text":"GcsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads GCS path into a dataframe using the storage connector. conn . read ( data_format = 'spark_formats' , path = 'gs://BUCKET/DATA' ) Arguments data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : GCS path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_6","text":"GcsConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_6","text":"GcsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_6","text":"GcsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_6","text":"GcsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#bigquery","text":"The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the read API. Authentication to GCP is handled by uploading the JSON keyfile for service account to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.","title":"BigQuery"},{"location":"generated/api/storage_connector_api/#properties_7","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_2","text":"Additional spark options [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#dataset","text":"BigQuery dataset (The dataset containing the table) [source]","title":"dataset"},{"location":"generated/api/storage_connector_api/#description_7","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_7","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#key_path_1","text":"JSON keyfile for service account [source]","title":"key_path"},{"location":"generated/api/storage_connector_api/#materialization_dataset","text":"BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query) [source]","title":"materialization_dataset"},{"location":"generated/api/storage_connector_api/#name_7","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#parent_project","text":"BigQuery parent project (Google Cloud Project ID of the table to bill for the export) [source]","title":"parent_project"},{"location":"generated/api/storage_connector_api/#query_project","text":"BigQuery project (The Google Cloud Project ID of the table) [source]","title":"query_project"},{"location":"generated/api/storage_connector_api/#query_table","text":"BigQuery table name","title":"query_table"},{"location":"generated/api/storage_connector_api/#methods_7","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_7","text":"BigQueryConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads results from BigQuery into a spark dataframe using the storage connector. Reading from bigquery is done via either specifying the BigQuery table or BigQuery query. For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector and read directly from the corresponding path. conn . read () OR, to read results from a BigQuery query, set Materialization Dataset on storage connector, and pass your SQL to query argument. conn . read ( query = 'SQL' ) Optionally, passing query argument will take priority at runtime if the table options were also set on the storage connector. This allows user to run from both a query or table with same connector, assuming all fields were set. Also, user can set the path argument to a bigquery table path to read at runtime, if table options were not set initially while creating the connector. conn . read ( path = 'project.dataset.table' ) Arguments query Optional[str] : BigQuery query. Defaults to None . data_format Optional[str] : Spark data format. Defaults to None . options dict : Spark options. Defaults to None . path Optional[str] : BigQuery table path. Defaults to None . Raises ValueError : Malformed arguments. Returns Dataframe : A Spark dataframe. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_7","text":"BigQueryConnector . refetch () Refetch storage connector. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_7","text":"BigQueryConnector . spark_options () Return spark options to be set for BigQuery spark connector [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_7","text":"BigQueryConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_7","text":"BigQueryConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/","text":"Training Dataset # [source] TrainingDataset # hsfs . training_dataset . TrainingDataset ( name , version , data_format , featurestore_id , location = \"\" , event_start_time = None , event_end_time = None , coalesce = False , description = None , storage_connector = None , splits = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , train_split = None , ) Creation # [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] coalesce # If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source] data_format # File format of the training dataset. [source] description # [source] event_end_time # [source] event_start_time # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] train_split # Set name of training dataset split that is used for training. [source] training_dataset_type # [source] transformation_functions # Set transformation functions. [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete # TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] from_response_json # TrainingDataset . from_response_json ( json_dict ) [source] from_response_json_single # TrainingDataset . from_response_json_single ( json_dict ) [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_serving_vector # TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_serving_vectors # TrainingDataset . get_serving_vectors ( entry , external = False ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] init_prepared_statement # TrainingDataset . init_prepared_statement ( batch = None , external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] json # TrainingDataset . json () [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] tf_data # TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , deterministic = False , file_pattern = \"*.tfrecord*\" , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . deterministic Optional[bool] : Controls the order in which the transformation produces elements. If set to False, the transformation is allowed to yield elements out of order to trade determinism for performance. Defaults to False . file_pattern Optional[str] : Returns a list of files that match the given pattern Defaults to *.tfrecord* . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source] to_dict # TrainingDataset . to_dict () [source] update_from_response_json # TrainingDataset . update_from_response_json ( json_dict ) [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#training-dataset","text":"[source]","title":"Training Dataset"},{"location":"generated/api/training_dataset_api/#trainingdataset","text":"hsfs . training_dataset . TrainingDataset ( name , version , data_format , featurestore_id , location = \"\" , event_start_time = None , event_end_time = None , coalesce = False , description = None , storage_connector = None , splits = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , train_split = None , )","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/training_dataset_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, train_split = None , ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. transformation_functions Optional[Dict[str, hsfs.transformation_function.TransformationFunction]] : A dictionary mapping tansformation functions to to the features they should be applied to before writing out the training data and at inference time. Defaults to {} , no transformations. train_split Optional[str] : If splits is set, provide the name of the split that is going to be used for training. The statistics of this split will be used for transformation functions if necessary. Defaults to None . Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/api/training_dataset_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/training_dataset_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/api/training_dataset_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/training_dataset_api/#coalesce","text":"If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source]","title":"coalesce"},{"location":"generated/api/training_dataset_api/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/api/training_dataset_api/#description","text":"[source]","title":"description"},{"location":"generated/api/training_dataset_api/#event_end_time","text":"[source]","title":"event_end_time"},{"location":"generated/api/training_dataset_api/#event_start_time","text":"[source]","title":"event_start_time"},{"location":"generated/api/training_dataset_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/training_dataset_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/training_dataset_api/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/api/training_dataset_api/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/api/training_dataset_api/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/api/training_dataset_api/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/api/training_dataset_api/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/api/training_dataset_api/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/api/training_dataset_api/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/api/training_dataset_api/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/api/training_dataset_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/api/training_dataset_api/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/api/training_dataset_api/#train_split","text":"Set name of training dataset split that is used for training. [source]","title":"train_split"},{"location":"generated/api/training_dataset_api/#training_dataset_type","text":"[source]","title":"training_dataset_type"},{"location":"generated/api/training_dataset_api/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/api/training_dataset_api/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/api/training_dataset_api/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/api/training_dataset_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/training_dataset_api/#add_tag","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/training_dataset_api/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/api/training_dataset_api/#delete","text":"TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/training_dataset_api/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/training_dataset_api/#from_response_json","text":"TrainingDataset . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/training_dataset_api/#from_response_json_single","text":"TrainingDataset . from_response_json_single ( json_dict ) [source]","title":"from_response_json_single"},{"location":"generated/api/training_dataset_api/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/api/training_dataset_api/#get_serving_vector","text":"TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vector"},{"location":"generated/api/training_dataset_api/#get_serving_vectors","text":"TrainingDataset . get_serving_vectors ( entry , external = False ) Returns assembled serving vectors in batches from online feature store. Arguments entry Dict[str, List[Any]] : dict of feature group primary key names as keys and value as list of primary keys provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns List[list] List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vectors"},{"location":"generated/api/training_dataset_api/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/api/training_dataset_api/#get_tag","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/training_dataset_api/#get_tags","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/training_dataset_api/#init_prepared_statement","text":"TrainingDataset . init_prepared_statement ( batch = None , external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments batch Optional[bool] : boolean, optional. If set to True, prepared statements will be initialised for retrieving serving vectors as a batch. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source]","title":"init_prepared_statement"},{"location":"generated/api/training_dataset_api/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/api/training_dataset_api/#json","text":"TrainingDataset . json () [source]","title":"json"},{"location":"generated/api/training_dataset_api/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. If the training dataset has split, the split parameter is mandatory. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/api/training_dataset_api/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the Query . Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the python engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the python engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/api/training_dataset_api/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/api/training_dataset_api/#tf_data","text":"TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , deterministic = False , file_pattern = \"*.tfrecord*\" , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . deterministic Optional[bool] : Controls the order in which the transformation produces elements. If set to False, the transformation is allowed to yield elements out of order to trade determinism for performance. Defaults to False . file_pattern Optional[str] : Returns a list of files that match the given pattern Defaults to *.tfrecord* . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source]","title":"tf_data"},{"location":"generated/api/training_dataset_api/#to_dict","text":"TrainingDataset . to_dict () [source]","title":"to_dict"},{"location":"generated/api/training_dataset_api/#update_from_response_json","text":"TrainingDataset . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/api/transformation_functions_api/","text":"Transformation Function # [source] TransformationFunction # hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ) Properties # [source] id # Training dataset id. [source] name # [source] output_type # [source] source_code_content # [source] transformation_fn # [source] transformer_code # [source] version # Methods # [source] delete # TransformationFunction . delete () Delete transformation function from backend. [source] save # TransformationFunction . save () Persist transformation function in backend. Creation # [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. Retrieval # [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"Transformation Functions"},{"location":"generated/api/transformation_functions_api/#transformation-function","text":"[source]","title":"Transformation Function"},{"location":"generated/api/transformation_functions_api/#transformationfunction","text":"hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , builtin_source_code = None , output_type = None , id = None , type = None , items = None , count = None , href = None , )","title":"TransformationFunction"},{"location":"generated/api/transformation_functions_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/transformation_functions_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/transformation_functions_api/#name","text":"[source]","title":"name"},{"location":"generated/api/transformation_functions_api/#output_type","text":"[source]","title":"output_type"},{"location":"generated/api/transformation_functions_api/#source_code_content","text":"[source]","title":"source_code_content"},{"location":"generated/api/transformation_functions_api/#transformation_fn","text":"[source]","title":"transformation_fn"},{"location":"generated/api/transformation_functions_api/#transformer_code","text":"[source]","title":"transformer_code"},{"location":"generated/api/transformation_functions_api/#version","text":"","title":"version"},{"location":"generated/api/transformation_functions_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/transformation_functions_api/#delete","text":"TransformationFunction . delete () Delete transformation function from backend. [source]","title":"delete"},{"location":"generated/api/transformation_functions_api/#save","text":"TransformationFunction . save () Persist transformation function in backend.","title":"save"},{"location":"generated/api/transformation_functions_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/transformation_functions_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object.","title":"create_transformation_function"},{"location":"generated/api/transformation_functions_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/transformation_functions_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/transformation_functions_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"get_transformation_functions"},{"location":"generated/api/validation_api/","text":"Validation # [source] ValidationResult # hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group. Properties # [source] features # Feature of the validation result on which the rule was applied. [source] message # Message describing the outcome of applying the rule against the feature. [source] rule # Feature of the validation result on which the rule was applied. [source] status # [source] value # The computed value of the feature according to the rule. Methods # {{expectation_methods}} Validate a dataframe # [source] validate # FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object. Retrieval # [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"Validation"},{"location":"generated/api/validation_api/#validation","text":"[source]","title":"Validation"},{"location":"generated/api/validation_api/#validationresult","text":"hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group.","title":"ValidationResult"},{"location":"generated/api/validation_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/validation_api/#features","text":"Feature of the validation result on which the rule was applied. [source]","title":"features"},{"location":"generated/api/validation_api/#message","text":"Message describing the outcome of applying the rule against the feature. [source]","title":"message"},{"location":"generated/api/validation_api/#rule","text":"Feature of the validation result on which the rule was applied. [source]","title":"rule"},{"location":"generated/api/validation_api/#status","text":"[source]","title":"status"},{"location":"generated/api/validation_api/#value","text":"The computed value of the feature according to the rule.","title":"value"},{"location":"generated/api/validation_api/#methods","text":"{{expectation_methods}}","title":"Methods"},{"location":"generated/api/validation_api/#validate-a-dataframe","text":"[source]","title":"Validate a dataframe"},{"location":"generated/api/validation_api/#validate","text":"FeatureGroup . validate ( dataframe = None , log_activity = False ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. log_activity : Whether to log the validation as a feature group activity. If a dataframe is not provided (None), the validation will be logged as a feature store activity. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/api/validation_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/validation_api/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations. Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"get_validations"},{"location":"integrations/assume_role/","text":"Assuming a role # When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to projects in Hopsworks, for a guide on how to configure this see role mapping . After an administrator configured role mappings in Hopsworks you can see the roles you can assume by going to your project settings. Cloud roles mapped to project. You can then use the Hops Python and Java APIs to assume the roles listed in your project's settings page. When calling the assume role method you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call the assume role method with no argument. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the Default button above the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. In the image above if a Data scientist called the assume role method with no arguments she will assume the role with id 1029 but if a Data owner called the same method she will assume the role with id 1. Use temporary credentials. Python from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () Scala import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. Assume role also sets environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role. To read s3 buckets with TensorFlow you also need to set AWS_REGION environment variable (s3 bucket region). The code below shows how to read training and validation datasets from s3 bucket using TensorFlow. Use temporary credentials with TensorFlow. from hops.credentials_provider import get_role , assume_role import tensorflow as tf import os assume_role ( role_arn = get_role ( 1 )) # s3 bucket region need to be set for TensorFlow os . environ [ \"AWS_REGION\" ] = \"eu-north-1\" train_filenames = [ \"s3://resource/train/train.tfrecords\" ] validation_filenames = [ \"s3://resourcet/validation/validation.tfrecords\" ] train_dataset = tf . data . TFRecordDataset ( train_filenames ) validation_dataset = tf . data . TFRecordDataset ( validation_filenames ) for raw_record in train_dataset . take ( 1 ): example = tf . train . Example () example . ParseFromString ( raw_record . numpy ()) print ( example )","title":"Assume role"},{"location":"integrations/assume_role/#assuming-a-role","text":"When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to projects in Hopsworks, for a guide on how to configure this see role mapping . After an administrator configured role mappings in Hopsworks you can see the roles you can assume by going to your project settings. Cloud roles mapped to project. You can then use the Hops Python and Java APIs to assume the roles listed in your project's settings page. When calling the assume role method you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call the assume role method with no argument. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the Default button above the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. In the image above if a Data scientist called the assume role method with no arguments she will assume the role with id 1029 but if a Data owner called the same method she will assume the role with id 1. Use temporary credentials. Python from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () Scala import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. Assume role also sets environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role. To read s3 buckets with TensorFlow you also need to set AWS_REGION environment variable (s3 bucket region). The code below shows how to read training and validation datasets from s3 bucket using TensorFlow. Use temporary credentials with TensorFlow. from hops.credentials_provider import get_role , assume_role import tensorflow as tf import os assume_role ( role_arn = get_role ( 1 )) # s3 bucket region need to be set for TensorFlow os . environ [ \"AWS_REGION\" ] = \"eu-north-1\" train_filenames = [ \"s3://resource/train/train.tfrecords\" ] validation_filenames = [ \"s3://resourcet/validation/validation.tfrecords\" ] train_dataset = tf . data . TFRecordDataset ( train_filenames ) validation_dataset = tf . data . TFRecordDataset ( validation_filenames ) for raw_record in train_dataset . take ( 1 ): example = tf . train . Example () example . ParseFromString ( raw_record . numpy ()) print ( example )","title":"Assuming a role"},{"location":"integrations/hdinsight/","text":"Configure HDInsight for the Hopsworks Feature Store # To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information. Step 1: Set up a Hopsworks API key # In order for HDInsight clusters to be able to communicate with the Hopsworks Feature Store, the clients running on HDInsight need a Hopsworks API key. In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Make sure you have the key handy for the next steps. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Step 2: Use a script action to install the Feature Store connector # HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks Step 3: Configure HDInsight for Feature Store access # The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store. Step 5: Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Azure HDInsight"},{"location":"integrations/hdinsight/#configure-hdinsight-for-the-hopsworks-feature-store","text":"To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information.","title":"Configure HDInsight for the Hopsworks Feature Store"},{"location":"integrations/hdinsight/#step-1-set-up-a-hopsworks-api-key","text":"In order for HDInsight clusters to be able to communicate with the Hopsworks Feature Store, the clients running on HDInsight need a Hopsworks API key. In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Make sure you have the key handy for the next steps. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Step 1: Set up a Hopsworks API key"},{"location":"integrations/hdinsight/#step-2-use-a-script-action-to-install-the-feature-store-connector","text":"HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks","title":"Step 2:  Use a script action to install the Feature Store connector"},{"location":"integrations/hdinsight/#step-3-configure-hdinsight-for-feature-store-access","text":"The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store.","title":"Step 3: Configure HDInsight for Feature Store access"},{"location":"integrations/hdinsight/#step-5-connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Step 5: Connect to the Feature Store"},{"location":"integrations/hdinsight/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/mlstudio_designer/","text":"Azure Machine Learning Designer Integration # Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connect to the Feature Store # To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[python]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose python as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Designer"},{"location":"integrations/mlstudio_designer/#azure-machine-learning-designer-integration","text":"Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Designer Integration"},{"location":"integrations/mlstudio_designer/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/mlstudio_designer/#connect-to-the-feature-store","text":"To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[python]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose python as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline","title":"Connect to the Feature Store"},{"location":"integrations/mlstudio_designer/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/mlstudio_notebooks/","text":"Azure Machine Learning Notebooks Integration # Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connect from an Azure Machine Learning Notebook # To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Notebooks"},{"location":"integrations/mlstudio_notebooks/#azure-machine-learning-notebooks-integration","text":"Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Notebooks Integration"},{"location":"integrations/mlstudio_notebooks/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/mlstudio_notebooks/#connect-from-an-azure-machine-learning-notebook","text":"To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook","title":"Connect from an Azure Machine Learning Notebook"},{"location":"integrations/mlstudio_notebooks/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"integrations/mlstudio_notebooks/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Connect to the Feature Store"},{"location":"integrations/mlstudio_notebooks/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/python/","text":"Python Environments (Local or Kubeflow) # Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard. Create a file called featurestore.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='python' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services . Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Python"},{"location":"integrations/python/#python-environments-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow.","title":"Python Environments (Local or Kubeflow)"},{"location":"integrations/python/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard. Create a file called featurestore.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/python/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"integrations/python/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='python' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services .","title":"Connect to the Feature Store"},{"location":"integrations/python/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/role_mapping/","text":"IAM role mapping # Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Create an instance profile role that contains the different resource roles that we want to allow selected users to be able to assume in the Hopsworks cluster. In the example below, we define 4 different resource roles: test-role, s3-role, dev-s3-role, and redshift - and later we will define which users will be allowed to assume which of these resources roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Create the resource roles and edit the trust relationship and add a policy document that will allow the instance profile to assume the resource roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Finally attach the instance profile to the master node of your Hopsworks AWS instance. Role chaining allows the instance profile to assume any of the 4 resource roles in the policy that was attached in step 1. Typically, we will not want any user in Hopsworks to assume any of the resource roles. You can grant selected users the ability to assume any of the 4 resource roles from the admin page in hopsworks. In particular, we specify in which project(s) a given resource role can be used. Within a given project, we can further restrict who can assume the resource role by mapping the role to the group of users (data owners or data scientists). Resource role mapping. By clicking the 'Resource role mapping' icon in the admin page shown in the image above you can add mappings by entering the project name and which roles in that project can access the resource role. Optionally, you can set a role mapping as default by marking the default checkbox. The default roles can only be changed by a Data owner who can do so in the project settings page. Add resource role to project mapping. Any member of a project can then go to the project settings page to see which roles they can assume. Resource role mapped to project. For instructions on how to use the assume role API see assuming a role .","title":"Resource role to project mapping"},{"location":"integrations/role_mapping/#iam-role-mapping","text":"Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Create an instance profile role that contains the different resource roles that we want to allow selected users to be able to assume in the Hopsworks cluster. In the example below, we define 4 different resource roles: test-role, s3-role, dev-s3-role, and redshift - and later we will define which users will be allowed to assume which of these resources roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Create the resource roles and edit the trust relationship and add a policy document that will allow the instance profile to assume the resource roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Finally attach the instance profile to the master node of your Hopsworks AWS instance. Role chaining allows the instance profile to assume any of the 4 resource roles in the policy that was attached in step 1. Typically, we will not want any user in Hopsworks to assume any of the resource roles. You can grant selected users the ability to assume any of the 4 resource roles from the admin page in hopsworks. In particular, we specify in which project(s) a given resource role can be used. Within a given project, we can further restrict who can assume the resource role by mapping the role to the group of users (data owners or data scientists). Resource role mapping. By clicking the 'Resource role mapping' icon in the admin page shown in the image above you can add mappings by entering the project name and which roles in that project can access the resource role. Optionally, you can set a role mapping as default by marking the default checkbox. The default roles can only be changed by a Data owner who can do so in the project settings page. Add resource role to project mapping. Any member of a project can then go to the project settings page to see which roles they can assume. Resource role mapped to project. For instructions on how to use the assume role API see assuming a role .","title":"IAM role mapping"},{"location":"integrations/sagemaker/","text":"AWS SageMaker Integration # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key on AWS # The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks. Identify your SageMaker role # You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance Store the API key # You have two options to make your API key accessible from SageMaker: Option 1: Using the AWS Systems Manager Parameter Store # Store the API key in the AWS Systems Manager Parameter Store # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store Grant access to the Parameter Store from the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role Option 2: Using the AWS Secrets Manager # Store the API key in the AWS Secrets Manager # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager Grant access to the SecretsManager to the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances. Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"python\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS . Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"AWS Sagemaker"},{"location":"integrations/sagemaker/#aws-sagemaker-integration","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS SageMaker Integration"},{"location":"integrations/sagemaker/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/sagemaker/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"integrations/sagemaker/#store-the-api-key-on-aws","text":"The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks.","title":"Store the API key on AWS"},{"location":"integrations/sagemaker/#identify-your-sagemaker-role","text":"You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance","title":"Identify your SageMaker role"},{"location":"integrations/sagemaker/#store-the-api-key","text":"You have two options to make your API key accessible from SageMaker:","title":"Store the API key"},{"location":"integrations/sagemaker/#option-1-using-the-aws-systems-manager-parameter-store","text":"","title":"Option 1: Using the AWS Systems Manager Parameter Store"},{"location":"integrations/sagemaker/#store-the-api-key-in-the-aws-systems-manager-parameter-store","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store","title":"Store the API key in the AWS Systems Manager Parameter Store"},{"location":"integrations/sagemaker/#grant-access-to-the-parameter-store-from-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role","title":"Grant access to the Parameter Store from the SageMaker notebook role"},{"location":"integrations/sagemaker/#option-2-using-the-aws-secrets-manager","text":"","title":"Option 2: Using the AWS Secrets Manager"},{"location":"integrations/sagemaker/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager","title":"Store the API key in the AWS Secrets Manager"},{"location":"integrations/sagemaker/#grant-access-to-the-secretsmanager-to-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role","title":"Grant access to the SecretsManager to the SageMaker notebook role"},{"location":"integrations/sagemaker/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[python]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"python\" extra dependencies ( hsfs[python] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances.","title":"Install HSFS"},{"location":"integrations/sagemaker/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'python' # Choose Python as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"python\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS .","title":"Connect to the Feature Store"},{"location":"integrations/sagemaker/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/spark/","text":"Spark Integration # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster. Download the Hopsworks Client Jars # In the Feature Store UI, select the integration tab and then select the Spark tab. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the client libraries for HopsHive and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster Download the certificates # Download the certificates from the same Spark tab in the Feature Store UI. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well. Configure your Spark cluster # Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.trustore.name trustStore.jks spark.sql.hive.metastore.jars [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars should point to the path with the Hive Jars which can be found in the clients.tar.gz . PySpark # To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Generating an API Key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select Api keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The created API-Key should at least have the following scopes: featurestore project job API-Keys can be generated in the User Settings on Hopsworks Info You are only ably to retrieve the API Key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connecting to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above. Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Spark"},{"location":"integrations/spark/#spark-integration","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Integration"},{"location":"integrations/spark/#download-the-hopsworks-client-jars","text":"In the Feature Store UI, select the integration tab and then select the Spark tab. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the client libraries for HopsHive and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster","title":"Download the Hopsworks Client Jars"},{"location":"integrations/spark/#download-the-certificates","text":"Download the certificates from the same Spark tab in the Feature Store UI. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well.","title":"Download the certificates"},{"location":"integrations/spark/#configure-your-spark-cluster","text":"Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.trustore.name trustStore.jks spark.sql.hive.metastore.jars [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars should point to the path with the Hive Jars which can be found in the clients.tar.gz .","title":"Configure your Spark cluster"},{"location":"integrations/spark/#pyspark","text":"To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"PySpark"},{"location":"integrations/spark/#generating-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select Api keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The created API-Key should at least have the following scopes: featurestore project job API-Keys can be generated in the User Settings on Hopsworks Info You are only ably to retrieve the API Key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generating an API Key"},{"location":"integrations/spark/#connecting-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Pandas on Python as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"python\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above.","title":"Connecting to the Feature Store"},{"location":"integrations/spark/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/storage-connectors/","text":"Storage Connectors # You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store information in Hopsworks about how to securely connect to external data stores. They can be used in both programs and in Hopsworks to easily and securely connect and ingest data to the Feature Store. External (on-demand) Feature Groups can also be defined with storage connectors, where only the metadata is stored in Hopsworks. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or more advanced multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. Mapping IAM Roles to Projects/Roles in Hopsworks In Hopsworks, you can specify a Cloud Role (IAM role or managed identity) and (1) in which Project and (2) what role within that Project can assume that Cloud Role. For example, you could limit access to a given IAM Role to users who have the 'Data Owner' role in a Project called 'RawFeatures'. That IAM Role could provide read access to a Redshift database/table, providing fine-grained access to Redshift from selected users in selected projects in Hopsworks. ADLS HopsFS JDBC Redshift S3 Snowflake Programmatic Connectors (Spark, Python, Java/Scala, Flink) # It is also possible to use the rich ecosystem of connectors available in programs run on Hopsworks. Just Spark has tens of open-source libraries for connecting to relational databases, key-value stores, file systems, object stores, search databases, and graph databases. In Hopsworks, you can securely save your credentials as secrets, and securely access them with API calls when you need to connect to your external store. Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Overview"},{"location":"integrations/storage-connectors/#storage-connectors","text":"You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store information in Hopsworks about how to securely connect to external data stores. They can be used in both programs and in Hopsworks to easily and securely connect and ingest data to the Feature Store. External (on-demand) Feature Groups can also be defined with storage connectors, where only the metadata is stored in Hopsworks. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or more advanced multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. Mapping IAM Roles to Projects/Roles in Hopsworks In Hopsworks, you can specify a Cloud Role (IAM role or managed identity) and (1) in which Project and (2) what role within that Project can assume that Cloud Role. For example, you could limit access to a given IAM Role to users who have the 'Data Owner' role in a Project called 'RawFeatures'. That IAM Role could provide read access to a Redshift database/table, providing fine-grained access to Redshift from selected users in selected projects in Hopsworks. ADLS HopsFS JDBC Redshift S3 Snowflake","title":"Storage Connectors"},{"location":"integrations/storage-connectors/#programmatic-connectors-spark-python-javascala-flink","text":"It is also possible to use the rich ecosystem of connectors available in programs run on Hopsworks. Just Spark has tens of open-source libraries for connecting to relational databases, key-value stores, file systems, object stores, search databases, and graph databases. In Hopsworks, you can securely save your credentials as secrets, and securely access them with API calls when you need to connect to your external store.","title":"Programmatic Connectors (Spark, Python, Java/Scala, Flink)"},{"location":"integrations/storage-connectors/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/databricks/api_key/","text":"Hopsworks API key # In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job kafka API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key # AWS # Step 1: Create an instance profile to attach to your Databricks clusters # Go to the AWS IAM choose Roles and click on Create Role . Select AWS Service as the type of trusted entity and EC2 as the use case as shown below: Create an instance profile Click on Next: Permissions , Next:Tags , and then Next: Review . Name the instance profile role and then click Create role . Step 2: Storing the API Key # Option 1: Using the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the name of the AWS role you have created in Step 1 . Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then search for the role that you have created in Step 1 . Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store Option 2: Using the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role you have created in Step 1 . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then the role that that you have created in Step 1 . Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager Step 3: Allow Databricks to use the AWS role created in Step 1 # First you need to get the AWS role used by Databricks for deployments as described in this step . Once you get the role name, go to AWS IAM , search for the role, and click on it. Then, select the Permissions tab, click on Add inline policy , select the JSON tab, and paste the following snippet. Replace [ACCOUNT_ID] with your AWS account id, and [MY_DATABRICKS_ROLE] with the AWS role name created in Step 1 . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PassRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::[ACCOUNT_ID]:role/[MY_DATABRICKS_ROLE]\" } ] } Click Review Policy , name the policy, and click Create Policy . Then, go to your Databricks workspace and follow this step to add the instance profile to your workspace. Finally, when launching Databricks clusters, select Advanced settings and choose the instance profile you have just added. Azure # On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store. Next Steps # Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Hopsworks API Key"},{"location":"integrations/databricks/api_key/#hopsworks-api-key","text":"In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key.","title":"Hopsworks API key"},{"location":"integrations/databricks/api_key/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job kafka API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/databricks/api_key/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"integrations/databricks/api_key/#store-the-api-key","text":"","title":"Store the API key"},{"location":"integrations/databricks/api_key/#aws","text":"","title":"AWS"},{"location":"integrations/databricks/api_key/#step-1-create-an-instance-profile-to-attach-to-your-databricks-clusters","text":"Go to the AWS IAM choose Roles and click on Create Role . Select AWS Service as the type of trusted entity and EC2 as the use case as shown below: Create an instance profile Click on Next: Permissions , Next:Tags , and then Next: Review . Name the instance profile role and then click Create role .","title":"Step 1: Create an instance profile to attach to your Databricks clusters"},{"location":"integrations/databricks/api_key/#step-2-storing-the-api-key","text":"Option 1: Using the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the name of the AWS role you have created in Step 1 . Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then search for the role that you have created in Step 1 . Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store Option 2: Using the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role you have created in Step 1 . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Once the API Key is stored, you need to grant access to it from the AWS role that you have created in Step 1 . In the AWS Management Console, go to IAM , select Roles and then the role that that you have created in Step 1 . Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager","title":"Step 2: Storing the API Key"},{"location":"integrations/databricks/api_key/#step-3-allow-databricks-to-use-the-aws-role-created-in-step-1","text":"First you need to get the AWS role used by Databricks for deployments as described in this step . Once you get the role name, go to AWS IAM , search for the role, and click on it. Then, select the Permissions tab, click on Add inline policy , select the JSON tab, and paste the following snippet. Replace [ACCOUNT_ID] with your AWS account id, and [MY_DATABRICKS_ROLE] with the AWS role name created in Step 1 . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PassRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::[ACCOUNT_ID]:role/[MY_DATABRICKS_ROLE]\" } ] } Click Review Policy , name the policy, and click Create Policy . Then, go to your Databricks workspace and follow this step to add the instance profile to your workspace. Finally, when launching Databricks clusters, select Advanced settings and choose the instance profile you have just added.","title":"Step 3: Allow Databricks to use the AWS role created in Step 1"},{"location":"integrations/databricks/api_key/#azure","text":"On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store.","title":"Azure"},{"location":"integrations/databricks/api_key/#next-steps","text":"Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/configuration/","text":"Databricks Integration # Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step. Prerequisites # In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks. Networking # If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the Hopsworks.ai VPC/VNet. Hopsworks API key # In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks . Databricks API key # Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure. Register a new Databricks Instance # Users can register a new Databricks instance by navigating to the Integrations tab of a project Feature Store. Registering a Databricks instance requires adding the instance address and the API key. The instance address should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of. Databricks Cluster # A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 8.x is suggested to be able to use the full suite of Hopsworks Feature Store capabilities. Configure a cluster # Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above. Connecting to the Feature Store # At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'local' , api_key_file = \"featurestore.key\" , # For Azure, store the API key locally secrets_store = \"local\" , hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Configuration"},{"location":"integrations/databricks/configuration/#databricks-integration","text":"Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step.","title":"Databricks Integration"},{"location":"integrations/databricks/configuration/#prerequisites","text":"In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks.","title":"Prerequisites"},{"location":"integrations/databricks/configuration/#networking","text":"If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the Hopsworks.ai VPC/VNet.","title":"Networking"},{"location":"integrations/databricks/configuration/#hopsworks-api-key","text":"In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks .","title":"Hopsworks API key"},{"location":"integrations/databricks/configuration/#databricks-api-key","text":"Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure.","title":"Databricks API key"},{"location":"integrations/databricks/configuration/#register-a-new-databricks-instance","text":"Users can register a new Databricks instance by navigating to the Integrations tab of a project Feature Store. Registering a Databricks instance requires adding the instance address and the API key. The instance address should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of.","title":"Register a new Databricks Instance"},{"location":"integrations/databricks/configuration/#databricks-cluster","text":"A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 8.x is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.","title":"Databricks Cluster"},{"location":"integrations/databricks/configuration/#configure-a-cluster","text":"Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above.","title":"Configure a cluster"},{"location":"integrations/databricks/configuration/#connecting-to-the-feature-store","text":"At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'local' , api_key_file = \"featurestore.key\" , # For Azure, store the API key locally secrets_store = \"local\" , hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Connecting to the Feature Store"},{"location":"integrations/databricks/configuration/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/networking/","text":"Networking # In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster. AWS # Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are working on Hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store, Online Feature Store, and Kafka services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details Azure # Step 1: Set up VNet peering between Hopsworks and Databricks # VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on Hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings Step 2: Configure the Network Security Group # The virtual network peering will allow full access between the Hopsworks virtual network and the Databricks virtual network by default. However, if you have a different setup, ensure that the Network Security Group of the Feature Store is configured to allow traffic from your Databricks clusters. Ensure that ports 443 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks cluster Network Security Group . Hopsworks.ai If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store, Online Feature Store, and Kafka services . Next Steps # Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/databricks/networking/#networking","text":"In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster.","title":"Networking"},{"location":"integrations/databricks/networking/#aws","text":"","title":"AWS"},{"location":"integrations/databricks/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are working on Hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"integrations/databricks/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store, Online Feature Store, and Kafka services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details","title":"Step 2: Configure the Security Group"},{"location":"integrations/databricks/networking/#azure","text":"","title":"Azure"},{"location":"integrations/databricks/networking/#step-1-set-up-vnet-peering-between-hopsworks-and-databricks","text":"VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on Hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings","title":"Step 1: Set up VNet peering between Hopsworks and Databricks"},{"location":"integrations/databricks/networking/#step-2-configure-the-network-security-group","text":"The virtual network peering will allow full access between the Hopsworks virtual network and the Databricks virtual network by default. However, if you have a different setup, ensure that the Network Security Group of the Feature Store is configured to allow traffic from your Databricks clusters. Ensure that ports 443 , 9083 , 9085 , 8020 , 50010 , and 9092 are reachable from the Databricks cluster Network Security Group . Hopsworks.ai If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store, Online Feature Store, and Kafka services .","title":"Step 2: Configure the Network Security Group"},{"location":"integrations/databricks/networking/#next-steps","text":"Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/emr/emr_configuration/","text":"Configure EMR for the Hopsworks Feature Store # To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide. Step 1: Set up a Hopsworks API key # In order for EMR clusters to be able to communicate with the Hopsworks Feature Store, the clients running on EMR need to be able to access a Hopsworks API key. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Account Settings to open the user settings. Select API . Give the key a name and select the project scope before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: project API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Store the API key in the AWS Secrets Manager # In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret Grant access to the secret to the EMR EC2 instance profile # Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager Step 2: Configure your EMR cluster # Add the Hopsworks Feature Store configuration to your EMR cluster # In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.hadoop.hops.ipc.server.ssl.enabled\" : true , \"spark.hadoop.fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"spark.hadoop.client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"spark.hadoop.hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"spark.hadoop.hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"spark.hadoop.hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"spark.hadoop.hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"spark.hadoop.hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" , \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.sql.hive.metastore.jars\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.hadoop.hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } }, ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store Add the Bootstrap Action to your EMR cluster # EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz || true mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks sudo pip3 install --upgrade hsfs~ = X.X.0 Note Don't forget to replace X.X.0 with the major and minor version of your Hopsworks deployment. To find your Hopsworks version, enter any of your projects and go to the settings tab inside your project. Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store. Next Steps # Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"integrations/emr/emr_configuration/#configure-emr-for-the-hopsworks-feature-store","text":"To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide.","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"integrations/emr/emr_configuration/#step-1-set-up-a-hopsworks-api-key","text":"In order for EMR clusters to be able to communicate with the Hopsworks Feature Store, the clients running on EMR need to be able to access a Hopsworks API key.","title":"Step 1: Set up a Hopsworks API key"},{"location":"integrations/emr/emr_configuration/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Account Settings to open the user settings. Select API . Give the key a name and select the project scope before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: project API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/emr/emr_configuration/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret","title":"Store the API key in the AWS Secrets Manager"},{"location":"integrations/emr/emr_configuration/#grant-access-to-the-secret-to-the-emr-ec2-instance-profile","text":"Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager","title":"Grant access to the secret to the EMR EC2 instance profile"},{"location":"integrations/emr/emr_configuration/#step-2-configure-your-emr-cluster","text":"","title":"Step 2: Configure your EMR cluster"},{"location":"integrations/emr/emr_configuration/#add-the-hopsworks-feature-store-configuration-to-your-emr-cluster","text":"In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.hadoop.hops.ipc.server.ssl.enabled\" : true , \"spark.hadoop.fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"spark.hadoop.client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"spark.hadoop.hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"spark.hadoop.hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"spark.hadoop.hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"spark.hadoop.hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"spark.hadoop.hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" , \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.sql.hive.metastore.jars\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.hadoop.hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } }, ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store","title":"Add the Hopsworks Feature Store configuration to your EMR cluster"},{"location":"integrations/emr/emr_configuration/#add-the-bootstrap-action-to-your-emr-cluster","text":"EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz || true mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks sudo pip3 install --upgrade hsfs~ = X.X.0 Note Don't forget to replace X.X.0 with the major and minor version of your Hopsworks deployment. To find your Hopsworks version, enter any of your projects and go to the settings tab inside your project. Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store.","title":"Add the Bootstrap Action to your EMR cluster"},{"location":"integrations/emr/emr_configuration/#next-steps","text":"Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/emr/networking/","text":"Networking # In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store. Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are on Hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups Next Steps # Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/emr/networking/#networking","text":"In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/emr/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are on Hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"integrations/emr/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups","title":"Step 2: Configure the Security Group"},{"location":"integrations/emr/networking/#next-steps","text":"Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/storage-connectors/adls/","text":"Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting persmissions to a service principal. Requirements # Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you. Configure an ADLS storage connector in the Hopsworks UI. Azure Create a ADLS Resource # When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. Common Problems: If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" buton to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container. References: How to create a service principal on Azure","title":"ADLS"},{"location":"integrations/storage-connectors/adls/#requirements","text":"Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you. Configure an ADLS storage connector in the Hopsworks UI.","title":"Requirements"},{"location":"integrations/storage-connectors/adls/#azure-create-a-adls-resource","text":"When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. Common Problems: If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" buton to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container. References: How to create a service principal on Azure","title":"Azure Create a ADLS Resource"},{"location":"integrations/storage-connectors/hopsfs/","text":"HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset. You can define a storage connector to a directory in the same project in HopsFS by selecting the directory.","title":"HopsFS"},{"location":"integrations/storage-connectors/jdbc/","text":"Most databases can be connected to using our generic JDBC Storage Connector (such as MySQL, Postgres, Oracle, DB2, and MongoDB). Typically, you will need to add username and password in your JDBC URL or as key/value parameters. Consult the documentation of your target database to determine the correct JDBC URL and parameters. You can define a storage connector to a JDBC enabled source using a JDBC connection string and optional parameters supplied at runtime.","title":"JDBC"},{"location":"integrations/storage-connectors/redshift/","text":"Amazon Redshift is a popular managed data warehouse on AWS. Configure the Redshift storage connector in the Hopsworks UI. In the UI for the Redshift connector, you should enter the following: Cluster identifier: The name of the cluster Database driver: You can use the default JDBC Redshift Driver com.amazon.redshift.jdbc42.Driver (More on this later) Database endpoint: The endpoint for the database. Should be in the format of [UUID].eu-west-1.redshift.amazonaws.com Database name: The name of the database to query Database port: The port of the cluster. Defaults to 5349 There are two options available for authenticating with the Redshift cluster. The first option is to configure a username and a password. The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user. With regards to the database driver, the library to interact with Redshift is not included in Hopsworks - you need to upload the driver yourself. First, you need to download the library . Select the driver version without the AWS SDK. You then upload the driver files to the \u201cResources\u201d dataset in your project, see the screenshot below. Upload the Redshift driver to Hopsworks. Then, you add the file to your notebook or job before launching it, as shown in the screenshots below. When you start a Jupyter notebook, you need to add the driver so it can be accessed in programs.","title":"Redshift"},{"location":"integrations/storage-connectors/s3/","text":"You can define an Amazon S3 storage connector using the bucket name and the IAM role used to access the bucket. The bucket may be encrypted. If you have enabled an IAM role for the cluster or Multiple (federated) IAM roles, you can select the IAM role that should be used to access the S3 bucket. Configure the S3 storage connector in the Hopsworks UI.","title":"S3"},{"location":"integrations/storage-connectors/snowflake/","text":"Snowflake is a popular cloud-native data warehouse service, and supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. To interact with Snowflake and to register and read external feature groups users need to define a storage connector using the UI: Snowflake connector UI To configure the connector users need to provide the Connection URL of the cluster. The Snowflake storage connector supports both username and password authentication as well as token-based authentication. Token-based authentication Beta Currently token-based authentication is in beta phase. Users are advised to use username/password and/or create a service account for accessing Snowflake from Hopsworks. The Hopsworks Snowflake storage connector allows users to specify several additional fields, though only two are mandatory: the database field and the schema field. The role field can be used to specify which Snowflake security role to assume for the session after the connection is established. The application field can also be specified to have better observability in Snowflake with regards to which application is running which query. The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project. Additional key/value options can also be specified to control the behavior of the Snowflake Spark connector. The available options are listed in the Snowflake documentation .","title":"Snowflake"}]}