{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hopsworks Feature Store","text":"<p>HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy.</p> <p>The library is environment independent and can be used in two modes:</p> <ul> <li> <p>Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages.</p> </li> <li> <p>Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow.</p> </li> </ul> <p>The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information checkout the Hopsworks documentation.</p>"},{"location":"#getting-started-on-hopsworks","title":"Getting Started On Hopsworks","text":"<p>Get started easily by registering an account on Hopsworks Serverless. Create your project and a new Api key. In a new python environment with Python 3.8 or higher, install the client library using pip:</p> <pre><code># Get all Hopsworks SDKs: Feature Store, Model Serving and Platform SDK\npip install hopsworks\n# or minimum install with the Feature Store SDK\npip install hsfs[python]\n# if using zsh don't forget the quotes\npip install 'hsfs[python]'\n</code></pre> <p>You can start a notebook and instantiate a connection and get the project feature store handler.</p> <pre><code>import hopsworks\n\nproject = hopsworks.login() # you will be prompted for your api key\nfs = project.get_feature_store()\n</code></pre> <p>or using <code>hsfs</code> directly:</p> <pre><code>import hsfs\n\nconnection = hsfs.connection(\n    host=\"c.app.hopsworks.ai\", #\n    project=\"your-project\",\n    api_key_value=\"your-api-key\",\n)\nfs = connection.get_feature_store()\n</code></pre> <p>Create a new feature group to start inserting feature values. <pre><code>fg = fs.create_feature_group(\"rain\",\n                        version=1,\n                        description=\"Rain features\",\n                        primary_key=['date', 'location_id'],\n                        online_enabled=True)\n\nfg.save(dataframe)\n</code></pre></p> <p>Upsert new data in to the feature group with <code>time_travel_format=\"HUDI\"</code>\". <pre><code>fg.insert(upsert_df)\n</code></pre></p> <p>Retrieve commit timeline metdata of the feature group with <code>time_travel_format=\"HUDI\"</code>\". <pre><code>fg.commit_details()\n</code></pre></p> <p>\"Reading feature group as of specific point in time\". <pre><code>fg = fs.get_feature_group(\"rain\", 1)\nfg.read(\"2020-10-20 07:34:11\").show()\n</code></pre></p> <p>Read updates  that occurred between specified points in time. <pre><code>fg = fs.get_feature_group(\"rain\", 1)\nfg.read_changes(\"2020-10-20 07:31:38\", \"2020-10-20 07:34:11\").show()\n</code></pre></p> <p>Join features together <pre><code>feature_join = rain_fg.select_all()\n                    .join(temperature_fg.select_all(), on=[\"date\", \"location_id\"])\n                    .join(location_fg.select_all())\nfeature_join.show(5)\n</code></pre></p> <p>join feature groups that correspond to specific point in time <pre><code>feature_join = rain_fg.select_all()\n                    .join(temperature_fg.select_all(), on=[\"date\", \"location_id\"])\n                    .join(location_fg.select_all())\n                    .as_of(\"2020-10-31\")\nfeature_join.show(5)\n</code></pre></p> <p>join feature groups that correspond to different time <pre><code>rain_fg_q = rain_fg.select_all().as_of(\"2020-10-20 07:41:43\")\ntemperature_fg_q = temperature_fg.select_all().as_of(\"2020-10-20 07:32:33\")\nlocation_fg_q = location_fg.select_all().as_of(\"2020-10-20 07:33:08\")\njoined_features_q = rain_fg_q.join(temperature_fg_q).join(location_fg_q)\n</code></pre></p> <p>Use the query object to create a training dataset: <pre><code>td = fs.create_training_dataset(\"rain_dataset\",\n                                version=1,\n                                data_format=\"tfrecords\",\n                                description=\"A test training dataset saved in TfRecords format\",\n                                splits={'train': 0.7, 'test': 0.2, 'validate': 0.1})\n\ntd.save(feature_join)\n</code></pre></p> <p>A short introduction to the Scala API: <pre><code>import com.logicalclocks.hsfs._\nval connection = HopsworksConnection.builder().build()\nval fs = connection.getFeatureStore();\nval attendances_features_fg = fs.getFeatureGroup(\"games_features\", 1);\nattendances_features_fg.show(1)\n</code></pre></p> <p>You can find more examples on how to use the library in our hops-examples repository.</p>"},{"location":"#usage","title":"Usage","text":"<p>Usage data is collected for improving quality of the library. It is turned on by default if the backend is \"c.app.hopsworks.ai\". To turn it off, use one of the following way: <pre><code># use environment variable\nimport os\nos.environ[\"ENABLE_HOPSWORKS_USAGE\"] = \"false\"\n\n# use `disable_usage_logging`\nimport hsfs\nhsfs.disable_usage_logging()\n</code></pre></p> <p>The source code can be found in python/hsfs/usage.py.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is available at Hopsworks Feature Store Documentation.</p>"},{"location":"#issues","title":"Issues","text":"<p>For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community.</p> <p>Please report any issue using Github issue tracking.</p> <p>Please attach the client environment from the output below in the issue: <pre><code>import hopsworks\nimport hsfs\nhopsworks.login().get_feature_store()\nprint(hsfs.get_env())\n</code></pre></p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you would like to contribute to this library, please see the Contribution Guidelines.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#python-development-setup","title":"Python development setup","text":"<ul> <li> <p>Fork and clone the repository</p> </li> <li> <p>Create a new Python environment with your favourite environment manager (e.g. virtualenv or conda) and Python 3.9 (newer versions will return a library conflict in <code>auto_doc.py</code>)</p> </li> <li> <p>Install repository in editable mode with development dependencies:</p> </li> </ul> <pre><code>cd python\npip install -e \".[python,dev]\"\n</code></pre> <ul> <li>Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through ruff. Run the following commands from the <code>python</code> directory:</li> </ul> <pre><code>cd python\npip install --user pre-commit\npre-commit install\n</code></pre> <p>Afterwards, pre-commit will run whenever you commit.</p> <ul> <li>To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use <code>ruff</code>, or run it via the command line:</li> </ul> <pre><code># linting\nruff check python --fix\n# formatting\nruff format python\n</code></pre>"},{"location":"CONTRIBUTING/#python-documentation","title":"Python documentation","text":"<p>We follow a few best practices for writing the Python documentation:</p> <ol> <li>Use the google docstring style:</li> </ol> <pre><code>\"\"\"[One Line Summary]\n\n[Extended Summary]\n\n[!!! example\n    import xyz\n]\n\n# Arguments\n    arg1: Type[, optional]. Description[, defaults to `default`]\n    arg2: Type[, optional]. Description[, defaults to `default`]\n\n# Returns\n    Type. Description.\n\n# Raises\n    Exception. Description.\n\"\"\"\n</code></pre> <p>If Python 3 type annotations are used, they are inserted automatically.</p> <ol> <li>Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring.</li> <li>REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults.</li> <li>Public Api such as metadata objects should be fully documented with defaults.</li> </ol>"},{"location":"CONTRIBUTING/#setup-and-build-documentation","title":"Setup and Build Documentation","text":"<p>We use <code>mkdocs</code> together with <code>mike</code> (for versioning) to build the documentation and a plugin called <code>keras-autodoc</code> to auto generate Python API documentation from docstrings.</p> <p>Background about <code>mike</code>: <code>mike</code> builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, <code>mike</code> maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like <code>dev</code> or <code>latest</code>, to indicate stable and unstable releases.</p> <ol> <li>Currently we are using our own version of <code>keras-autodoc</code></li> </ol> <pre><code>pip install git+https://github.com/logicalclocks/keras-autodoc\n</code></pre> <ol> <li>Install HSFS with <code>docs</code> extras:</li> </ol> <pre><code>pip install -e \".[python,dev,docs]\"\n</code></pre> <ol> <li>To build the docs, first run the auto doc script:</li> </ol> <pre><code>cd ..\npython auto_doc.py\n</code></pre>"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","title":"Option 1: Build only current version of docs","text":"<ol> <li>Either build the docs, or serve them dynamically:</li> </ol> <p>Note: Links and pictures might not resolve properly later on when checking with this build.    The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and    therefore another level is added to all paths, e.g. <code>docs.hopsworks.ai/[version-or-alias]</code>.    Using relative links should not be affected by this, however, building the docs with version    (Option 2) is recommended.</p> <pre><code>mkdocs build\n# or\nmkdocs serve\n</code></pre>"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","title":"Option 2 (Preferred): Build multi-version doc with <code>mike</code>","text":""},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","title":"Versioning on docs.hopsworks.ai","text":"<p>On docs.hopsworks.ai we implement the following versioning scheme:</p> <ul> <li>current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev], where <code>dev</code> is an alias to indicate that this is an unstable version.</li> <li>the latest release: rendered with full current version, e.g. 2.1.5 [latest] with <code>latest</code> alias to indicate that this is the latest stable release.</li> <li>previous stable releases: rendered without alias, e.g. 2.1.4.</li> </ul>"},{"location":"CONTRIBUTING/#build-instructions","title":"Build Instructions","text":"<ol> <li> <p>For this you can either checkout and make a local copy of the <code>upstream/gh-pages</code> branch, where <code>mike</code> maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating:</p> <p>Building one branch:</p> <p>Checkout your dev branch with modified docs:</p> <pre><code>git checkout [dev-branch]\n</code></pre> <p>Generate API docs if necessary:</p> <pre><code>python auto_doc.py\n</code></pre> <p>Build docs with a version and alias</p> <pre><code>mike deploy [version] [alias] --update-alias\n\n# for example, if you are updating documentation to be merged to master,\n# which will become the new SNAPSHOT version:\nmike deploy 2.2.0-SNAPSHOT dev --update-alias\n\n# if you are updating docs of the latest stable release branch\nmike deploy [version] latest --update-alias\n\n# if you are updating docs of a previous stable release branch\nmike deploy [version]\n</code></pre> <p>If no gh-pages branch existed in your local repository, this will have created it.</p> <p>Important: If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows</p> <pre><code>mike set-default [version-or-alias]\n</code></pre> <p>You can now checkout the gh-pages branch and serve:</p> <pre><code>git checkout gh-pages\nmike serve\n</code></pre> <p>You can also list all available versions/aliases:</p> <pre><code>mike list\n</code></pre> <p>Delete and reset your local gh-pages branch:</p> <pre><code>mike delete --all\n\n# or delete single version\nmike delete [version-or-alias]\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#adding-new-api-documentation","title":"Adding new API documentation","text":"<p>To add new documentation for APIs, you need to add information about the method/class to document to the <code>auto_doc.py</code> script:</p> <pre><code>PAGES = {\n    \"connection.md\": [\n        \"hsfs.connection.Connection.connection\"\n    ]\n    \"new_template.md\": [\n            \"module\",\n            \"xyz.asd\"\n    ]\n}\n</code></pre> <p>Now you can add a template markdown file to the <code>docs/templates</code> directory with the name you specified in the auto-doc script. The <code>new_template.md</code> file should contain a tag to identify the place at which the API documentation should be inserted:</p> <pre><code>## The XYZ package\n\n{{module}}\n\nSome extra content here.\n\n!!! example\n    ```python\n    import xyz\n    ```\n\n{{xyz.asd}}\n</code></pre> <p>Finally, run the <code>auto_doc.py</code> script, as decribed above, to update the documentation.</p> <p>For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation.</p>"},{"location":"generated/api/connection_api/","title":"Connection","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#connection_1","title":"Connection","text":"<pre><code>hsfs.connection.Connection(\n    host=None,\n    port=443,\n    project=None,\n    engine=None,\n    region_name=\"default\",\n    secrets_store=\"parameterstore\",\n    hostname_verification=True,\n    trust_store_path=None,\n    cert_folder=\"/tmp\",\n    api_key_file=None,\n    api_key_value=None,\n)\n</code></pre> <p>A feature store connection object.</p> <p>The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to.</p> <p>This class provides convenience classmethods accessible from the <code>hsfs</code>-module:</p> <p>Connection factory</p> <p>For convenience, <code>hsfs</code> provides a factory method, accessible from the top level module, so you don't have to import the <code>Connection</code> class manually:</p> <pre><code>import hsfs\nconn = hsfs.connection()\n</code></pre> <p>Save API Key as File</p> <p>To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store.</p> <p>You can then connect by simply passing the path to the key file when instantiating a connection:</p> <pre><code>    import hsfs\n    conn = hsfs.connection(\n        'my_instance',                      # Hostname of your Feature Store instance\n        443,                                # Port to reach your Hopsworks instance, defaults to 443\n        'my_project',                       # Name of your Hopsworks Feature Store project\n        api_key_file='featurestore.key',    # The file containing the API key generated above\n        hostname_verification=True)         # Disable for self-signed certificates\n    )\n    fs = conn.get_feature_store()           # Get the project's default feature store\n\n    # or\n\n    import hopsworks\n    project = hopsworks.login()\n    fs = project.get_feature_store()\n</code></pre> <p>Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides.</p> <p>Arguments</p> <ul> <li>host <code>str | None</code>: The hostname of the Hopsworks instance in the form of <code>[UUID].cloud.hopsworks.ai</code>,     defaults to <code>None</code>. Do not use the url including <code>https://</code> when connecting     programatically.</li> <li>port <code>int</code>: The port on which the Hopsworks instance can be reached,     defaults to <code>443</code>.</li> <li>project <code>str | None</code>: The name of the project to connect to. When running on Hopsworks, this     defaults to the project from where the client is run from.     Defaults to <code>None</code>.</li> <li>engine <code>str | None</code>: Which engine to use, <code>\"spark\"</code>, <code>\"python\"</code> or <code>\"training\"</code>. Defaults to <code>None</code>,     which initializes the engine to Spark if the environment provides Spark, for     example on Hopsworks and Databricks, or falls back on Hive in Python if Spark is not     available, e.g. on local Python environments or AWS SageMaker. This option     allows you to override this behaviour. <code>\"training\"</code> engine is useful when only     feature store metadata is needed, for example training dataset location and label     information when Hopsworks training experiment is conducted.</li> <li>region_name <code>str</code>: The name of the AWS region in which the required secrets are     stored, defaults to <code>\"default\"</code>.</li> <li>secrets_store <code>str</code>: The secrets storage to be used, either <code>\"secretsmanager\"</code>,     <code>\"parameterstore\"</code> or <code>\"local\"</code>, defaults to <code>\"parameterstore\"</code>.</li> <li>hostname_verification <code>bool</code>: Whether or not to verify Hopsworks\u2019 certificate, defaults     to <code>True</code>.</li> <li>trust_store_path <code>str | None</code>: Path on the file system containing the Hopsworks certificates,     defaults to <code>None</code>.</li> <li>cert_folder <code>str</code>: The directory to store retrieved HopsFS certificates, defaults to     <code>\"/tmp\"</code>. Only required when running without a Spark environment.</li> <li>api_key_file <code>str | None</code>: Path to a file containing the API Key, if provided,     <code>secrets_store</code> will be ignored, defaults to <code>None</code>.</li> <li>api_key_value <code>str | None</code>: API Key as string, if provided, <code>secrets_store</code> will be ignored<code>,     however, this should be used with care, especially if the used notebook or     job script is accessible by multiple parties. Defaults to</code>None`.</li> </ul> <p>Returns</p> <p><code>Connection</code>. Feature Store connection handle to perform operations on a     Hopsworks project.</p>"},{"location":"generated/api/connection_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#api_key_file","title":"api_key_file","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#api_key_value","title":"api_key_value","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#cert_folder","title":"cert_folder","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#host","title":"host","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#hostname_verification","title":"hostname_verification","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#port","title":"port","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#project","title":"project","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#region_name","title":"region_name","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#secrets_store","title":"secrets_store","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#trust_store_path","title":"trust_store_path","text":""},{"location":"generated/api/connection_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/connection_api/#close","title":"close","text":"<pre><code>Connection.close()\n</code></pre> <p>Close a connection gracefully.</p> <p>This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker.</p> <p>Usage is recommended but optional.</p> <p>Example</p> <pre><code>import hsfs\nconn = hsfs.connection()\nconn.close()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/connection_api/#connect","title":"connect","text":"<pre><code>Connection.connect()\n</code></pre> <p>Instantiate the connection.</p> <p>Creating a <code>Connection</code> object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the <code>close()</code> method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call <code>connect()</code> again to reopen the connection.</p> <p>Example</p> <pre><code>import hsfs\nconn = hsfs.connection()\nconn.close()\nconn.connect()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/connection_api/#connection_2","title":"connection","text":"<pre><code>Connection.connection(\n    host=None,\n    port=443,\n    project=None,\n    engine=None,\n    region_name=\"default\",\n    secrets_store=\"parameterstore\",\n    hostname_verification=True,\n    trust_store_path=None,\n    cert_folder=\"/tmp\",\n    api_key_file=None,\n    api_key_value=None,\n)\n</code></pre> <p>Connection factory method, accessible through <code>hsfs.connection()</code>.</p> <p>[source]</p>"},{"location":"generated/api/connection_api/#get_feature_store","title":"get_feature_store","text":"<pre><code>Connection.get_feature_store(name=None)\n</code></pre> <p>Get a reference to a feature store to perform operations on.</p> <p>Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required.</p> <p>How to get feature store instance</p> <pre><code>import hsfs\nconn = hsfs.connection()\nfs = conn.get_feature_store()\n\n# or\n\nimport hopsworks\nproject = hopsworks.login()\nfs = project.get_feature_store()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: The name of the feature store, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>FeatureStore</code>. A feature store handle object to perform operations on.</p>"},{"location":"generated/api/embedding_feature_api/","title":"EmbeddingFeature","text":"<p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#embeddingfeature_1","title":"EmbeddingFeature","text":"<pre><code>hsfs.embedding.EmbeddingFeature(\n    name=None,\n    dimension=None,\n    similarity_function_type=\"l2_norm\",\n    model=None,\n    feature_group=None,\n    embedding_index=None,\n)\n</code></pre> <p>Represents an embedding feature.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: The name of the embedding feature.</li> <li>dimension <code>int | None</code>: The dimensionality of the embedding feature.</li> <li>similarity_function_type <code>hsfs.embedding.SimilarityFunctionType</code>: The type of similarity function used for the embedding feature.   Available functions are <code>L2</code>, <code>COSINE</code>, and <code>DOT_PRODUCT</code>.   (default is <code>SimilarityFunctionType.L2</code>).</li> <li>model: <code>hsml.model.Model</code> A Model in hsml.</li> <li>feature_group: The feature group object that contains the embedding feature.</li> <li>embedding_index: <code>EmbeddingIndex</code> The index for managing embedding features.</li> </ul>"},{"location":"generated/api/embedding_feature_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#dimenstion","title":"dimenstion","text":"<p>int: The dimensionality of the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#embedding_index","title":"embedding_index","text":"<p>EmbeddingIndex: The index for managing embedding features.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#feature_group","title":"feature_group","text":"<p>FeatureGroup: The feature group object that contains the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#model","title":"model","text":"<p>hsml.model.Model: The Model in hsml.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#name","title":"name","text":"<p>str: The name of the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_feature_api/#similarity_function_type","title":"similarity_function_type","text":"<p>SimilarityFunctionType: The type of similarity function used for the embedding feature.</p>"},{"location":"generated/api/embedding_index_api/","title":"EmbeddingIndex","text":"<p>[source]</p>"},{"location":"generated/api/embedding_index_api/#embeddingindex_1","title":"EmbeddingIndex","text":"<pre><code>hsfs.embedding.EmbeddingIndex(index_name=None, features=None, col_prefix=None)\n</code></pre> <p>Represents an index for managing embedding features.</p> <p>Arguments</p> <ul> <li>index_name <code>str | None</code>: The name of the embedding index. The name of the project index is used if not provided.</li> <li>features <code>List[hsfs.embedding.EmbeddingFeature] | None</code>: A list of <code>EmbeddingFeature</code> objects for the features that     contain embeddings that should be indexed for similarity search.</li> <li>col_prefix <code>str | None</code>: The prefix to be added to column names when using project index.     It is managed by Hopsworks and should not be provided.</li> </ul> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256)\nembeddings = embedding_index.get_embeddings()\n</code></pre>"},{"location":"generated/api/embedding_index_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/embedding_index_api/#col_prefix","title":"col_prefix","text":"<p>str: The prefix to be added to column names.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#feature_group","title":"feature_group","text":"<p>FeatureGroup: The feature group object that contains the embedding feature.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#index_name","title":"index_name","text":"<p>str: The name of the embedding index.</p>"},{"location":"generated/api/embedding_index_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/embedding_index_api/#add_embedding","title":"add_embedding","text":"<pre><code>EmbeddingIndex.add_embedding(name, dimension, similarity_function_type=\"l2_norm\", model=None)\n</code></pre> <p>Adds a new embedding feature to the index.</p> <p>Example: <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256)\n\n# Attach a hsml model to the embedding feature\nembedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=256, model=hsml_model)\n</code></pre></p> <p>Arguments</p> <ul> <li>name <code>str</code>: The name of the embedding feature.</li> <li>dimension <code>int</code>: The dimensionality of the embedding feature.</li> <li>similarity_function_type <code>hsfs.embedding.SimilarityFunctionType | None</code>: The type of similarity function to be used.</li> <li>model (hsml.model.Model, optional): The hsml model used to generate the embedding.     Defaults to None.</li> </ul> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#count","title":"count","text":"<pre><code>EmbeddingIndex.count(options=None)\n</code></pre> <p>Count the number of records in the feature group.</p> <p>Arguments</p> <ul> <li>options <code>map | None</code>: The options used for the request to the vector database.     The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</li> </ul> <p>Returns</p> <p>int: The number of records in the feature group.</p> <p>Raises:</p> <p>ValueError: If the feature group is not initialized. FeaturestoreException: If an error occurs during the count operation.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#get_embedding","title":"get_embedding","text":"<pre><code>EmbeddingIndex.get_embedding(name)\n</code></pre> <p>Returns the <code>hsfs.embedding.EmbeddingFeature</code> object associated with the feature name.</p> <p>Arguments</p> <ul> <li>name (str): The name of the embedding feature.</li> </ul> <p>Returns</p> <p><code>hsfs.embedding.EmbeddingFeature</code> object</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#get_embeddings","title":"get_embeddings","text":"<pre><code>EmbeddingIndex.get_embeddings()\n</code></pre> <p>Returns the list of <code>hsfs.embedding.EmbeddingFeature</code> objects associated with the index.</p> <p>Returns</p> <p>A list of <code>hsfs.embedding.EmbeddingFeature</code> objects</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#json","title":"json","text":"<pre><code>EmbeddingIndex.json()\n</code></pre> <p>Serialize the EmbeddingIndex object to a JSON string.</p> <p>[source]</p>"},{"location":"generated/api/embedding_index_api/#to_dict","title":"to_dict","text":"<pre><code>EmbeddingIndex.to_dict()\n</code></pre> <p>Convert the EmbeddingIndex object to a dictionary.</p> <p>Returns:     dict: A dictionary representation of the EmbeddingIndex object.</p>"},{"location":"generated/api/expectation_api/","title":"Expectation","text":"<p>{{expectation}}</p>"},{"location":"generated/api/expectation_api/#properties","title":"Properties","text":"<p>{{expectation_properties}}</p>"},{"location":"generated/api/expectation_api/#methods","title":"Methods","text":"<p>{{expectation_methods}}</p>"},{"location":"generated/api/expectation_api/#creation","title":"Creation","text":"<p>{{expectation_create}}</p>"},{"location":"generated/api/expectation_api/#retrieval","title":"Retrieval","text":"<p>{{expectation_getall}}</p> <p>{{expectation_get}}</p>"},{"location":"generated/api/expectation_suite_api/","title":"Expectation Suite","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#expectationsuite","title":"ExpectationSuite","text":"<pre><code>hsfs.expectation_suite.ExpectationSuite(\n    expectation_suite_name,\n    expectations,\n    meta,\n    id=None,\n    data_asset_type=None,\n    ge_cloud_id=None,\n    run_validation=True,\n    validation_ingestion_policy=\"ALWAYS\",\n    feature_store_id=None,\n    feature_group_id=None,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    type=None,\n    created=None,\n    **kwargs\n)\n</code></pre> <p>Metadata object representing an feature validation expectation in the Feature Store.</p>"},{"location":"generated/api/expectation_suite_api/#creation-with-great-expectations","title":"Creation with Great Expectations","text":"<pre><code>import great_expectations as ge\n\nexpectation_suite = ge.core.ExpectationSuite(\n    \"new_expectation_suite\",\n    expectations=[\n        ge.core.ExpectationConfiguration(\n            expectation_type=\"expect_column_max_to_be_between\",\n            kwargs={\n                \"column\": \"feature\",\n                \"min_value\": -1,\n                \"max_value\": 1\n            }\n        )\n    ]\n)\n</code></pre>"},{"location":"generated/api/expectation_suite_api/#attach-to-feature-group","title":"Attach to Feature Group","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#save_expectation_suite","title":"save_expectation_suite","text":"<pre><code>FeatureGroup.save_expectation_suite(\n    expectation_suite, run_validation=True, validation_ingestion_policy=\"ALWAYS\", overwrite=False\n)\n</code></pre> <p>Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.save_expectation_suite(expectation_suite, run_validation=True)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite</code>: The expectation suite to attach to the Feature Group.</li> <li>overwrite <code>bool</code>: If an Expectation Suite is already attached, overwrite it.     The new suite will have its own validation history, but former reports are preserved.</li> <li>run_validation <code>bool</code>: Set whether the expectation_suite will run on ingestion</li> <li>validation_ingestion_policy <code>str</code>: Set the policy for ingestion to the Feature Group.<ul> <li>\"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group.</li> <li>\"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result.</li> </ul> </li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p>"},{"location":"generated/api/expectation_suite_api/#single-expectation-api","title":"Single Expectation API","text":"<p>An API to edit the expectation list based on Great Expectations API.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#add_expectation","title":"add_expectation","text":"<pre><code>ExpectationSuite.add_expectation(expectation, ge_type=True)\n</code></pre> <p>Append an expectation to the local suite or in the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code># check if the minimum value of specific column is within a range of 0 and 1\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\n# check if the length of specific column value is within a range of 3 and 10\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The new expectation object.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True.</li> </ul> <p>Returns</p> <p>The new expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#replace_expectation","title":"replace_expectation","text":"<pre><code>ExpectationSuite.replace_expectation(expectation, ge_type=True)\n</code></pre> <p>Update an expectation from the suite locally or from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>updated_expectation = expectation_suite.replace_expectation(new_expectation_object)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The updated expectation object. The meta field should contain an expectationId field.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True.</li> </ul> <p>Returns</p> <p>The updated expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#remove_expectation","title":"remove_expectation","text":"<pre><code>ExpectationSuite.remove_expectation(expectation_id=None)\n</code></pre> <p>Remove an expectation from the suite locally and from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>expectation_suite.remove_expectation(expectation_id=123)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int | None</code>: Id of the expectation to remove. The expectation will be deleted both locally and from the backend.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p>"},{"location":"generated/api/expectation_suite_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#data_asset_type","title":"data_asset_type","text":"<p>Data asset type of the expectation suite, not used by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#expectation_suite_name","title":"expectation_suite_name","text":"<p>Name of the expectation suite.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#expectations","title":"expectations","text":"<p>List of expectations to run at validation.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#ge_cloud_id","title":"ge_cloud_id","text":"<p>ge_cloud_id of the expectation suite, not used by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#ge_cloud_id_1","title":"ge_cloud_id","text":"<p>ge_cloud_id of the expectation suite, not used by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#id","title":"id","text":"<p>Id of the expectation suite, set by backend.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#meta","title":"meta","text":"<p>Meta field of the expectation suite to store additional informations.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#run_validation","title":"run_validation","text":"<p>Boolean to determine whether or not the expectation suite shoudl run on ingestion.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#validation_ingestion_policy","title":"validation_ingestion_policy","text":"<p>Whether to ingest a df based on the validation result.</p> <p>\"STRICT\" : ingest df only if all expectations succeed, \"ALWAYS\" : always ingest df, even if one or more expectations fail</p>"},{"location":"generated/api/expectation_suite_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#add_expectation_1","title":"add_expectation","text":"<pre><code>ExpectationSuite.add_expectation(expectation, ge_type=True)\n</code></pre> <p>Append an expectation to the local suite or in the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code># check if the minimum value of specific column is within a range of 0 and 1\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_min_to_be_between\",\n        kwargs={\n            \"column\": \"foo_id\",\n            \"min_value\": 0,\n            \"max_value\": 1\n        }\n    )\n)\n\n# check if the length of specific column value is within a range of 3 and 10\nexpectation_suite.add_expectation(\n    ge.core.ExpectationConfiguration(\n        expectation_type=\"expect_column_value_lengths_to_be_between\",\n        kwargs={\n            \"column\": \"bar_name\",\n            \"min_value\": 3,\n            \"max_value\": 10\n        }\n    )\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The new expectation object.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True.</li> </ul> <p>Returns</p> <p>The new expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#from_ge_type","title":"from_ge_type","text":"<pre><code>ExpectationSuite.from_ge_type(\n    ge_expectation_suite,\n    run_validation=True,\n    validation_ingestion_policy=\"ALWAYS\",\n    id=None,\n    feature_store_id=None,\n    feature_group_id=None,\n)\n</code></pre> <p>Used to create a Hopsworks Expectation Suite instance from a great_expectations instance.</p> <p>Arguments</p> <ul> <li>ge_expectation_suite <code>great_expectations.core.expectation_suite.ExpectationSuite</code>: great_expectations.ExpectationSuite     The great_expectations ExpectationSuite instance to convert to a Hopsworks ExpectationSuite.</li> <li>run_validation <code>bool</code>: bool     Whether to run validation on inserts when the expectation suite is attached.</li> <li>validation_ingestion_policy <code>str</code>: str     The validation ingestion policy to use when the expectation suite is attached. Defaults to \"ALWAYS\".     Options are \"STRICT\" or \"ALWAYS\".</li> <li>id <code>int | None</code>: int     The id of the expectation suite in Hopsworks. If not provided, a new expectation suite will be created.</li> <li>feature_store_id <code>int | None</code>: int     The id of the feature store of the feature group to which the expectation suite belongs.</li> <li>feature_group_id <code>int | None</code>: int     The id of the feature group to which the expectation suite belongs.</li> </ul> <p>Returns</p> <p>Hopsworks Expectation Suite instance.</p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#from_response_json","title":"from_response_json","text":"<pre><code>ExpectationSuite.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#get_expectation","title":"get_expectation","text":"<pre><code>ExpectationSuite.get_expectation(expectation_id, ge_type=True)\n</code></pre> <p>Fetch expectation with expectation_id from the backend.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexpectation_suite = fg.get_expectation_suite()\nselected_expectation = expectation_suite.get_expectation(expectation_id=123)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int</code>: Id of the expectation to fetch from the backend.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True.</li> </ul> <p>Returns</p> <p>The expectation with expectation_id registered in the backend.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#json","title":"json","text":"<pre><code>ExpectationSuite.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#remove_expectation_1","title":"remove_expectation","text":"<pre><code>ExpectationSuite.remove_expectation(expectation_id=None)\n</code></pre> <p>Remove an expectation from the suite locally and from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>expectation_suite.remove_expectation(expectation_id=123)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int | None</code>: Id of the expectation to remove. The expectation will be deleted both locally and from the backend.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#replace_expectation_1","title":"replace_expectation","text":"<pre><code>ExpectationSuite.replace_expectation(expectation, ge_type=True)\n</code></pre> <p>Update an expectation from the suite locally or from the backend if attached to a Feature Group.</p> <p>Example</p> <pre><code>updated_expectation = expectation_suite.replace_expectation(new_expectation_object)\n</code></pre> <p>Arguments</p> <ul> <li>expectation <code>hsfs.ge_expectation.GeExpectation | great_expectations.core.expectation_configuration.ExpectationConfiguration</code>: The updated expectation object. The meta field should contain an expectationId field.</li> <li>ge_type <code>bool</code>: Whether to return native Great Expectations object or Hopsworks abstraction, defaults to True.</li> </ul> <p>Returns</p> <p>The updated expectation attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code></p> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#to_dict","title":"to_dict","text":"<pre><code>ExpectationSuite.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#to_ge_type","title":"to_ge_type","text":"<pre><code>ExpectationSuite.to_ge_type()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/expectation_suite_api/#to_json_dict","title":"to_json_dict","text":"<pre><code>ExpectationSuite.to_json_dict(decamelize=False)\n</code></pre>"},{"location":"generated/api/external_feature_group_api/","title":"ExternalFeatureGroup","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#externalfeaturegroup_1","title":"ExternalFeatureGroup","text":"<pre><code>hsfs.feature_group.ExternalFeatureGroup(\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=None,\n    options=None,\n    name=None,\n    version=None,\n    description=None,\n    primary_key=None,\n    featurestore_id=None,\n    featurestore_name=None,\n    created=None,\n    creator=None,\n    id=None,\n    features=None,\n    location=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    href=None,\n    online_topic_name=None,\n    topic_name=None,\n    notification_topic_name=None,\n    spine=False,\n    deprecated=False,\n    embedding_index=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/external_feature_group_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#create_external_feature_group","title":"create_external_feature_group","text":"<pre><code>FeatureStore.create_external_feature_group(\n    name,\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=\"\",\n    options=None,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    embedding_index=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    topic_name=None,\n    notification_topic_name=None,\n)\n</code></pre> <p>Create a external feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.create_external_feature_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    query=query,\n                    storage_connector=connector,\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date'\n                    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually:</p> <pre><code>external_fg = fs.create_external_feature_group(\n            name=\"sales\",\n            version=1,\n            description=\"Physical shop sales features\",\n            query=query,\n            storage_connector=connector,\n            primary_key=['ss_store_sk'],\n            event_time='sale_date',\n            online_enabled=True\n            )\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to create.</li> <li>storage_connector <code>hsfs.StorageConnector</code>: the storage connector used to establish connectivity     with the data source.</li> <li>query <code>str | None</code>: A string containing a SQL query valid for the target data source.     the query will be used to pull data from the data sources when the     feature group is used.</li> <li>data_format <code>str | None</code>: If the external feature groups refers to a directory with data,     the data format to use when reading it</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> <li>options <code>Dict[str, str] | None</code>: Additional options to be used by the engine when reading data from the     specified storage connector. For example, <code>{\"header\": True}</code> when reading     CSV files with column names in the first row.</li> <li>version <code>int | None</code>: Version of the external feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the external feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the external feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this external feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ expectation_suite__: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</p> </li> <li>online_enabled <code>bool | None</code>: Define whether it should be possible to sync the feature group to     the online feature store for low latency access, defaults to <code>False</code>.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>. The external feature group metadata object.</p>"},{"location":"generated/api/external_feature_group_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_external_feature_group","title":"get_external_feature_group","text":"<pre><code>FeatureStore.get_external_feature_group(name, version=None)\n</code></pre> <p>Get a external feature group entity from the feature store.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.get_external_feature_group(\"external_fg_test\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> <li>version <code>int</code>: Version of the external feature group to retrieve,     defaults to <code>None</code> and will return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: The external feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul>"},{"location":"generated/api/external_feature_group_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#avro_schema","title":"avro_schema","text":"<p>Avro schema representation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#created","title":"created","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#creator","title":"creator","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#data_format","title":"data_format","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#deprecated","title":"deprecated","text":"<p>Setting if the feature group is deprecated.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#description","title":"description","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#embedding_index","title":"embedding_index","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#event_time","title":"event_time","text":"<p>Event time feature in the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#expectation_suite","title":"expectation_suite","text":"<p>Expectation Suite configuration object defining the settings for data validation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#feature_store","title":"feature_store","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#features","title":"features","text":"<p>Feature Group schema (alias)</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#id","title":"id","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#location","title":"location","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#name","title":"name","text":"<p>Name of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#notification_topic_name","title":"notification_topic_name","text":"<p>The topic used for feature group notifications.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#online_enabled","title":"online_enabled","text":"<p>Setting if the feature group is available in online storage.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#options","title":"options","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#path","title":"path","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#primary_key","title":"primary_key","text":"<p>List of features building the primary key.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#query","title":"query","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#schema","title":"schema","text":"<p>Feature Group schema</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#statistics","title":"statistics","text":"<p>Get the latest computed statistics for the whole feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#statistics_config","title":"statistics_config","text":"<p>Statistics configuration object defining the settings for statistics computation of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#storage_connector","title":"storage_connector","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#subject","title":"subject","text":"<p>Subject of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#topic_name","title":"topic_name","text":"<p>The topic used for feature group data ingestion.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#version","title":"version","text":"<p>Version number of the feature group.</p>"},{"location":"generated/api/external_feature_group_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#add_tag","title":"add_tag","text":"<pre><code>ExternalFeatureGroup.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature group.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.add_tag(name=\"example_tag\", value=\"42\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#append_features","title":"append_features","text":"<pre><code>ExternalFeatureGroup.append_features(features)\n</code></pre> <p>Append features to the schema of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# define features to be inserted in the feature group\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.append_features(features)\n</code></pre> <p>Safe append</p> <p>This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema.</p> <p>It is only possible to append features to a feature group. Removing features is considered a breaking change. Note that feature views built on top of this feature group will not read appended feature data. Create a new feature view based on an updated query via <code>fg.select</code> to include the new features.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: Feature or list. A feature object or list thereof to append to     the schema of the feature group.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#check_deprecated","title":"check_deprecated","text":"<pre><code>ExternalFeatureGroup.check_deprecated()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#compute_statistics","title":"compute_statistics","text":"<pre><code>ExternalFeatureGroup.compute_statistics()\n</code></pre> <p>Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nstatistics_metadata = fg.compute_statistics()\n</code></pre> <p>Returns</p> <p><code>Statistics</code>. The statistics metadata object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. Unable to persist the statistics. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>ExternalFeatureGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>ExternalFeatureGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#delete","title":"delete","text":"<pre><code>ExternalFeatureGroup.delete()\n</code></pre> <p>Drop the entire feature group along with its feature data.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(\n        name='bitcoin_price',\n        version=1\n        )\n\n# delete the feature group\nfg.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#delete_expectation_suite","title":"delete_expectation_suite","text":"<pre><code>ExternalFeatureGroup.delete_expectation_suite()\n</code></pre> <p>Delete the expectation suite attached to the Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_expectation_suite()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#delete_tag","title":"delete_tag","text":"<pre><code>ExternalFeatureGroup.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#filter","title":"filter","text":"<pre><code>ExternalFeatureGroup.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\n# connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.filter(Feature(\"weekly_sales\") &gt; 1000)\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group:</p> <p>Example</p> <pre><code>fg.filter(fg.feature1 == 1).show(10)\n</code></pre> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...):</p> <p>Example</p> <pre><code>fg.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#find_neighbors","title":"find_neighbors","text":"<pre><code>ExternalFeatureGroup.find_neighbors(embedding, col=None, k=10, filter=None, options=None)\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> <p>Arguments</p> <ul> <li>embedding <code>List[int | float]</code>: The target embedding for which neighbors are to be found.</li> <li>col <code>str | None</code>: The column name used to compute similarity score. Required only if there are multiple embeddings (optional).</li> <li>k <code>int | None</code>: The number of nearest neighbors to retrieve (default is 10).</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: A filter expression to restrict the search space (optional).</li> <li>options <code>dict | None</code>: The options used for the request to the vector database.     The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</li> </ul> <p>Returns</p> <p>A list of tuples representing the nearest neighbors. Each tuple contains: <code>(The similarity score, A list of feature values)</code></p> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index = embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#from_response_json","title":"from_response_json","text":"<pre><code>ExternalFeatureGroup.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_all_statistics","title":"get_all_statistics","text":"<pre><code>ExternalFeatureGroup.get_all_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns all the statistics metadata computed before a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, all the statistics metadata are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_all_validation_reports","title":"get_all_validation_reports","text":"<pre><code>ExternalFeatureGroup.get_all_validation_reports(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nval_reports = fg.get_all_validation_reports()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p>Union[List[<code>ValidationReport</code>], <code>ValidationReport</code>]. All validation reports attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_complex_features","title":"get_complex_features","text":"<pre><code>ExternalFeatureGroup.get_complex_features()\n</code></pre> <p>Returns the names of all features with a complex data type in this feature group.</p> <p>Example</p> <pre><code>complex_dtype_features = fg.get_complex_features()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_expectation_suite","title":"get_expectation_suite","text":"<pre><code>ExternalFeatureGroup.get_expectation_suite(ge_type=True)\n</code></pre> <p>Return the expectation suite attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexp_suite = fg.get_expectation_suite()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p><code>ExpectationSuite</code>. The expectation suite attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_feature","title":"get_feature","text":"<pre><code>ExternalFeatureGroup.get_feature(name)\n</code></pre> <p>Retrieve a <code>Feature</code> object from the schema of the feature group.</p> <p>There are several ways to access features of a feature group:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get Feature instanse\nfg.feature1\nfg[\"feature1\"]\nfg.get_feature(\"feature1\")\n</code></pre> <p>Note</p> <p>Attribute access to features works only for non-reserved names. For example features named <code>id</code> or <code>name</code> will not be accessible via <code>fg.name</code>, instead this will return the name of the feature group itself. Fall back on using the <code>get_feature</code> method.</p> <p>Arguments:</p> <p>name: The name of the feature to retrieve</p> <p>Returns:</p> <p>Feature: The feature object</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>ExternalFeatureGroup.get_feature_monitoring_configs(\n    name=None, feature_name=None, config_id=None\n)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>ExternalFeatureGroup.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fg.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n\n# fetch feature monitoring history for a given feature monitoring config id\nfm_history = fg.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_fg_name","title":"get_fg_name","text":"<pre><code>ExternalFeatureGroup.get_fg_name()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_generated_feature_groups","title":"get_generated_feature_groups","text":"<pre><code>ExternalFeatureGroup.get_generated_feature_groups()\n</code></pre> <p>Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_generated_feature_views","title":"get_generated_feature_views","text":"<pre><code>ExternalFeatureGroup.get_generated_feature_views()\n</code></pre> <p>Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_latest_validation_report","title":"get_latest_validation_report","text":"<pre><code>ExternalFeatureGroup.get_latest_validation_report(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the Feature Group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nlatest_val_report = fg.get_latest_validation_report()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p><code>ValidationReport</code>. The latest validation report attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>ExternalFeatureGroup.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_statistics","title":"get_statistics","text":"<pre><code>ExternalFeatureGroup.get_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns the statistics computed at a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, the most recent statistics are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>ExternalFeatureGroup.get_storage_connector()\n</code></pre> <p>Get the storage connector using this feature group, based on explicit provenance. Only the accessible storage connector is returned. For more items use the base method - get_storage_connector_provenance</p> <p>Returns</p> <p>`StorageConnector: Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_storage_connector_provenance","title":"get_storage_connector_provenance","text":"<pre><code>ExternalFeatureGroup.get_storage_connector_provenance()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are storage connectors. These storage connector can be accessible, deleted or inaccessible. For deleted and inaccessible storage connector, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the storage connector used to generated this feature group</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_tag","title":"get_tag","text":"<pre><code>ExternalFeatureGroup.get_tag(name)\n</code></pre> <p>Get the tags of a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_tag_value = fg.get_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_tags","title":"get_tags","text":"<pre><code>ExternalFeatureGroup.get_tags()\n</code></pre> <p>Retrieves all tags attached to a feature group.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#get_validation_history","title":"get_validation_history","text":"<pre><code>ExternalFeatureGroup.get_validation_history(\n    expectation_id,\n    start_validation_time=None,\n    end_validation_time=None,\n    filter_by=None,\n    ge_type=True,\n)\n</code></pre> <p>Fetch validation history of an Expectation specified by its id.</p> <p>Example</p> <pre><code>validation_history = fg.get_validation_history(\n    expectation_id=1,\n    filter_by=[\"REJECTED\", \"UNKNOWN\"],\n    start_validation_time=\"2022-01-01 00:00:00\",\n    end_validation_time=datetime.datetime.now(),\n    ge_type=False\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int</code>: id of the Expectation for which to fetch the validation history</li> <li>filter_by <code>List[str] | None</code>: list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\".</li> <li>start_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> <li>end_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>Return</p> <p>Union[List[<code>ValidationResult</code>], List[<code>ExpectationValidationResult</code>]] A list of validation result connected to the expectation_id</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#insert","title":"insert","text":"<pre><code>ExternalFeatureGroup.insert(\n    features, write_options=None, validation_options=None, save_code=True, wait=False\n)\n</code></pre> <p>Insert the dataframe feature values ONLY in the online feature store.</p> <p>External Feature Groups contains metadata about feature data in an external storage system. External storage system are usually offline, meaning feature values cannot be retrieved in real-time. In order to use the feature values for real-time use-cases, you can insert them in Hopsoworks Online Feature Store via this method.</p> <p>The Online Feature Store has a single-entry per primary key value, meaining that providing a new value with for a given primary key will overwrite the existing value. No record of the previous value is kept.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the External Feature Group instance\nfg = fs.get_feature_group(name=\"external_sales_records\", version=1)\n\n# get the feature values, e.g reading from csv files in a S3 bucket\nfeature_values = ...\n\n# insert the feature values in the online feature store\nfg.insert(feature_values)\n</code></pre> <p>Note</p> <p>Data Validation via Great Expectation is supported if you have attached an expectation suite to your External Feature Group. However, as opposed to regular Feature Groups, this can lead to discrepancies between the data in the external storage system and the online feature store.</p> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list]</code>: DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation    suite of the feature group should be fetched before every insert.</li> </ul> </li> <li>save_code <code>bool | None</code>: When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create     the feature group or used to insert data to it. When calling the <code>insert</code> method repeatedly     with small batches of data, this can slow down the writes. Use this option to turn off saving     code. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p>Tuple(None, <code>ge.core.ExpectationSuiteValidationResult</code>) The validation report if validation is enabled.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. e.g fail to create feature group, dataframe schema does not match     existing feature group schema, etc. <code>hsfs.client.exceptions.DataValidationException</code>. If data validation fails and the expectation     suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#json","title":"json","text":"<pre><code>ExternalFeatureGroup.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#prepare_spark_location","title":"prepare_spark_location","text":"<pre><code>ExternalFeatureGroup.prepare_spark_location()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#read","title":"read","text":"<pre><code>ExternalFeatureGroup.read(dataframe_type=\"default\", online=False, read_options=None)\n</code></pre> <p>Get the feature group as a DataFrame.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ndf = fg.read()\n</code></pre> <p>Engine Support</p> <p>Spark only</p> <p>Reading an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups.</p> <p>Arguments</p> <ul> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>online <code>bool</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> <li>read_options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs to pass to the spark engine.     Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>numpy.ndarray</code>. A two-dimensional Numpy array. <code>list</code>. A two-dimensional Python list.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#save","title":"save","text":"<pre><code>ExternalFeatureGroup.save()\n</code></pre> <p>Persist the metadata for this external feature group.</p> <p>Without calling this method, your feature group will only exist in your Python Kernel, but not in Hopsworks.</p> <pre><code>query = \"SELECT * FROM sales\"\n\nfg = feature_store.create_external_feature_group(name=\"sales\",\n    version=1,\n    description=\"Physical shop sales features\",\n    query=query,\n    storage_connector=connector,\n    primary_key=['ss_store_sk'],\n    event_time='sale_date'\n)\n\nfg.save()\n\n\n----\n\n&lt;span style=\"float:right;\"&gt;[[source]](https://github.com/logicalclocks/feature-store-api/tree/b0dfcb7d14be2a2b98995933daf6e57f46d26ca0/python/hsfs/feature_group.py#L934)&lt;/span&gt;\n\n### save_expectation_suite\n\n\n```python\nExternalFeatureGroup.save_expectation_suite(\n    expectation_suite, run_validation=True, validation_ingestion_policy=\"ALWAYS\", overwrite=False\n)\n</code></pre> <p>Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.save_expectation_suite(expectation_suite, run_validation=True)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite</code>: The expectation suite to attach to the Feature Group.</li> <li>overwrite <code>bool</code>: If an Expectation Suite is already attached, overwrite it.     The new suite will have its own validation history, but former reports are preserved.</li> <li>run_validation <code>bool</code>: Set whether the expectation_suite will run on ingestion</li> <li>validation_ingestion_policy <code>str</code>: Set the policy for ingestion to the Feature Group.<ul> <li>\"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group.</li> <li>\"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result.</li> </ul> </li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#save_validation_report","title":"save_validation_report","text":"<pre><code>ExternalFeatureGroup.save_validation_report(\n    validation_report, ingestion_result=\"UNKNOWN\", ge_type=True\n)\n</code></pre> <p>Save validation report to hopsworks platform along previous reports of the same Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(..., expectation_suite=expectation_suite)\n\nvalidation_report = great_expectations.from_pandas(\n    my_experimental_features_df,\n    fg.get_expectation_suite()).validate()\n\nfg.save_validation_report(validation_report, ingestion_result=\"EXPERIMENT\")\n</code></pre> <p>Arguments</p> <ul> <li>validation_report <code>Dict[str, Any] | hsfs.validation_report.ValidationReport | great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult</code>: The validation report to attach to the Feature Group.</li> <li>ingestion_result <code>str</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select","title":"select","text":"<pre><code>ExternalFeatureGroup.select(features)\n</code></pre> <p>Select a subset of features of the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select([\"id\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature]</code>: A list of <code>Feature</code> objects or feature names as     strings to be selected.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select_all","title":"select_all","text":"<pre><code>ExternalFeatureGroup.select_all(include_primary_key=True, include_event_time=True)\n</code></pre> <p>Select all features in the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# show first 5 rows\nquery.show(5)\n\n\n# select all features exclude primary key and event time\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\nquery = fg.select_all()\nquery.features\n# [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)]\n\nquery = fg.select_all(include_primary_key=False, include_event_time=False)\nquery.features\n# [Feature('f1', ...), Feature('f2', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>include_primary_key <code>bool | None</code>: If True, include primary key of the feature group     to the feature list. Defaults to True.</li> <li>include_event_time <code>bool | None</code>: If True, include event time of the feature group     to the feature list. Defaults to True.</li> </ul> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#select_except","title":"select_except","text":"<pre><code>ExternalFeatureGroup.select_except(features=None)\n</code></pre> <p>Select all features including primary key and event time feature of the feature group except provided <code>features</code> and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select_except([\"ts\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature] | None</code>: A list of <code>Feature</code> objects or feature names as     strings to be excluded from the selection. Defaults to [],     selecting all features.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#show","title":"show","text":"<pre><code>ExternalFeatureGroup.show(n, online=False)\n</code></pre> <p>Show the first <code>n</code> rows of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# make a query and show top 5 rows\nfg.select(['date','weekly_sales','is_holiday']).show(5)\n</code></pre> <p>Arguments</p> <ul> <li>n <code>int</code>: int. Number of rows to show.</li> <li>online <code>bool</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#to_dict","title":"to_dict","text":"<pre><code>ExternalFeatureGroup.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_deprecated","title":"update_deprecated","text":"<pre><code>ExternalFeatureGroup.update_deprecated(deprecate=True)\n</code></pre> <p>Deprecate the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_deprecated(deprecate=True)\n</code></pre> <p>Safe update</p> <p>This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged.</p> <p>Arguments</p> <ul> <li>deprecate <code>bool</code>: Boolean value identifying if the feature group should be deprecated. Defaults to True.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_description","title":"update_description","text":"<pre><code>ExternalFeatureGroup.update_description(description)\n</code></pre> <p>Update the description of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_description(description=\"Much better description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_feature_description","title":"update_feature_description","text":"<pre><code>ExternalFeatureGroup.update_feature_description(feature_name, description)\n</code></pre> <p>Update the description of a single feature in this feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_feature_description(feature_name=\"min_temp\",\n                              description=\"Much better feature description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: Name of the feature to be updated.</li> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_features","title":"update_features","text":"<pre><code>ExternalFeatureGroup.update_features(features)\n</code></pre> <p>Update metadata of features in this feature group.</p> <p>Currently it's only supported to update the description of a feature.</p> <p>Unsafe update</p> <p>Note that if you use an existing <code>Feature</code> object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: <code>Feature</code> or list of features. A feature object or list thereof to     be updated.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>ExternalFeatureGroup.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_notification_topic_name","title":"update_notification_topic_name","text":"<pre><code>ExternalFeatureGroup.update_notification_topic_name(notification_topic_name)\n</code></pre> <p>Update the notification topic name of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_notification_topic_name(notification_topic_name=\"notification_topic_name\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name.</p> <p>Arguments</p> <ul> <li>notification_topic_name <code>str</code>: Name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If set to None no notifications are sent.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#update_statistics_config","title":"update_statistics_config","text":"<pre><code>ExternalFeatureGroup.update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the feature group.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_statistics_config()\n</code></pre> <p>Returns</p> <p><code>FeatureGroup</code>. The updated metadata object of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/external_feature_group_api/#validate","title":"validate","text":"<pre><code>ExternalFeatureGroup.validate(\n    dataframe=None,\n    expectation_suite=None,\n    save_report=False,\n    validation_options=None,\n    ingestion_result=\"UNKNOWN\",\n    ge_type=True,\n)\n</code></pre> <p>Run validation based on the attached expectations.</p> <p>Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get feature group instance\nfg = fs.get_or_create_feature_group(...)\n\nge_report = fg.validate(df, save_report=False)\n</code></pre> <p>Arguments</p> <ul> <li>dataframe <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | None</code>: The dataframe to run the data validation expectations against.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | None</code>: Optionally provide an Expectation Suite to override the     one that is possibly attached to the feature group. This is useful for     testing new Expectation suites. When an extra suite is provided, the results     will never be persisted. Defaults to <code>None</code>.</li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>ingestion_result <code>str</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>save_report <code>bool | None</code>: Whether to save the report to the backend. This is only possible if the Expectation suite     is initialised and attached to the Feature Group. Defaults to False.</li> <li>ge_type <code>bool</code>: Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True.</li> </ul> <p>Returns</p> <p>A Validation Report produced by Great Expectations.</p>"},{"location":"generated/api/feature_api/","title":"Feature","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#feature_1","title":"Feature","text":"<pre><code>hsfs.feature.Feature(\n    name,\n    type=None,\n    description=None,\n    primary=False,\n    partition=False,\n    hudi_precombine_key=False,\n    online_type=None,\n    default_value=None,\n    feature_group_id=None,\n    feature_group=None,\n    **kwargs\n)\n</code></pre> <p>Metadata object representing a feature in a feature group in the Feature Store.</p> <p>See Training Dataset Feature for the feature representation of training dataset schemas.</p>"},{"location":"generated/api/feature_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#default_value","title":"default_value","text":"<p>Default value of the feature as string, if the feature was appended to the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#description","title":"description","text":"<p>Description of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#feature_group_id","title":"feature_group_id","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#hudi_precombine_key","title":"hudi_precombine_key","text":"<p>Whether the feature is part of the hudi precombine key of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#name","title":"name","text":"<p>Name of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#online_type","title":"online_type","text":"<p>Data type of the feature in the online feature store.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#partition","title":"partition","text":"<p>Whether the feature is part of the partition key of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#primary","title":"primary","text":"<p>Whether the feature is part of the primary key of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#type","title":"type","text":"<p>Data type of the feature in the offline feature store.</p> <p>Not a Python type</p> <p>This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.</p>"},{"location":"generated/api/feature_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_api/#contains","title":"contains","text":"<pre><code>Feature.contains(other)\n</code></pre> <p>Deprecated</p> <p><code>contains</code> method is deprecated. Use <code>isin</code> instead.</p> <p>[source]</p>"},{"location":"generated/api/feature_api/#from_response_json","title":"from_response_json","text":"<pre><code>Feature.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#is_complex","title":"is_complex","text":"<pre><code>Feature.is_complex()\n</code></pre> <p>Returns true if the feature has a complex type.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nselected_feature = fg.get_feature(\"min_temp\")\nselected_feature.is_complex()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#isin","title":"isin","text":"<pre><code>Feature.isin(other)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#json","title":"json","text":"<pre><code>Feature.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#like","title":"like","text":"<pre><code>Feature.like(other)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_api/#to_dict","title":"to_dict","text":"<pre><code>Feature.to_dict()\n</code></pre> <p>Get structured info about specific Feature in python dictionary format.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nselected_feature = fg.get_feature(\"min_temp\")\nselected_feature.to_dict()\n</code></pre>"},{"location":"generated/api/feature_descriptive_statistics_api/","title":"Feature Descriptive Statistics","text":"<p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#featuredescriptivestatistics","title":"FeatureDescriptiveStatistics","text":"<pre><code>hsfs.core.feature_descriptive_statistics.FeatureDescriptiveStatistics(\n    feature_name,\n    feature_type=None,\n    count=None,\n    completeness=None,\n    num_non_null_values=None,\n    num_null_values=None,\n    approx_num_distinct_values=None,\n    min=None,\n    max=None,\n    sum=None,\n    mean=None,\n    stddev=None,\n    percentiles=None,\n    distinctness=None,\n    entropy=None,\n    uniqueness=None,\n    exact_num_distinct_values=None,\n    extended_statistics=None,\n    id=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_descriptive_statistics_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#approx_num_distinct_values","title":"approx_num_distinct_values","text":"<p>Approximate number of distinct values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#completeness","title":"completeness","text":"<p>Fraction of non-null values in a column.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#count","title":"count","text":"<p>Number of values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#distinctness","title":"distinctness","text":"<p>Fraction of distinct values of a feature over the number of all its values. Distinct values occur at least once.</p> <p>Example</p> <p>[a, a, b] contains two distinct values a and b, so distinctness is 2/3.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#entropy","title":"entropy","text":"<p>Entropy is a measure of the level of information contained in an event (feature value) when considering all possible events (all feature values). Entropy is estimated using observed value counts as the negative sum of (value_count/total_count) * log(value_count/total_count).</p> <p>Example</p> <p>[a, b, b, c, c] has three distinct values with counts [1, 2, 2].</p> <p>Entropy is then (-1/5*log(1/5)-2/5*log(2/5)-2/5*log(2/5)) = 1.055.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#exact_num_distinct_values","title":"exact_num_distinct_values","text":"<p>Exact number of distinct values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#extended_statistics","title":"extended_statistics","text":"<p>Additional statistics computed on the feature values such as histograms and correlations.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#feature_name","title":"feature_name","text":"<p>Name of the feature.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#feature_type","title":"feature_type","text":"<p>Data type of the feature. It can be one of Boolean, Fractional, Integral, or String.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#id","title":"id","text":"<p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#max","title":"max","text":"<p>Maximum value.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#mean","title":"mean","text":"<p>Mean value.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#min","title":"min","text":"<p>Minimum value.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#num_non_null_values","title":"num_non_null_values","text":"<p>Number of non-null values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#num_null_values","title":"num_null_values","text":"<p>Number of null values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#percentiles","title":"percentiles","text":"<p>Percentiles.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#stddev","title":"stddev","text":"<p>Standard deviation of the feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#sum","title":"sum","text":"<p>Sum of all feature values.</p> <p>[source]</p>"},{"location":"generated/api/feature_descriptive_statistics_api/#uniqueness","title":"uniqueness","text":"<p>Fraction of unique values over the number of all values of a column. Unique values occur exactly once.</p> <p>Example</p> <p>[a, a, b] contains one unique value b, so uniqueness is 1/3.</p>"},{"location":"generated/api/feature_group_api/","title":"FeatureGroup","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#featuregroup_1","title":"FeatureGroup","text":"<pre><code>hsfs.feature_group.FeatureGroup(\n    name,\n    version,\n    featurestore_id,\n    description=\"\",\n    partition_key=None,\n    primary_key=None,\n    hudi_precombine_key=None,\n    featurestore_name=None,\n    embedding_index=None,\n    created=None,\n    creator=None,\n    id=None,\n    features=None,\n    location=None,\n    online_enabled=False,\n    time_travel_format=None,\n    statistics_config=None,\n    online_topic_name=None,\n    topic_name=None,\n    notification_topic_name=None,\n    event_time=None,\n    stream=False,\n    expectation_suite=None,\n    parents=None,\n    href=None,\n    delta_streamer_job_conf=None,\n    deprecated=False,\n    storage_connector=None,\n    path=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_group_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#create_feature_group","title":"create_feature_group","text":"<pre><code>FeatureStore.create_feature_group(\n    name,\n    version=None,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    stream=False,\n    expectation_suite=None,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Create a feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.create_feature_group(\n        name='air_quality',\n        description='Air Quality characteristics of each day',\n        version=1,\n        primary_key=['city','date'],\n        online_enabled=True,\n        event_time='date'\n    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>save()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default to <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_or_create_feature_group","title":"get_or_create_feature_group","text":"<pre><code>FeatureStore.get_or_create_feature_group(\n    name,\n    version,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    expectation_suite=None,\n    event_time=None,\n    stream=False,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n        description=\"Electricity prices from NORD POOL\",\n        primary_key=[\"day\", \"area\"],\n        online_enabled=True,\n        event_time=\"timestamp\",\n        )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>insert()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int</code>: Version of the feature group to retrieve or create.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     the vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default is <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p>"},{"location":"generated/api/feature_group_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature_group","title":"get_feature_group","text":"<pre><code>FeatureStore.get_feature_group(name, version=None)\n</code></pre> <p>Get a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n    )\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to get.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>: The feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul>"},{"location":"generated/api/feature_group_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#avro_schema","title":"avro_schema","text":"<p>Avro schema representation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#created","title":"created","text":"<p>Timestamp when the feature group was created.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#creator","title":"creator","text":"<p>Username of the creator.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#deprecated","title":"deprecated","text":"<p>Setting if the feature group is deprecated.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#description","title":"description","text":"<p>Description of the feature group contents.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#embedding_index","title":"embedding_index","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#event_time","title":"event_time","text":"<p>Event time feature in the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#expectation_suite","title":"expectation_suite","text":"<p>Expectation Suite configuration object defining the settings for data validation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#feature_store","title":"feature_store","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#features","title":"features","text":"<p>Feature Group schema (alias)</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","title":"hudi_precombine_key","text":"<p>Feature name that is the hudi precombine key.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#id","title":"id","text":"<p>Feature group id.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#location","title":"location","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#materialization_job","title":"materialization_job","text":"<p>Get the Job object reference for the materialization job for this Feature Group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#name","title":"name","text":"<p>Name of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#notification_topic_name","title":"notification_topic_name","text":"<p>The topic used for feature group notifications.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#online_enabled","title":"online_enabled","text":"<p>Setting if the feature group is available in online storage.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#parents","title":"parents","text":"<p>Parent feature groups as origin of the data in the current feature group. This is part of explicit provenance</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#partition_key","title":"partition_key","text":"<p>List of features building the partition key.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#path","title":"path","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#primary_key","title":"primary_key","text":"<p>List of features building the primary key.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#schema","title":"schema","text":"<p>Feature Group schema</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#statistics","title":"statistics","text":"<p>Get the latest computed statistics for the whole feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#statistics_config","title":"statistics_config","text":"<p>Statistics configuration object defining the settings for statistics computation of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#storage_connector","title":"storage_connector","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#stream","title":"stream","text":"<p>Whether to enable real time stream writing capabilities.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#subject","title":"subject","text":"<p>Subject of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#time_travel_format","title":"time_travel_format","text":"<p>Setting of the feature group time travel format.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#topic_name","title":"topic_name","text":"<p>The topic used for feature group data ingestion.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#version","title":"version","text":"<p>Version number of the feature group.</p>"},{"location":"generated/api/feature_group_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_group_api/#add_tag","title":"add_tag","text":"<pre><code>FeatureGroup.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature group.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.add_tag(name=\"example_tag\", value=\"42\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#append_features","title":"append_features","text":"<pre><code>FeatureGroup.append_features(features)\n</code></pre> <p>Append features to the schema of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# define features to be inserted in the feature group\nfeatures = [\n    Feature(name=\"id\",type=\"int\",online_type=\"int\"),\n    Feature(name=\"name\",type=\"string\",online_type=\"varchar(20)\")\n]\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.append_features(features)\n</code></pre> <p>Safe append</p> <p>This method appends the features to the feature group description safely. In case of failure your local metadata object will contain the correct schema.</p> <p>It is only possible to append features to a feature group. Removing features is considered a breaking change. Note that feature views built on top of this feature group will not read appended feature data. Create a new feature view based on an updated query via <code>fg.select</code> to include the new features.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: Feature or list. A feature object or list thereof to append to     the schema of the feature group.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#as_of","title":"as_of","text":"<pre><code>FeatureGroup.as_of(wallclock_time=None, exclude_until=None)\n</code></pre> <p>Get Query object to retrieve all features of the group at a point in the past.</p> <p>Pyspark/Spark Only</p> <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>This method selects all features in the feature group and returns a Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset.</p> <p>Reading features at a specific point in time:</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get data at a specific point in time and show it\nfg.as_of(\"2020-10-20 07:34:11\").read().show()\n</code></pre> <p>Reading commits incrementally between specified points in time:</p> <pre><code>fg.as_of(\"2020-10-20 07:34:11\", exclude_until=\"2020-10-19 07:34:11\").read().show()\n</code></pre> <p>The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit.</p> <p>Reading only the changes from a single commit</p> <pre><code>fg.as_of(\"2020-10-20 07:31:38\", exclude_until=\"2020-10-20 07:31:37\").read().show()\n</code></pre> <p>When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded.</p> <p>Reading the latest state of features, excluding commits before a specified point in time:</p> <pre><code>fg.as_of(None, exclude_until=\"2020-10-20 07:31:38\").read().show()\n</code></pre> <p>Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\nfg1.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")\n    .join(fg2.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\"))\n</code></pre> <p>If instead you apply another <code>as_of</code> selection after the join, all joined feature groups will be queried with this interval:</p> <p>Example</p> <pre><code>fg1.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")  # as_of is not applied\n    .join(fg2.select_all().as_of(\"2020-10-20\", exclude_until=\"2020-10-15\"))  # as_of is not applied\n    .as_of(\"2020-10-20\", exclude_until=\"2020-10-19\")\n</code></pre> <p>Warning</p> <p>This function only works for feature groups with time_travel_format='HUDI'.</p> <p>Warning</p> <p>Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: <code>hoodie.keep.min.commits</code> and <code>hoodie.keep.max.commits</code> when calling the <code>insert()</code> method.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: Read data as of this point in time. Strings should be formatted in one of the     following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> <li>exclude_until <code>str | int | datetime.datetime | datetime.date | None</code>: Exclude commits until this point in time. String should be formatted in one of the     following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied time travel condition.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#check_deprecated","title":"check_deprecated","text":"<pre><code>FeatureGroup.check_deprecated()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#commit_delete_record","title":"commit_delete_record","text":"<pre><code>FeatureGroup.commit_delete_record(delete_df, write_options=None)\n</code></pre> <p>Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on feature groups stored as HUDI or DELTA.</p> <p>Arguments</p> <ul> <li>delete_df <code>hsfs.feature_group.pyspark.sql.DataFrame</code>: dataFrame containing records to be deleted.</li> <li>write_options <code>Dict[Any, Any] | None</code>: User provided write options. Defaults to <code>{}</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#commit_details","title":"commit_details","text":"<pre><code>FeatureGroup.commit_details(wallclock_time=None, limit=None)\n</code></pre> <p>Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\ncommit_details = fg.commit_details()\n</code></pre> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: Commit details as of specific point in time. Defaults to <code>None</code>.      Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>limit <code>int | None</code>: Number of commits to retrieve. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Dict[str, Dict[str, str]]</code>. Dictionary object of commit metadata timeline, where Key is commit id and value is <code>Dict[str, str]</code> with key value pairs of date committed on, number of rows updated, inserted and deleted.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. If the feature group does not have <code>HUDI</code> time travel format</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#compute_statistics","title":"compute_statistics","text":"<pre><code>FeatureGroup.compute_statistics(wallclock_time=None)\n</code></pre> <p>Recompute the statistics for the feature group and save them to the feature store.</p> <p>Statistics are only computed for data in the offline storage of the feature group.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: If specified will recompute statistics on     feature group as of specific point in time. If not specified then will compute statistics     as of most recent time of this feature group. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. The statistics metadata object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. Unable to persist the statistics.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>FeatureGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>FeatureGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delete","title":"delete","text":"<pre><code>FeatureGroup.delete()\n</code></pre> <p>Drop the entire feature group along with its feature data.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(\n        name='bitcoin_price',\n        version=1\n        )\n\n# delete the feature group\nfg.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delete_expectation_suite","title":"delete_expectation_suite","text":"<pre><code>FeatureGroup.delete_expectation_suite()\n</code></pre> <p>Delete the expectation suite attached to the Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_expectation_suite()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delete_tag","title":"delete_tag","text":"<pre><code>FeatureGroup.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#delta_vacuum","title":"delta_vacuum","text":"<pre><code>FeatureGroup.delta_vacuum(retention_hours=None)\n</code></pre> <p>Vacuum files that are no longer referenced by a Delta table and are older than the retention threshold. This method can only be used on feature groups stored as DELTA.</p> <p>Example</p> <p>```python</p> <p>Arguments</p> <ul> <li>retention_hours <code>int | None</code>: User provided retention period. The default retention threshold for the files is 7 days.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#connect-to-the-feature-store","title":"connect to the Feature Store","text":"<p>fs = ...</p>"},{"location":"generated/api/feature_group_api/#get-the-feature-group-instance","title":"get the Feature Group instance","text":"<p>fg = fs.get_or_create_feature_group(...)</p> <p>commit_details = fg.delta_vacuum(retention_hours = 168)</p>"},{"location":"generated/api/feature_group_api/#filter","title":"filter","text":"<pre><code>FeatureGroup.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\n# connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.filter(Feature(\"weekly_sales\") &gt; 1000)\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group:</p> <p>Example</p> <pre><code>fg.filter(fg.feature1 == 1).show(10)\n</code></pre> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...):</p> <p>Example</p> <pre><code>fg.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#finalize_multi_part_insert","title":"finalize_multi_part_insert","text":"<pre><code>FeatureGroup.finalize_multi_part_insert()\n</code></pre> <p>Finalizes and exits the multi part insert context opened by <code>multi_part_insert</code> in a blocking fashion once all rows have been transmitted.</p> <p>Multi part insert with manual context management</p> <p>Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwhile loop:\n    small_batch_df = ...\n    feature_group.multi_part_insert(small_batch_df)\n\n# IMPORTANT: finalize the multi part insert to make sure all rows\n# have been transmitted\nfeature_group.finalize_multi_part_insert()\n</code></pre> Note that the first call to <code>multi_part_insert</code> initiates the context and be sure to finalize it. The <code>finalize_multi_part_insert</code> is a blocking call that returns once all rows have been transmitted.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#find_neighbors","title":"find_neighbors","text":"<pre><code>FeatureGroup.find_neighbors(embedding, col=None, k=10, filter=None, options=None)\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> <p>Arguments</p> <ul> <li>embedding <code>List[int | float]</code>: The target embedding for which neighbors are to be found.</li> <li>col <code>str | None</code>: The column name used to compute similarity score. Required only if there are multiple embeddings (optional).</li> <li>k <code>int | None</code>: The number of nearest neighbors to retrieve (default is 10).</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: A filter expression to restrict the search space (optional).</li> <li>options <code>dict | None</code>: The options used for the request to the vector database.     The keys are attribute values of the <code>hsfs.core.opensearch.OpensearchRequestOption</code> class.</li> </ul> <p>Returns</p> <p>A list of tuples representing the nearest neighbors. Each tuple contains: <code>(The similarity score, A list of feature values)</code></p> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index = embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#from_response_json","title":"from_response_json","text":"<pre><code>FeatureGroup.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_all_statistics","title":"get_all_statistics","text":"<pre><code>FeatureGroup.get_all_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns all the statistics metadata computed before a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, all the statistics metadata are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_all_validation_reports","title":"get_all_validation_reports","text":"<pre><code>FeatureGroup.get_all_validation_reports(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nval_reports = fg.get_all_validation_reports()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p>Union[List[<code>ValidationReport</code>], <code>ValidationReport</code>]. All validation reports attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_complex_features","title":"get_complex_features","text":"<pre><code>FeatureGroup.get_complex_features()\n</code></pre> <p>Returns the names of all features with a complex data type in this feature group.</p> <p>Example</p> <pre><code>complex_dtype_features = fg.get_complex_features()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_expectation_suite","title":"get_expectation_suite","text":"<pre><code>FeatureGroup.get_expectation_suite(ge_type=True)\n</code></pre> <p>Return the expectation suite attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nexp_suite = fg.get_expectation_suite()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p><code>ExpectationSuite</code>. The expectation suite attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature","title":"get_feature","text":"<pre><code>FeatureGroup.get_feature(name)\n</code></pre> <p>Retrieve a <code>Feature</code> object from the schema of the feature group.</p> <p>There are several ways to access features of a feature group:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get Feature instanse\nfg.feature1\nfg[\"feature1\"]\nfg.get_feature(\"feature1\")\n</code></pre> <p>Note</p> <p>Attribute access to features works only for non-reserved names. For example features named <code>id</code> or <code>name</code> will not be accessible via <code>fg.name</code>, instead this will return the name of the feature group itself. Fall back on using the <code>get_feature</code> method.</p> <p>Arguments:</p> <p>name: The name of the feature to retrieve</p> <p>Returns:</p> <p>Feature: The feature object</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureGroup.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>FeatureGroup.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fg.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n\n# fetch feature monitoring history for a given feature monitoring config id\nfm_history = fg.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_fg_name","title":"get_fg_name","text":"<pre><code>FeatureGroup.get_fg_name()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_generated_feature_groups","title":"get_generated_feature_groups","text":"<pre><code>FeatureGroup.get_generated_feature_groups()\n</code></pre> <p>Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_generated_feature_views","title":"get_generated_feature_views","text":"<pre><code>FeatureGroup.get_generated_feature_views()\n</code></pre> <p>Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_latest_validation_report","title":"get_latest_validation_report","text":"<pre><code>FeatureGroup.get_latest_validation_report(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the Feature Group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nlatest_val_report = fg.get_latest_validation_report()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p><code>ValidationReport</code>. The latest validation report attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>FeatureGroup.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_statistics","title":"get_statistics","text":"<pre><code>FeatureGroup.get_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns the statistics computed at a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, the most recent statistics are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_statistics_by_commit_window","title":"get_statistics_by_commit_window","text":"<pre><code>FeatureGroup.get_statistics_by_commit_window(\n    from_commit_time=None, to_commit_time=None, feature_names=None\n)\n</code></pre> <p>Returns the statistics computed on a specific commit window for this feature group. If time travel is not enabled, it raises an exception.</p> <p>If <code>from_commit_time</code> is <code>None</code>, the commit window starts from the first commit. If <code>to_commit_time</code> is <code>None</code>, the commit window ends at the last commit.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\nfg_statistics = fg.get_statistics_by_commit_window(from_commit_time=None, to_commit_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>to_commit_time <code>str | int | datetime.datetime | datetime.date | None</code>: Date and time of the last commit of the window. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>from_commit_time <code>str | int | datetime.datetime | datetime.date | None</code>: Date and time of the first commit of the window. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>FeatureGroup.get_storage_connector()\n</code></pre> <p>Get the storage connector using this feature group, based on explicit provenance. Only the accessible storage connector is returned. For more items use the base method - get_storage_connector_provenance</p> <p>Returns</p> <p>`StorageConnector: Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_storage_connector_provenance","title":"get_storage_connector_provenance","text":"<pre><code>FeatureGroup.get_storage_connector_provenance()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are storage connectors. These storage connector can be accessible, deleted or inaccessible. For deleted and inaccessible storage connector, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the storage connector used to generated this feature group</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_tag","title":"get_tag","text":"<pre><code>FeatureGroup.get_tag(name)\n</code></pre> <p>Get the tags of a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_tag_value = fg.get_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_tags","title":"get_tags","text":"<pre><code>FeatureGroup.get_tags()\n</code></pre> <p>Retrieves all tags attached to a feature group.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#get_validation_history","title":"get_validation_history","text":"<pre><code>FeatureGroup.get_validation_history(\n    expectation_id,\n    start_validation_time=None,\n    end_validation_time=None,\n    filter_by=None,\n    ge_type=True,\n)\n</code></pre> <p>Fetch validation history of an Expectation specified by its id.</p> <p>Example</p> <pre><code>validation_history = fg.get_validation_history(\n    expectation_id=1,\n    filter_by=[\"REJECTED\", \"UNKNOWN\"],\n    start_validation_time=\"2022-01-01 00:00:00\",\n    end_validation_time=datetime.datetime.now(),\n    ge_type=False\n)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_id <code>int</code>: id of the Expectation for which to fetch the validation history</li> <li>filter_by <code>List[str] | None</code>: list of ingestion_result category to keep. Ooptions are \"INGESTED\", \"REJECTED\", \"FG_DATA\", \"EXPERIMENT\", \"UNKNOWN\".</li> <li>start_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result posterior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> <li>end_validation_time <code>str | int | datetime.datetime | datetime.date | None</code>: fetch only validation result prior to the provided time, inclusive. Supported format include timestamps(int), datetime, date or string formatted to be datutils parsable. See examples above.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>Return</p> <p>Union[List[<code>ValidationResult</code>], List[<code>ExpectationValidationResult</code>]] A list of validation result connected to the expectation_id</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#insert","title":"insert","text":"<pre><code>FeatureGroup.insert(\n    features,\n    overwrite=False,\n    operation=\"upsert\",\n    storage=None,\n    write_options=None,\n    validation_options=None,\n    save_code=True,\n    wait=False,\n)\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group.</p> <p>Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is <code>online_enabled=True</code>.</p> <p>The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, a Polars DataFrame or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is <code>HUDI</code> then <code>operation</code> argument can be either <code>insert</code> or <code>upsert</code>.</p> <p>If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified <code>features</code> dataframe as feature group to the online/offline feature store.</p> <p>Changed in 3.3.0</p> <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Upsert new feature data with time travel format <code>HUDI</code></p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n    name='bitcoin_price',\n    description='Bitcoin price aggregated for days',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n\nfg.insert(df_bitcoin_processed)\n</code></pre> <p>Async insert</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg1 = fs.get_or_create_feature_group(\n    name='feature_group_name1',\n    description='Description of the first FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n# async insertion in order not to wait till finish of the job\nfg.insert(df_for_fg1, write_options={\"wait_for_job\" : False})\n\nfg2 = fs.get_or_create_feature_group(\n    name='feature_group_name2',\n    description='Description of the second FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\nfg.insert(df_for_fg2)\n</code></pre> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list]</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>overwrite <code>bool</code>: Drop all data in the feature group before     inserting new data. This does not affect metadata, defaults to False.</li> <li>operation <code>str | None</code>: Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.     Defaults to <code>\"upsert\"</code>.</li> <li>storage <code>str | None</code>: Overwrite default behaviour, write to offline     storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>, defaults     to <code>None</code> (If the streaming APIs are enabled, specifying the storage option is not supported).</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation    suite of the feature group should be fetched before every insert.</li> </ul> </li> <li>save_code <code>bool | None</code>: When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create     the feature group or used to insert data to it. When calling the <code>insert</code> method repeatedly     with small batches of data, this can slow down the writes. Use this option to turn off saving     code. Defaults to <code>True</code>.</li> <li>wait <code>bool</code>: Wait for job to finish before returning, defaults to <code>False</code>.     Shortcut for read_options <code>{\"wait_for_job\": False}</code>.</li> </ul> <p>Returns</p> <p>(<code>Job</code>, <code>ValidationReport</code>) A tuple with job information if python engine is used and the validation report if validation is enabled.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. e.g fail to create feature group, dataframe schema does not match     existing feature group schema, etc. <code>hsfs.client.exceptions.DataValidationException</code>. If data validation fails and the expectation     suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#insert_stream","title":"insert_stream","text":"<pre><code>FeatureGroup.insert_stream(\n    features,\n    query_name=None,\n    output_mode=\"append\",\n    await_termination=False,\n    timeout=None,\n    checkpoint_dir=None,\n    write_options=None,\n)\n</code></pre> <p>Ingest a Spark Structured Streaming Dataframe to the online feature store.</p> <p>This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments.</p> <p>It is possible to stop the returned query with the <code>.stop()</code> and check its status with <code>.isActive</code>.</p> <p>To get a list of all active queries, use:</p> <pre><code>sqm = spark.streams\n\n# get the list of active streaming queries\n[q.name for q in sqm.active]\n</code></pre> <p>Engine Support</p> <p>Spark only</p> <p>Stream ingestion using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming.</p> <p>Data Validation Support</p> <p><code>insert_stream</code> does not perform any data validation using Great Expectations even when a expectation suite is attached.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature_group.pyspark.sql.DataFrame</code>: Features in Streaming Dataframe to be saved.</li> <li>query_name <code>str | None</code>: It is possible to optionally specify a name for the query to     make it easier to recognise in the Spark UI. Defaults to <code>None</code>.</li> <li>output_mode <code>str | None</code>: Specifies how data of a streaming DataFrame/Dataset is     written to a streaming sink. (1) <code>\"append\"</code>: Only the new rows in the     streaming DataFrame/Dataset will be written to the sink. (2)     <code>\"complete\"</code>: All the rows in the streaming DataFrame/Dataset will be     written to the sink every time there is some update. (3) <code>\"update\"</code>:     only the rows that were updated in the streaming DataFrame/Dataset will     be written to the sink every time there are some updates.     If the query doesn\u2019t contain aggregations, it will be equivalent to     append mode. Defaults to <code>\"append\"</code>.</li> <li>await_termination <code>bool</code>: Waits for the termination of this query, either by     query.stop() or by an exception. If the query has terminated with an     exception, then the exception will be thrown. If timeout is set, it     returns whether the query has terminated or not within the timeout     seconds. Defaults to <code>False</code>.</li> <li>timeout <code>int | None</code>: Only relevant in combination with <code>await_termination=True</code>.     Defaults to <code>None</code>.</li> <li>checkpoint_dir <code>str | None</code>: Checkpoint directory location. This will be used to as a reference to     from where to resume the streaming job. If <code>None</code> then hsfs will construct as     \"insert_stream_\" + online_topic_name. Defaults to <code>None</code>.     write_options: Additional write options for Spark as key-value pairs.     Defaults to <code>{}</code>.</li> </ul> <p>Returns</p> <p><code>StreamingQuery</code>: Spark Structured Streaming Query object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#json","title":"json","text":"<pre><code>FeatureGroup.json()\n</code></pre> <p>Get specific Feature Group metadata in json format.</p> <p>Example</p> <pre><code>fg.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#multi_part_insert","title":"multi_part_insert","text":"<pre><code>FeatureGroup.multi_part_insert(\n    features=None,\n    overwrite=False,\n    operation=\"upsert\",\n    storage=None,\n    write_options=None,\n    validation_options=None,\n)\n</code></pre> <p>Get FeatureGroupWriter for optimized multi part inserts or call this method to start manual multi part optimized inserts.</p> <p>In use cases where very small batches (1 to 1000) rows per Dataframe need to be written to the feature store repeatedly, it might be inefficient to use the standard <code>feature_group.insert()</code> method as it performs some background actions to update the metadata of the feature group object first.</p> <p>For these cases, the feature group provides the <code>multi_part_insert</code> API, which is optimized for writing many small Dataframes after another.</p> <p>There are two ways to use this API:</p> <p>Python Context Manager</p> <p>Using the Python <code>with</code> syntax you can acquire a FeatureGroupWriter object that implements the same <code>multi_part_insert</code> API. <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwith feature_group.multi_part_insert() as writer:\n    # run inserts in a loop:\n    while loop:\n        small_batch_df = ...\n        writer.insert(small_batch_df)\n</code></pre> The writer batches the small Dataframes and transmits them to Hopsworks efficiently. When exiting the context, the feature group writer is sure to exit only once all the rows have been transmitted.</p> <p>Multi part insert with manual context management</p> <p>Instead of letting Python handle the entering and exiting of the multi part insert context, you can start and finalize the context manually. <pre><code>feature_group = fs.get_or_create_feature_group(\"fg_name\", version=1)\n\nwhile loop:\n    small_batch_df = ...\n    feature_group.multi_part_insert(small_batch_df)\n\n# IMPORTANT: finalize the multi part insert to make sure all rows\n# have been transmitted\nfeature_group.finalize_multi_part_insert()\n</code></pre> Note that the first call to <code>multi_part_insert</code> initiates the context and be sure to finalize it. The <code>finalize_multi_part_insert</code> is a blocking call that returns once all rows have been transmitted.</p> <p>Once you are done with the multi part insert, it is good practice to start the materialization job in order to write the data to the offline storage: <pre><code>feature_group.materialization_job.run(await_termination=True)\n</code></pre></p> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list] | None</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>overwrite <code>bool</code>: Drop all data in the feature group before     inserting new data. This does not affect metadata, defaults to False.</li> <li>operation <code>str | None</code>: Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.     Defaults to <code>\"upsert\"</code>.</li> <li>storage <code>str | None</code>: Overwrite default behaviour, write to offline     storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>, defaults     to <code>None</code>.</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job does not get started automatically   for multi part inserts.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>False</code> for multi part inserts,    to control whether the expectation suite of the feature group should be fetched before every insert.</li> </ul> </li> </ul> <p>Returns</p> <p>(<code>Job</code>, <code>ValidationReport</code>) A tuple with job information if python engine is used and the validation report if validation is enabled. <code>FeatureGroupWriter</code> When used as a context manager with Python <code>with</code> statement.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#prepare_spark_location","title":"prepare_spark_location","text":"<pre><code>FeatureGroup.prepare_spark_location()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#read","title":"read","text":"<pre><code>FeatureGroup.read(\n    wallclock_time=None, online=False, dataframe_type=\"default\", read_options=None\n)\n</code></pre> <p>Read the feature group into a dataframe.</p> <p>Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments.</p> <p>Set <code>online</code> to <code>True</code> to read from the online storage, or change <code>dataframe_type</code> to read as a different format.</p> <p>Read feature group as of latest state:</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\nfg.read()\n</code></pre> <p>Read feature group as of specific point in time:</p> <pre><code>fg = fs.get_or_create_feature_group(...)\nfg.read(\"2020-10-20 07:34:11\")\n</code></pre> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: If specified will retrieve feature group as of specific point in time. Defaults to <code>None</code>.     If not specified, will return as of most recent time.     Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>online <code>bool</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.      Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>read_options <code>dict | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to read feature group   with Hive instead of Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code></li> <li>key <code>\"pandas_types\"</code> and value <code>True</code> to retrieve columns as   Pandas nullable types   rather than numpy/object(string) types (experimental). Defaults to <code>{}</code>.</li> </ul> </li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>polars.DataFrame</code>. A Polars DataFrame. <code>numpy.ndarray</code>. A two-dimensional Numpy array. <code>list</code>. A two-dimensional Python list.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. No data is available for feature group with this commit date, If time travel enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#read_changes","title":"read_changes","text":"<pre><code>FeatureGroup.read_changes(start_wallclock_time, end_wallclock_time, read_options=None)\n</code></pre> <p>Reads updates of this feature that occurred between specified points in time.</p> <p>Deprecated</p> <pre><code>    `read_changes` method is deprecated. Use\n    `as_of(end_wallclock_time, exclude_until=start_wallclock_time).read(read_options=read_options)`\n    instead.\n</code></pre> <p>Pyspark/Spark Only</p> <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>Warning</p> <p>This function only works for feature groups with time_travel_format='HUDI'.</p> <p>Arguments</p> <ul> <li>start_wallclock_time <code>str | int | datetime.datetime | datetime.date</code>: Start time of the time travel query. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>end_wallclock_time <code>str | int | datetime.datetime | datetime.date</code>: End time of the time travel query. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>read_options <code>dict | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code> Defaults to <code>{}</code>.</li> </ul> </li> </ul> <p>Returns</p> <p><code>DataFrame</code>. The spark dataframe containing the incremental changes of feature data.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.  No data is available for feature group with this commit date. <code>hsfs.client.exceptions.FeatureStoreException</code>. If the feature group does not have <code>HUDI</code> time travel format</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#save","title":"save","text":"<pre><code>FeatureGroup.save(features=None, write_options=None, validation_options=None, wait=False)\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store.</p> <p>Changed in 3.3.0</p> <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Calling <code>save</code> creates the metadata for the feature group in the feature store. If a Pandas DataFrame, Polars DatFrame, RDD or Ndarray is provided, the data is written to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if <code>online_enabled</code> for the feature group, also to the online feature store. The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[hsfs.feature.Feature] | None</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray or a list of features. Features to be saved.     This argument is optional if the feature list is provided in the create_feature_group or     in the get_or_create_feature_group method invokation.</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it does not wait.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection, consider   changing the producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>wait <code>bool</code>: Wait for job to finish before returning, defaults to <code>False</code>.     Shortcut for read_options <code>{\"wait_for_job\": False}</code>.</li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to ingest the feature group data.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. Unable to create feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#save_expectation_suite","title":"save_expectation_suite","text":"<pre><code>FeatureGroup.save_expectation_suite(\n    expectation_suite, run_validation=True, validation_ingestion_policy=\"ALWAYS\", overwrite=False\n)\n</code></pre> <p>Attach an expectation suite to a feature group and saves it for future use. If an expectation suite is already attached, it is replaced. Note that the provided expectation suite is modified inplace to include expectationId fields.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.save_expectation_suite(expectation_suite, run_validation=True)\n</code></pre> <p>Arguments</p> <ul> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite</code>: The expectation suite to attach to the Feature Group.</li> <li>overwrite <code>bool</code>: If an Expectation Suite is already attached, overwrite it.     The new suite will have its own validation history, but former reports are preserved.</li> <li>run_validation <code>bool</code>: Set whether the expectation_suite will run on ingestion</li> <li>validation_ingestion_policy <code>str</code>: Set the policy for ingestion to the Feature Group.<ul> <li>\"STRICT\" only allows DataFrame passing validation to be inserted into Feature Group.</li> <li>\"ALWAYS\" always insert the DataFrame to the Feature Group, irrespective of overall validation result.</li> </ul> </li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#save_validation_report","title":"save_validation_report","text":"<pre><code>FeatureGroup.save_validation_report(\n    validation_report, ingestion_result=\"UNKNOWN\", ge_type=True\n)\n</code></pre> <p>Save validation report to hopsworks platform along previous reports of the same Feature Group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(..., expectation_suite=expectation_suite)\n\nvalidation_report = great_expectations.from_pandas(\n    my_experimental_features_df,\n    fg.get_expectation_suite()).validate()\n\nfg.save_validation_report(validation_report, ingestion_result=\"EXPERIMENT\")\n</code></pre> <p>Arguments</p> <ul> <li>validation_report <code>Dict[str, Any] | hsfs.validation_report.ValidationReport | great_expectations.core.expectation_validation_result.ExpectationSuiteValidationResult</code>: The validation report to attach to the Feature Group.</li> <li>ingestion_result <code>str</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select","title":"select","text":"<pre><code>FeatureGroup.select(features)\n</code></pre> <p>Select a subset of features of the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select([\"id\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature]</code>: A list of <code>Feature</code> objects or feature names as     strings to be selected.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select_all","title":"select_all","text":"<pre><code>FeatureGroup.select_all(include_primary_key=True, include_event_time=True)\n</code></pre> <p>Select all features in the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# show first 5 rows\nquery.show(5)\n\n\n# select all features exclude primary key and event time\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\nquery = fg.select_all()\nquery.features\n# [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)]\n\nquery = fg.select_all(include_primary_key=False, include_event_time=False)\nquery.features\n# [Feature('f1', ...), Feature('f2', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>include_primary_key <code>bool | None</code>: If True, include primary key of the feature group     to the feature list. Defaults to True.</li> <li>include_event_time <code>bool | None</code>: If True, include event time of the feature group     to the feature list. Defaults to True.</li> </ul> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#select_except","title":"select_except","text":"<pre><code>FeatureGroup.select_except(features=None)\n</code></pre> <p>Select all features including primary key and event time feature of the feature group except provided <code>features</code> and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select_except([\"ts\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature] | None</code>: A list of <code>Feature</code> objects or feature names as     strings to be excluded from the selection. Defaults to [],     selecting all features.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#show","title":"show","text":"<pre><code>FeatureGroup.show(n, online=False)\n</code></pre> <p>Show the first <code>n</code> rows of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# make a query and show top 5 rows\nfg.select(['date','weekly_sales','is_holiday']).show(5)\n</code></pre> <p>Arguments</p> <ul> <li>n <code>int</code>: int. Number of rows to show.</li> <li>online <code>bool | None</code>: bool, optional. If <code>True</code> read from online feature store, defaults     to <code>False</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#to_dict","title":"to_dict","text":"<pre><code>FeatureGroup.to_dict()\n</code></pre> <p>Get structured info about specific Feature Group in python dictionary format.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_deprecated","title":"update_deprecated","text":"<pre><code>FeatureGroup.update_deprecated(deprecate=True)\n</code></pre> <p>Deprecate the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_deprecated(deprecate=True)\n</code></pre> <p>Safe update</p> <p>This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged.</p> <p>Arguments</p> <ul> <li>deprecate <code>bool</code>: Boolean value identifying if the feature group should be deprecated. Defaults to True.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_description","title":"update_description","text":"<pre><code>FeatureGroup.update_description(description)\n</code></pre> <p>Update the description of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_description(description=\"Much better description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_feature_description","title":"update_feature_description","text":"<pre><code>FeatureGroup.update_feature_description(feature_name, description)\n</code></pre> <p>Update the description of a single feature in this feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_feature_description(feature_name=\"min_temp\",\n                              description=\"Much better feature description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: Name of the feature to be updated.</li> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_features","title":"update_features","text":"<pre><code>FeatureGroup.update_features(features)\n</code></pre> <p>Update metadata of features in this feature group.</p> <p>Currently it's only supported to update the description of a feature.</p> <p>Unsafe update</p> <p>Note that if you use an existing <code>Feature</code> object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: <code>Feature</code> or list of features. A feature object or list thereof to     be updated.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>FeatureGroup.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_notification_topic_name","title":"update_notification_topic_name","text":"<pre><code>FeatureGroup.update_notification_topic_name(notification_topic_name)\n</code></pre> <p>Update the notification topic name of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_notification_topic_name(notification_topic_name=\"notification_topic_name\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name.</p> <p>Arguments</p> <ul> <li>notification_topic_name <code>str</code>: Name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If set to None no notifications are sent.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#update_statistics_config","title":"update_statistics_config","text":"<pre><code>FeatureGroup.update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the feature group.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_statistics_config()\n</code></pre> <p>Returns</p> <p><code>FeatureGroup</code>. The updated metadata object of the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_group_api/#validate","title":"validate","text":"<pre><code>FeatureGroup.validate(\n    dataframe=None,\n    expectation_suite=None,\n    save_report=False,\n    validation_options=None,\n    ingestion_result=\"UNKNOWN\",\n    ge_type=True,\n)\n</code></pre> <p>Run validation based on the attached expectations.</p> <p>Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get feature group instance\nfg = fs.get_or_create_feature_group(...)\n\nge_report = fg.validate(df, save_report=False)\n</code></pre> <p>Arguments</p> <ul> <li>dataframe <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | None</code>: The dataframe to run the data validation expectations against.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | None</code>: Optionally provide an Expectation Suite to override the     one that is possibly attached to the feature group. This is useful for     testing new Expectation suites. When an extra suite is provided, the results     will never be persisted. Defaults to <code>None</code>.</li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>ingestion_result <code>str</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>save_report <code>bool | None</code>: Whether to save the report to the backend. This is only possible if the Expectation suite     is initialised and attached to the Feature Group. Defaults to False.</li> <li>ge_type <code>bool</code>: Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True.</li> </ul> <p>Returns</p> <p>A Validation Report produced by Great Expectations.</p>"},{"location":"generated/api/feature_monitoring_config_api/","title":"Feature Monitoring Configuration","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#featuremonitoringconfig","title":"FeatureMonitoringConfig","text":"<pre><code>hsfs.core.feature_monitoring_config.FeatureMonitoringConfig(\n    feature_store_id,\n    name,\n    feature_name=None,\n    feature_monitoring_type=FeatureMonitoringType.STATISTICS_COMPUTATION,\n    job_name=None,\n    detection_window_config=None,\n    reference_window_config=None,\n    statistics_comparison_config=None,\n    job_schedule=None,\n    description=None,\n    id=None,\n    feature_group_id=None,\n    feature_view_name=None,\n    feature_view_version=None,\n    href=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_monitoring_config_api/#creation-from-feature-group","title":"Creation from Feature Group","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>FeatureGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>FeatureGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p>"},{"location":"generated/api/feature_monitoring_config_api/#creation-from-feature-view","title":"Creation from Feature View","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_statistics_monitoring_1","title":"create_statistics_monitoring","text":"<pre><code>FeatureView.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable statistics monitoring\nmy_config = fv._create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature view.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#create_feature_monitoring_1","title":"create_feature_monitoring","text":"<pre><code>FeatureView.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfg = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # compare to a given value\n    specific_value=0.5,\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p>"},{"location":"generated/api/feature_monitoring_config_api/#retrieval-from-feature-group","title":"Retrieval from Feature Group","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureGroup.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p>"},{"location":"generated/api/feature_monitoring_config_api/#retrieval-from-feature-view","title":"Retrieval from Feature View","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_feature_monitoring_configs_1","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureView.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# fetch all feature monitoring configs attached to the feature view\nfm_configs = fv.get_feature_monitoring_configs()\n# fetch a single feature monitoring config by name\nfm_config = fv.get_feature_monitoring_configs(name=\"my_config\")\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fv.get_feature_monitoring_configs(feature_name=\"my_feature\")\n# fetch a single feature monitoring config with a particular id\nfm_config = fv.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p>"},{"location":"generated/api/feature_monitoring_config_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#description","title":"description","text":"<p>Description of the feature monitoring configuration.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#detection_window_config","title":"detection_window_config","text":"<p>Configuration for the detection window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#enabled","title":"enabled","text":"<p>Controls whether or not this config is spawning new feature monitoring jobs. This field belongs to the scheduler configuration but is made transparent to the user for convenience.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_group_id","title":"feature_group_id","text":"<p>Id of the Feature Group to which this feature monitoring configuration is attached.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_monitoring_type","title":"feature_monitoring_type","text":"<p>The type of feature monitoring to perform. Used for internal validation. Options are:     - STATISTICS_COMPUTATION if no reference window (and, therefore, comparison config) is provided     - STATISTICS_COMPARISON if a reference window (and, therefore, comparison config) is provided.</p> <p>This property is read-only.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_name","title":"feature_name","text":"<p>The name of the feature to monitor. If not set, all features of the Feature Group or Feature View are monitored, only available for scheduled statistics.</p> <p>This property is read-only</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_store_id","title":"feature_store_id","text":"<p>Id of the Feature Store.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_view_name","title":"feature_view_name","text":"<p>Name of the Feature View to which this feature monitoring configuration is attached.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#feature_view_version","title":"feature_view_version","text":"<p>Version of the Feature View to which this feature monitoring configuration is attached.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#id","title":"id","text":"<p>Id of the feature monitoring configuration.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#job_name","title":"job_name","text":"<p>Name of the feature monitoring job.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#job_schedule","title":"job_schedule","text":"<p>Schedule of the feature monitoring job. This field belongs to the job configuration but is made transparent to the user for convenience.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#name","title":"name","text":"<p>The name of the feature monitoring config. A Feature Group or Feature View cannot have multiple feature monitoring configurations with the same name. The name of a feature monitoring configuration is limited to 63 characters.</p> <p>This property is read-only once the feature monitoring configuration has been saved.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#reference_window_config","title":"reference_window_config","text":"<p>Configuration for the reference window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#statistics_comparison_config","title":"statistics_comparison_config","text":"<p>Configuration for the comparison of detection and reference statistics.</p>"},{"location":"generated/api/feature_monitoring_config_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#compare_on","title":"compare_on","text":"<pre><code>FeatureMonitoringConfig.compare_on(metric, threshold, strict=False, relative=False)\n</code></pre> <p>Sets the statistics comparison criteria for feature monitoring with a reference window.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring, a detection window and a reference window\nmy_monitoring_config = fg.create_feature_monitoring(\n    ...\n).with_detection_window(...).with_reference_window(...)\n# Choose a metric and set a threshold for the difference\n# e.g compare the relative mean of detection and reference window\nmy_monitoring_config.compare_on(\n    metric=\"mean\",\n    threshold=1.0,\n    relative=True,\n).save()\n</code></pre> <p>Note</p> <p>Detection window and reference window/value/training_dataset must be set prior to comparison configuration.</p> <p>Arguments</p> <ul> <li>metric <code>str | None</code>: The metric to use for comparison. Different metric are available for different feature type.</li> <li>threshold <code>float | None</code>: The threshold to apply to the difference to potentially trigger an alert.</li> <li>strict <code>bool | None</code>: Whether to use a strict comparison (e.g. &gt; or &lt;) or a non-strict comparison (e.g. &gt;= or &lt;=).</li> <li>relative <code>bool | None</code>: Whether to use a relative comparison (e.g. relative mean) or an absolute comparison (e.g. absolute mean).</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#delete","title":"delete","text":"<pre><code>FeatureMonitoringConfig.delete()\n</code></pre> <p>Deletes the feature monitoring configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Delete the feature monitoring config\nmy_monitoring_config.delete()\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#disable","title":"disable","text":"<pre><code>FeatureMonitoringConfig.disable()\n</code></pre> <p>Disables the schedule of the feature monitoring job.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Disable the feature monitoring config\nmy_monitoring_config.disable()\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#enable","title":"enable","text":"<pre><code>FeatureMonitoringConfig.enable()\n</code></pre> <p>Enables the schedule of the feature monitoring job. The scheduler can be configured via the <code>job_schedule</code> property.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Enable the feature monitoring config\nmy_monitoring_config.enable()\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_history","title":"get_history","text":"<pre><code>FeatureMonitoringConfig.get_history(start_time=None, end_time=None, with_statistics=True)\n</code></pre> <p>Fetch the history of the computed statistics and comparison results for this configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Fetch the history of the computed statistics for this configuration\nhistory = my_monitoring_config.get_history(\n    start_time=\"2021-01-01\",\n    end_time=\"2021-01-31\",\n)\n</code></pre> <p>Args:</p> <p>start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results.</p> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#get_job","title":"get_job","text":"<pre><code>FeatureMonitoringConfig.get_job()\n</code></pre> <p>Get the feature monitoring job which computes and compares statistics on the detection and reference windows.</p> <p>Example</p> <pre><code># Fetch registered config by name via feature group or feature view\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Get the job which computes statistics on detection and reference window\njob = my_monitoring_config.get_job()\n# Print job history and ongoing executions\njob.executions\n</code></pre> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>Returns</p> <p><code>Job</code>. A handle for the job computing the statistics.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#run_job","title":"run_job","text":"<pre><code>FeatureMonitoringConfig.run_job()\n</code></pre> <p>Trigger the feature monitoring job which computes and compares statistics on the detection and reference windows.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Trigger the feature monitoring job once\nmy_monitoring_config.run_job()\n</code></pre> <p>Info</p> <p>The feature monitoring job will be triggered asynchronously and the method will return immediately. Calling this method does not affect the ongoing schedule.</p> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul> <p>Returns</p> <p><code>Job</code>. A handle for the job computing the statistics.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#save","title":"save","text":"<pre><code>FeatureMonitoringConfig.save()\n</code></pre> <p>Saves the feature monitoring configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_statistics_monitoring(\n    name=\"my_monitoring_config\",\n).save()\n</code></pre> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The saved FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#update","title":"update","text":"<pre><code>FeatureMonitoringConfig.update()\n</code></pre> <p>Updates allowed fields of the saved feature monitoring configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Update the percentage of rows to use when computing the statistics\nmy_monitoring_config.detection_window.row_percentage = 10\nmy_monitoring_config.update()\n</code></pre> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_detection_window","title":"with_detection_window","text":"<pre><code>FeatureMonitoringConfig.with_detection_window(\n    time_offset=None, window_length=None, row_percentage=None\n)\n</code></pre> <p>Sets the detection window of data to compute statistics on.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Compute statistics on a regular basis\nfg.create_statistics_monitoring(\n    name=\"regular_stats\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    time_offset=\"1d\",\n    window_length=\"1d\",\n    row_percentage=0.1,\n).save()\n# Compute and compare statistics\nfg.create_feature_monitoring(\n    name=\"regular_stats\",\n    feature_name=\"my_feature\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    time_offset=\"1d\",\n    window_length=\"1d\",\n    row_percentage=0.1,\n).with_reference_window(...).compare_on(...).save()\n</code></pre> <p>Arguments</p> <ul> <li>time_offset <code>str | None</code>: The time offset from the current time to the start of the time window.</li> <li>window_length <code>str | None</code>: The length of the time window.</li> <li>row_percentage <code>float | None</code>: The fraction of rows to use when computing the statistics [0, 1.0].</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_training_dataset","title":"with_reference_training_dataset","text":"<pre><code>FeatureMonitoringConfig.with_reference_training_dataset(training_dataset_version=None)\n</code></pre> <p>Sets the reference training dataset to compare statistics with. See also <code>with_reference_value(...)</code> and <code>with_reference_window(...)</code> for other reference options.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Only for feature views: Compare to the statistics computed for one of your training datasets\n# particularly useful if it has been used to train a model currently in production\nmy_monitoring_config.with_reference_training_dataset(\n    training_dataset_version=3,\n).compare_on(...).save()\n</code></pre> <p>Provide a comparison configuration</p> <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: The version of the training dataset to use as reference.</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_value","title":"with_reference_value","text":"<pre><code>FeatureMonitoringConfig.with_reference_value(value=None)\n</code></pre> <p>Sets the reference value to compare statistics with. See also <code>with_reference_window(...)</code> and <code>with_reference_training_dataset(...)</code> for other reference options.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Simplest reference window is a specific value\nmy_monitoring_config.with_reference_value(\n    value=0.0,\n).compare_on(...).save()\n</code></pre> <p>Provide a comparison configuration</p> <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> <p>Arguments</p> <ul> <li>value <code>float | int | None</code>: A float value to use as reference.</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_config_api/#with_reference_window","title":"with_reference_window","text":"<pre><code>FeatureMonitoringConfig.with_reference_window(\n    time_offset=None, window_length=None, row_percentage=None\n)\n</code></pre> <p>Sets the reference window of data to compute statistics on. See also <code>with_reference_value(...)</code> and <code>with_reference_training_dataset(...)</code> for other reference options.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Setup feature monitoring and a detection window\nmy_monitoring_config = fg.create_feature_monitoring(...).with_detection_window(...)\n# Statistics computed on a rolling time window, e.g. same day last week\nmy_monitoring_config.with_reference_window(\n    time_offset=\"1w\",\n    window_length=\"1d\",\n).compare_on(...).save()\n</code></pre> <p>Provide a comparison configuration</p> <p>You must provide a comparison configuration via <code>compare_on()</code> before saving the feature monitoring config.</p> <p>Arguments</p> <ul> <li>time_offset <code>str | None</code>: The time offset from the current time to the start of the time window.</li> <li>window_length <code>str | None</code>: The length of the time window.</li> <li>row_percentage <code>float | None</code>: The percentage of rows to use when computing the statistics. Defaults to 20%.</li> </ul> <p>Returns</p> <p><code>FeatureMonitoringConfig</code>. The updated FeatureMonitoringConfig object.</p>"},{"location":"generated/api/feature_monitoring_result_api/","title":"Feature Monitoring Result","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#featuremonitoringresult","title":"FeatureMonitoringResult","text":"<pre><code>hsfs.core.feature_monitoring_result.FeatureMonitoringResult(\n    feature_store_id,\n    execution_id,\n    monitoring_time,\n    config_id,\n    feature_name,\n    difference=None,\n    shift_detected=False,\n    detection_statistics_id=None,\n    reference_statistics_id=None,\n    empty_detection_window=False,\n    empty_reference_window=False,\n    specific_value=None,\n    raised_exception=False,\n    detection_statistics=None,\n    reference_statistics=None,\n    id=None,\n    href=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_monitoring_result_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#get_history","title":"get_history","text":"<pre><code>FeatureMonitoringConfig.get_history(start_time=None, end_time=None, with_statistics=True)\n</code></pre> <p>Fetch the history of the computed statistics and comparison results for this configuration.</p> <p>Example</p> <pre><code># Fetch your feature group or feature view\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n# Fetch registered config by name\nmy_monitoring_config = fg.get_feature_monitoring_configs(name=\"my_monitoring_config\")\n# Fetch the history of the computed statistics for this configuration\nhistory = my_monitoring_config.get_history(\n    start_time=\"2021-01-01\",\n    end_time=\"2021-01-31\",\n)\n</code></pre> <p>Args:</p> <p>start_time: The start time of the time range to fetch the history for. end_time: The end time of the time range to fetch the history for. with_statistics: Whether to include the computed statistics in the results.</p> <p>Raises</p> <ul> <li><code>FeatureStoreException</code>: If the feature monitoring config has not been saved.</li> </ul>"},{"location":"generated/api/feature_monitoring_result_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#config_id","title":"config_id","text":"<p>Id of the feature monitoring configuration containing this result.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#detection_statistics","title":"detection_statistics","text":"<p>Feature descriptive statistics computed on the detection window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#detection_statistics_id","title":"detection_statistics_id","text":"<p>Id of the feature descriptive statistics computed on the detection window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#difference","title":"difference","text":"<p>Difference between detection and reference values. It can be relative or absolute difference, depending on the statistics comparison configuration provided in <code>relative</code> parameter passed to <code>compare_on()</code> when enabling feature monitoring.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#empty_detection_window","title":"empty_detection_window","text":"<p>Whether or not the detection window was empty in this feature monitoring run.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#empty_reference_window","title":"empty_reference_window","text":"<p>Whether or not the reference window was empty in this feature monitoring run.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#execution_id","title":"execution_id","text":"<p>Execution id of the feature monitoring job.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#feature_name","title":"feature_name","text":"<p>Name of the feature being monitored.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#feature_store_id","title":"feature_store_id","text":"<p>Id of the Feature Store.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#id","title":"id","text":"<p>Id of the feature monitoring result.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#monitoring_time","title":"monitoring_time","text":"<p>Time at which this feature monitoring result was created.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#reference_statistics","title":"reference_statistics","text":"<p>Feature descriptive statistics computed on the reference window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#reference_statistics_id","title":"reference_statistics_id","text":"<p>Id of the feature descriptive statistics computed on the reference window.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#shift_detected","title":"shift_detected","text":"<p>Whether or not shift was detected in the detection window based on the computed statistics and the threshold provided in <code>compare_on()</code> when enabling feature monitoring.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_result_api/#specific_value","title":"specific_value","text":"<p>Specific value used as reference in the statistics comparison.</p>"},{"location":"generated/api/feature_monitoring_window_config_api/","title":"Feature Monitoring Window Configuration","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#monitoringwindowconfig","title":"MonitoringWindowConfig","text":"<pre><code>hsfs.core.monitoring_window_config.MonitoringWindowConfig(\n    id=None,\n    window_config_type=WindowConfigType.SPECIFIC_VALUE,\n    time_offset=None,\n    window_length=None,\n    training_dataset_version=None,\n    specific_value=None,\n    row_percentage=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_monitoring_window_config_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#id","title":"id","text":"<p>Id of the window configuration.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#row_percentage","title":"row_percentage","text":"<p>The percentage of rows to fetch and compute the statistics on. Only used for windows of type <code>ROLLING_TIME</code> and <code>ALL_TIME</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#specific_value","title":"specific_value","text":"<p>The specific value to use as reference. Only used for windows of type <code>SPECIFIC_VALUE</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#time_offset","title":"time_offset","text":"<p>The time offset from the current time to the start of the time window. Only used for windows of type <code>ROLLING_TIME</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#training_dataset_version","title":"training_dataset_version","text":"<p>The version of the training dataset to use as reference. Only used for windows of type <code>TRAINING_DATASET</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#window_config_type","title":"window_config_type","text":"<p>Type of the window. It can be one of <code>ALL_TIME</code>, <code>ROLLING_TIME</code>, <code>TRAINING_DATASET</code> or <code>SPECIFIC_VALUE</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_monitoring_window_config_api/#window_length","title":"window_length","text":"<p>The length of the time window. Only used for windows of type <code>ROLLING_TIME</code>.</p>"},{"location":"generated/api/feature_store_api/","title":"Feature Store","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#featurestore","title":"FeatureStore","text":"<pre><code>hsfs.feature_store.FeatureStore(\n    featurestore_id,\n    featurestore_name,\n    created,\n    project_name,\n    project_id,\n    offline_featurestore_name,\n    online_enabled,\n    num_feature_groups=None,\n    num_training_datasets=None,\n    num_storage_connectors=None,\n    num_feature_views=None,\n    online_featurestore_name=None,\n    online_featurestore_size=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_store_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_store","title":"get_feature_store","text":"<pre><code>Connection.get_feature_store(name=None)\n</code></pre> <p>Get a reference to a feature store to perform operations on.</p> <p>Defaulting to the project name of default feature store. To get a Shared feature stores, the project name of the feature store is required.</p> <p>How to get feature store instance</p> <pre><code>import hsfs\nconn = hsfs.connection()\nfs = conn.get_feature_store()\n\n# or\n\nimport hopsworks\nproject = hopsworks.login()\nfs = project.get_feature_store()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: The name of the feature store, defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>FeatureStore</code>. A feature store handle object to perform operations on.</p>"},{"location":"generated/api/feature_store_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#id","title":"id","text":"<p>Id of the feature store.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#name","title":"name","text":"<p>Name of the feature store.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","title":"offline_featurestore_name","text":"<p>Name of the offline feature store database.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#online_enabled","title":"online_enabled","text":"<p>Indicator whether online feature store is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#online_featurestore_name","title":"online_featurestore_name","text":"<p>Name of the online feature store database.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#project_id","title":"project_id","text":"<p>Id of the project in which the feature store is located.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#project_name","title":"project_name","text":"<p>Name of the project in which the feature store is located.</p>"},{"location":"generated/api/feature_store_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_external_feature_group","title":"create_external_feature_group","text":"<pre><code>FeatureStore.create_external_feature_group(\n    name,\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=\"\",\n    options=None,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    embedding_index=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    topic_name=None,\n    notification_topic_name=None,\n)\n</code></pre> <p>Create a external feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.create_external_feature_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    query=query,\n                    storage_connector=connector,\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date'\n                    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>You can enable online storage for external feature groups, however, the sync from the external storage to Hopsworks online storage needs to be done manually:</p> <pre><code>external_fg = fs.create_external_feature_group(\n            name=\"sales\",\n            version=1,\n            description=\"Physical shop sales features\",\n            query=query,\n            storage_connector=connector,\n            primary_key=['ss_store_sk'],\n            event_time='sale_date',\n            online_enabled=True\n            )\nexternal_fg.save()\n\n# read from external storage and filter data to sync to online\ndf = external_fg.read().filter(external_fg.customer_status == \"active\")\n\n# insert to online storage\nexternal_fg.insert(df)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to create.</li> <li>storage_connector <code>hsfs.StorageConnector</code>: the storage connector used to establish connectivity     with the data source.</li> <li>query <code>str | None</code>: A string containing a SQL query valid for the target data source.     the query will be used to pull data from the data sources when the     feature group is used.</li> <li>data_format <code>str | None</code>: If the external feature groups refers to a directory with data,     the data format to use when reading it</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> <li>options <code>Dict[str, str] | None</code>: Additional options to be used by the engine when reading data from the     specified storage connector. For example, <code>{\"header\": True}</code> when reading     CSV files with column names in the first row.</li> <li>version <code>int | None</code>: Version of the external feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the external feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the external feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this external feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ expectation_suite__: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</p> </li> <li>online_enabled <code>bool | None</code>: Define whether it should be possible to sync the feature group to     the online feature store for low latency access, defaults to <code>False</code>.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>. The external feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_feature_group","title":"create_feature_group","text":"<pre><code>FeatureStore.create_feature_group(\n    name,\n    version=None,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    stream=False,\n    expectation_suite=None,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Create a feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.create_feature_group(\n        name='air_quality',\n        description='Air Quality characteristics of each day',\n        version=1,\n        primary_key=['city','date'],\n        online_enabled=True,\n        event_time='date'\n    )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>save()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default to <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_feature_view","title":"create_feature_view","text":"<pre><code>FeatureStore.create_feature_view(\n    name,\n    query,\n    version=None,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n)\n</code></pre> <p>Create a feature view metadata object and saved it to hopsworks.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the feature group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# get the transformation functions\nstandard_scaler = fs.get_transformation_function(name='standard_scaler')\n\n# construct dictionary of \"feature - transformation function\" pairs\ntransformation_functions = {col_name: standard_scaler for col_name in df.columns}\n\nfeature_view = fs.create_feature_view(\n    name='air_quality_fv',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# define dictionary with column names and transformation functions pairs\nmapping_transformers = ...\n\n# create feature view\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    version=1,\n    transformation_functions=mapping_transformers,\n    query=query\n)\n</code></pre> <p>Warning</p> <p><code>as_of</code> argument in the <code>Query</code> will be ignored because feature view does not support time travel query.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to create.</li> <li>query <code>hsfs.constructor.query.Query</code>: Feature store <code>Query</code>.</li> <li>version <code>int | None</code>: Version of the feature view to create, defaults to <code>None</code> and     will create the feature view with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature view to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>labels <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the feature view. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>inference_helper_columns <code>List[str] | None</code>: A list of feature names that are not used in training the model itself but can be     used during batch or online inference for extra information. Inference helper column name(s) must be     part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name     when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be     omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during     online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</li> <li>training_helper_columns <code>List[str] | None</code>: A list of feature names that are not the part of the model schema itself but can be     used during training as a helper for extra information. Training helper column name(s) must be     part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when     defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the training helper columns will be omitted during both batch and online inference.     Training helper columns can be optionally fetched with training data. For more details see     documentation for feature view's get training data methods.  Defaults to `[], no training helper     columns.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     vector and at inference time. Defaults to <code>{}</code>, no     transformations.</li> </ul> <p>Returns:</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","title":"create_on_demand_feature_group","text":"<pre><code>FeatureStore.create_on_demand_feature_group(\n    name,\n    storage_connector,\n    query=None,\n    data_format=None,\n    path=\"\",\n    options=None,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    features=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    topic_name=None,\n    notification_topic_name=None,\n)\n</code></pre> <p>Create a external feature group metadata object.</p> <p>Deprecated</p> <p><code>create_on_demand_feature_group</code> method is deprecated. Use the <code>create_external_feature_group</code> method instead.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the <code>save()</code> method.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to create.</li> <li>storage_connector <code>hsfs.StorageConnector</code>: the storage connector used to establish connectivity     with the data source.</li> <li>query <code>str | None</code>: A string containing a SQL query valid for the target data source.     the query will be used to pull data from the data sources when the     feature group is used.</li> <li>data_format <code>str | None</code>: If the external feature groups refers to a directory with data,     the data format to use when reading it</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> <li>options <code>Dict[str, str] | None</code>: Additional options to be used by the engine when reading data from the     specified storage connector. For example, <code>{\"header\": True}</code> when reading     CSV files with column names in the first row.</li> <li>version <code>int | None</code>: Version of the external feature group to retrieve, defaults to <code>None</code> and     will create the feature group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the external feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the external feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this external feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li> <p>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ expectation_suite__: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</p> </li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>. The external feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_training_dataset","title":"create_training_dataset","text":"<pre><code>FeatureStore.create_training_dataset(\n    name,\n    version=None,\n    description=\"\",\n    data_format=\"tfrecords\",\n    coalesce=False,\n    storage_connector=None,\n    splits=None,\n    location=\"\",\n    seed=None,\n    statistics_config=None,\n    label=None,\n    transformation_functions=None,\n    train_split=None,\n)\n</code></pre> <p>Create a training dataset metadata object.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. From version 3.0 training datasets created with this API are not visibile in the API anymore.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the <code>save()</code> method with a <code>DataFrame</code> or <code>Query</code>.</p> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to create.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and     will create the training dataset with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"tfrecords\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>splits <code>Dict[str, float] | None</code>: A dictionary defining training dataset splits to be created. Keys in     the dictionary define the name of the split as <code>str</code>, values represent     percentage of samples in the split as <code>float</code>. Currently, only random     splits are supported. Defaults to empty dict<code>{}</code>, creating only a single     training dataset without splits.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>label <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the training dataset. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     training data and at inference time. Defaults to <code>{}</code>, no     transformations.</li> <li>train_split <code>str | None</code>: If <code>splits</code> is set, provide the name of the split that is going     to be used for training. The statistics of this split will be used for     transformation functions if necessary. Defaults to <code>None</code>.</li> </ul> <p>Returns:</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#create_transformation_function","title":"create_transformation_function","text":"<pre><code>FeatureStore.create_transformation_function(transformation_function, output_type, version=None)\n</code></pre> <p>Create a transformation function metadata object.</p> <p>Example</p> <pre><code># define function\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        output_type=int,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the <code>save()</code> method of the transformation function metadata object.</p> <p>Arguments</p> <ul> <li>transformation_function <code>callable</code>: callable object.</li> <li>output_type <code>str | bytes | int | numpy.int8 | numpy.int16 | numpy.int32 | numpy.int64 | float | numpy.float64 | datetime.datetime | numpy.datetime64 | datetime.date | bool</code>: python or numpy output type that will be inferred as pyspark.sql.types type.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#from_response_json","title":"from_response_json","text":"<pre><code>FeatureStore.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_external_feature_group","title":"get_external_feature_group","text":"<pre><code>FeatureStore.get_external_feature_group(name, version=None)\n</code></pre> <p>Get a external feature group entity from the feature store.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fg = fs.get_external_feature_group(\"external_fg_test\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> <li>version <code>int</code>: Version of the external feature group to retrieve,     defaults to <code>None</code> and will return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: The external feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_external_feature_groups","title":"get_external_feature_groups","text":"<pre><code>FeatureStore.get_external_feature_groups(name)\n</code></pre> <p>Get a list of all versions of an external feature group entity from the feature store.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nexternal_fgs_list = fs.get_external_feature_groups(\"external_fg_test\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: List of external feature group metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_group","title":"get_feature_group","text":"<pre><code>FeatureStore.get_feature_group(name, version=None)\n</code></pre> <p>Get a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n    )\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to get.</li> <li>version <code>int | None</code>: Version of the feature group to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>: The feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_groups","title":"get_feature_groups","text":"<pre><code>FeatureStore.get_feature_groups(name)\n</code></pre> <p>Get a list of all versions of a feature group entity from the feature store.</p> <p>Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfgs_list = fs.get_feature_groups(\n        name=\"electricity_prices\"\n    )\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to get.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>: List of feature group metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_view","title":"get_feature_view","text":"<pre><code>FeatureStore.get_feature_view(name, version=None)\n</code></pre> <p>Get a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(\n    name='feature_view_name',\n    version=1\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> <li>version <code>int</code>: Version of the feature view to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_feature_views","title":"get_feature_views","text":"<pre><code>FeatureStore.get_feature_views(name)\n</code></pre> <p>Get a list of all versions of a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get a list of all versions of a feature view\nfeature_view = fs.get_feature_views(\n    name='feature_view_name'\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: List of feature view metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","title":"get_on_demand_feature_group","text":"<pre><code>FeatureStore.get_on_demand_feature_group(name, version=None)\n</code></pre> <p>Get a external feature group entity from the feature store.</p> <p>Deprecated</p> <p><code>get_on_demand_feature_group</code> method is deprecated. Use the <code>get_external_feature_group</code> method instead.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> <li>version <code>int</code>: Version of the external feature group to retrieve,     defaults to <code>None</code> and will return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: The external feature group metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_groups","title":"get_on_demand_feature_groups","text":"<pre><code>FeatureStore.get_on_demand_feature_groups(name)\n</code></pre> <p>Get a list of all versions of an external feature group entity from the feature store.</p> <p>Deprecated</p> <p><code>get_on_demand_feature_groups</code> method is deprecated. Use the <code>get_external_feature_groups</code> method instead.</p> <p>Getting a external feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the <code>Query</code>-API to perform joins between feature groups.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the external feature group to get.</li> </ul> <p>Returns</p> <p><code>ExternalFeatureGroup</code>: List of external feature group metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","title":"get_online_storage_connector","text":"<pre><code>FeatureStore.get_online_storage_connector()\n</code></pre> <p>Get the storage connector for the Online Feature Store of the respective project's feature store.</p> <p>The returned storage connector depends on the project that you are connected to.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nonline_storage_connector = fs.get_online_storage_connector()\n</code></pre> <p>Returns</p> <p><code>StorageConnector</code>. JDBC storage connector to the Online Feature Store.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_or_create_feature_group","title":"get_or_create_feature_group","text":"<pre><code>FeatureStore.get_or_create_feature_group(\n    name,\n    version,\n    description=\"\",\n    online_enabled=False,\n    time_travel_format=\"HUDI\",\n    partition_key=None,\n    primary_key=None,\n    embedding_index=None,\n    hudi_precombine_key=None,\n    features=None,\n    statistics_config=None,\n    expectation_suite=None,\n    event_time=None,\n    stream=False,\n    parents=None,\n    topic_name=None,\n    notification_topic_name=None,\n    storage_connector=None,\n    path=None,\n)\n</code></pre> <p>Get feature group metadata object or create a new one if it doesn't exist. This method doesn't update existing feature group metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n        name=\"electricity_prices\",\n        version=1,\n        description=\"Electricity prices from NORD POOL\",\n        primary_key=[\"day\", \"area\"],\n        online_enabled=True,\n        event_time=\"timestamp\",\n        )\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the <code>insert()</code> method with a DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature group to create.</li> <li>version <code>int</code>: Version of the feature group to retrieve or create.</li> <li>description <code>str | None</code>: A string describing the contents of the feature group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>online_enabled <code>bool | None</code>: Define whether the feature group should be made available     also in the online feature store for low latency access, defaults to     <code>False</code>.</li> <li>time_travel_format <code>str | None</code>: Format used for time travel, defaults to <code>\"HUDI\"</code>.</li> <li>partition_key <code>List[str] | None</code>: A list of feature names to be used as partition key when     writing the feature data to the offline storage, defaults to empty list     <code>[]</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     feature group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the feature group won't have any primary key.</li> <li>embedding_index <code>hsfs.embedding.EmbeddingIndex | None</code>: <code>EmbeddingIndex</code>. If an embedding index is provided,     the vector database is used as online feature store. This enables similarity search by     using <code>find_neighbors</code>.     default is <code>None</code></li> <li>hudi_precombine_key <code>str | None</code>: A feature name to be used as a precombine key for the <code>\"HUDI\"</code>     feature group. Defaults to <code>None</code>. If feature group has time travel format     <code>\"HUDI\"</code> and hudi precombine key was not specified then the first primary key of     the feature group will be used as hudi precombine key.</li> <li>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the feature group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame provided in the <code>save</code> method.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation, <code>\"histograms\"</code> to compute feature value frequencies and     <code>\"exact_uniqueness\"</code> to compute uniqueness, distinctness and entropy.     The values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | great_expectations.core.expectation_suite.ExpectationSuite | None</code>: Optionally, attach an expectation suite to the feature     group which dataframes should be validated against upon insertion.     Defaults to <code>None</code>.</li> <li> <p>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this feature group. If event_time is set     the feature group can be used for point-in-time joins. Defaults to <code>None</code>.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ stream__: Optionally, Define whether the feature group should support real time stream writing capabilities.     Stream enabled Feature Groups have unified single API for writing streaming features transparently     to both online and offline store.</p> </li> <li>parents <code>List[hsfs.feature_group.FeatureGroup] | None</code>: Optionally, Define the parents of this feature group as the     origin where the data is coming from.</li> <li>topic_name <code>str | None</code>: Optionally, define the name of the topic used for data ingestion. If left undefined it     defaults to using project topic.</li> <li>notification_topic_name <code>str | None</code>: Optionally, define the name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If left undefined no notifications are sent.</li> <li>storage_connector <code>hsfs.StorageConnector | Dict[str, Any]</code>: the storage connector used to establish connectivity     with the data source.</li> <li>path <code>str | None</code>: The location within the scope of the storage connector, from where to read     the data for the external feature group</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The feature group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_or_create_feature_view","title":"get_or_create_feature_view","text":"<pre><code>FeatureStore.get_or_create_feature_view(\n    name,\n    query,\n    version,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n)\n</code></pre> <p>Get feature view metadata object or create a new one if it doesn't exist. This method doesn't update existing feature view metadata object.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfeature_view = fs.get_or_create_feature_view(\n    name='bitcoin_feature_view',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to create.</li> <li>query <code>hsfs.constructor.query.Query</code>: Feature store <code>Query</code>.</li> <li>version <code>int</code>: Version of the feature view to create.</li> <li>description <code>str | None</code>: A string describing the contents of the feature view to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>labels <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the feature view. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>inference_helper_columns <code>List[str] | None</code>: A list of feature names that are not used in training the model itself but can be     used during batch or online inference for extra information. Inference helper column name(s) must be     part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name     when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be     omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during     online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</li> <li>training_helper_columns <code>List[str] | None</code>: A list of feature names that are not the part of the model schema itself but can be     used during training as a helper for extra information. Training helper column name(s) must be     part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when     defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the training helper columns will be omitted during both batch and online inference.     Training helper columns can be optionally fetched with training data. For more details see     documentation for feature view's get training data methods.  Defaults to `[], no training helper     columns.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     vector and at inference time. Defaults to <code>{}</code>, no     transformations.</li> </ul> <p>Returns:</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_or_create_spine_group","title":"get_or_create_spine_group","text":"<pre><code>FeatureStore.get_or_create_spine_group(\n    name,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    event_time=None,\n    features=None,\n    dataframe=None,\n)\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date',\n                    dataframe=spine_df\n                    )\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> <p>Note</p> <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring: <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre></p> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the spine group to create.</li> <li>version <code>int | None</code>: Version of the spine group to retrieve, defaults to <code>None</code> and     will create the spine group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the spine group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     spine group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this spine group. If event_time is set     the spine group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li> <p>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the spine group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ dataframe__: DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features.</p> </li> </ul> <p>Returns</p> <p><code>SpineGroup</code>. The spine group metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>FeatureStore.get_storage_connector(name)\n</code></pre> <p>Get a previously created storage connector from the feature store.</p> <p>Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.</p> <p>If you want to connect to the online feature store, see the <code>get_online_storage_connector</code> method to get the JDBC connector for the Online Feature Store.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nsc = fs.get_storage_connector(\"demo_fs_meb10000_Training_Datasets\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the storage connector to retrieve.</li> </ul> <p>Returns</p> <p><code>StorageConnector</code>. Storage connector object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_training_dataset","title":"get_training_dataset","text":"<pre><code>FeatureStore.get_training_dataset(name, version=None)\n</code></pre> <p>Get a training dataset entity from the feature store.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version.</p> <p>It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to get.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve training dataset from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_training_datasets","title":"get_training_datasets","text":"<pre><code>FeatureStore.get_training_datasets(name)\n</code></pre> <p>Get a list of all versions of a training dataset entity from the feature store.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to get.</li> </ul> <p>Returns</p> <p><code>TrainingDataset</code>: List of training dataset metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature group from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_transformation_function","title":"get_transformation_function","text":"<pre><code>FeatureStore.get_transformation_function(name, version=None)\n</code></pre> <p>Get  transformation function metadata object.</p> <p>Get transformation function by name. This will default to version 1</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n</code></pre> <p>Get built-in transformation function min max scaler</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler_fn = fs.get_transformation_function(name=\"min_max_scaler\")\n</code></pre> <p>Get transformation function by name and version</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=2)\n</code></pre> <p>You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s).</p> <p>Attach transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=1)\n\n# attach transformation functions\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=query,\n    labels=[\"target_column\"],\n    transformation_functions={\n        \"column_to_transform\": min_max_scaler\n    }\n)\n</code></pre> <p>Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for <code>min_max_scaler</code>; mean and standard deviation for <code>standard_scaler</code> etc.</p> <p>Attach built-in transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# retrieve transformation functions\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\n# attach built-in transformation functions while creating feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = {\n        \"category_column\": label_encoder,\n        \"weight\": robust_scaler,\n        \"age\": min_max_scaler,\n        \"salary\": standard_scaler\n    }\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: name of transformation function.</li> <li>version <code>int | None</code>: version of transformation function. Optional, if not provided all functions that match to provided     name will be retrieved.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#get_transformation_functions","title":"get_transformation_functions","text":"<pre><code>FeatureStore.get_transformation_functions()\n</code></pre> <p>Get  all transformation functions metadata objects.</p> <p>Get all transformation functions</p> <pre><code># get feature store instance\nfs = ...\n\n# get all transformation functions\nlist_transformation_fns = fs.get_transformation_functions()\n</code></pre> <p>Returns:</p> <p><code>List[TransformationFunction]</code>. List of transformation function instances.</p> <p>[source]</p>"},{"location":"generated/api/feature_store_api/#sql","title":"sql","text":"<pre><code>FeatureStore.sql(query, dataframe_type=\"default\", online=False, read_options=None)\n</code></pre> <p>Execute SQL command on the offline or online feature store database</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# construct the query and show head rows\nquery_res_head = fs.sql(\"SELECT * FROM `fg_1`\").head()\n</code></pre> <p>Arguments</p> <ul> <li>query <code>str</code>: The SQL query to execute.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> <li>online <code>bool | None</code>: Set to true to execute the query against the online feature store.     Defaults to False.</li> <li>read_options <code>dict | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code> If running queries on the online feature store, users can provide an entry <code>{'external': True}</code>, this instructs the library to use the <code>host</code> parameter in the <code>hsfs.connection()</code> to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Defaults to <code>{}</code>.</li> </ul> </li> </ul> <p>Returns</p> <p><code>DataFrame</code>: DataFrame depending on the chosen type.</p>"},{"location":"generated/api/feature_view_api/","title":"Feature View","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#featureview","title":"FeatureView","text":"<pre><code>hsfs.feature_view.FeatureView(\n    name,\n    query,\n    featurestore_id,\n    id=None,\n    version=None,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n    featurestore_name=None,\n    serving_keys=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/feature_view_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_feature_view","title":"create_feature_view","text":"<pre><code>FeatureStore.create_feature_view(\n    name,\n    query,\n    version=None,\n    description=\"\",\n    labels=None,\n    inference_helper_columns=None,\n    training_helper_columns=None,\n    transformation_functions=None,\n)\n</code></pre> <p>Create a feature view metadata object and saved it to hopsworks.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the feature group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# get the transformation functions\nstandard_scaler = fs.get_transformation_function(name='standard_scaler')\n\n# construct dictionary of \"feature - transformation function\" pairs\ntransformation_functions = {col_name: standard_scaler for col_name in df.columns}\n\nfeature_view = fs.create_feature_view(\n    name='air_quality_fv',\n    version=1,\n    transformation_functions=transformation_functions,\n    query=query\n)\n</code></pre> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# define dictionary with column names and transformation functions pairs\nmapping_transformers = ...\n\n# create feature view\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    version=1,\n    transformation_functions=mapping_transformers,\n    query=query\n)\n</code></pre> <p>Warning</p> <p><code>as_of</code> argument in the <code>Query</code> will be ignored because feature view does not support time travel query.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to create.</li> <li>query <code>hsfs.constructor.query.Query</code>: Feature store <code>Query</code>.</li> <li>version <code>int | None</code>: Version of the feature view to create, defaults to <code>None</code> and     will create the feature view with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the feature view to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>labels <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the feature view. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>inference_helper_columns <code>List[str] | None</code>: A list of feature names that are not used in training the model itself but can be     used during batch or online inference for extra information. Inference helper column name(s) must be     part of the <code>Query</code> object. If inference helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to be prepended to the original column name     when defining <code>inference_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the inference helper columns optionally can be omitted during batch (<code>get_batch_data</code>) and will be     omitted during online  inference (<code>get_feature_vector(s)</code>). To get inference helper column(s) during     online inference use <code>get_inference_helper(s)</code> method. Defaults to `[], no helper columns.</li> <li>training_helper_columns <code>List[str] | None</code>: A list of feature names that are not the part of the model schema itself but can be     used during training as a helper for extra information. Training helper column name(s) must be     part of the <code>Query</code> object. If training helper column name(s) belong to feature group that is part     of a <code>Join</code> with <code>prefix</code> defined, then this prefix needs to prepended to the original column name when     defining <code>training_helper_columns</code> list. When replaying a <code>Query</code> during model inference,     the training helper columns will be omitted during both batch and online inference.     Training helper columns can be optionally fetched with training data. For more details see     documentation for feature view's get training data methods.  Defaults to `[], no training helper     columns.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     vector and at inference time. Defaults to <code>{}</code>, no     transformations.</li> </ul> <p>Returns:</p> <p><code>FeatureView</code>: The feature view metadata object.</p>"},{"location":"generated/api/feature_view_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_view","title":"get_feature_view","text":"<pre><code>FeatureStore.get_feature_view(name, version=None)\n</code></pre> <p>Get a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(\n    name='feature_view_name',\n    version=1\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> <li>version <code>int</code>: Version of the feature view to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: The feature view metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_views","title":"get_feature_views","text":"<pre><code>FeatureStore.get_feature_views(name)\n</code></pre> <p>Get a list of all versions of a feature view entity from the feature store.</p> <p>Getting a feature view from the Feature Store means getting its metadata.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get a list of all versions of a feature view\nfeature_view = fs.get_feature_views(\n    name='feature_view_name'\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature view to get.</li> </ul> <p>Returns</p> <p><code>FeatureView</code>: List of feature view metadata objects.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve feature view from the feature store.</li> </ul>"},{"location":"generated/api/feature_view_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#description","title":"description","text":"<p>Description of the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#features","title":"features","text":"<p>Feature view schema. (alias)</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#featurestore_id","title":"featurestore_id","text":"<p>Feature store id.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#id","title":"id","text":"<p>Feature view id.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#inference_helper_columns","title":"inference_helper_columns","text":"<p>The helper column sof the feature view.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#labels","title":"labels","text":"<p>The labels/prediction feature of the feature view.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#name","title":"name","text":"<p>Name of the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#primary_keys","title":"primary_keys","text":"<p>Set of primary key names that is required as keys in input dict object for <code>get_feature_vector(s)</code> method. When there are duplicated primary key names and prefix is not defined in the query, prefix is generated and prepended to the primary key name in this format \"fgId_{feature_group_id}_{join_index}\" where <code>join_index</code> is the order of the join.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#query","title":"query","text":"<p>Query of the feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#schema","title":"schema","text":"<p>Feature view schema.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#serving_keys","title":"serving_keys","text":"<p>All primary keys of the feature groups included in the query.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#training_helper_columns","title":"training_helper_columns","text":"<p>The helper column sof the feature view.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#transformation_functions","title":"transformation_functions","text":"<p>Get transformation functions.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#version","title":"version","text":"<p>Version number of the feature view.</p>"},{"location":"generated/api/feature_view_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/feature_view_api/#add_tag","title":"add_tag","text":"<pre><code>FeatureView.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature view.</p> <p>A tag consists of a name and value pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# attach a tag to a feature view\nfeature_view.add_tag(name=\"tag_schema\", value={\"key\", \"value\"})\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#add_training_dataset_tag","title":"add_training_dataset_tag","text":"<pre><code>FeatureView.add_training_dataset_tag(training_dataset_version, name, value)\n</code></pre> <p>Attach a tag to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# attach a tag to a training dataset\nfeature_view.add_training_dataset_tag(\n    training_dataset_version=1,\n    name=\"tag_schema\",\n    value={\"key\", \"value\"}\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Dict[str, Any] | hsfs.tag.Tag</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#clean","title":"clean","text":"<pre><code>FeatureView.clean(feature_store_id, feature_view_name, feature_view_version)\n</code></pre> <p>Delete the feature view and all associated metadata and training data. This can delete corrupted feature view which cannot be retrieved due to a corrupted query for example.</p> <p>Example</p> <pre><code># delete a feature view and all associated metadata\nfrom hsfs.feature_view import FeatureView\n\nFeatureView.clean(\n    feature_store_id=1,\n    feature_view_name='feature_view_name',\n    feature_view_version=1\n)\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS.</p> <p>Arguments</p> <ul> <li>feature_store_id <code>int</code>: int. Id of feature store.</li> <li>feature_view_name <code>str</code>: str. Name of feature view.</li> <li>feature_view_version <code>str</code>: str. Version of feature view.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>FeatureView.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfg = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # compare to a given value\n    specific_value=0.5,\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>FeatureView.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# enable statistics monitoring\nmy_config = fv._create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature view.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_train_test_split","title":"create_train_test_split","text":"<pre><code>FeatureView.create_train_test_split(\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    storage_connector=None,\n    location=\"\",\n    description=\"\",\n    extra_filter=None,\n    data_format=\"parquet\",\n    coalesce=False,\n    seed=None,\n    statistics_config=None,\n    write_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n)\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>. The training data is split into train and test set at random or according to time ranges. The training data can be retrieved by calling <code>feature_view.get_train_test_split</code>.</p> <p>Create random splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    test_size=0.2,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as string</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-01-01 00:00:00\"\ntrain_end = \"2022-06-06 23:59:59\"\ntest_start = \"2022-06-07 00:00:00\"\ntest_end = \"2022-12-25 23:59:59\"\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as datetime object</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\ntrain_start = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\ntrain_end = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\ntest_start = datetime.strptime(\"2022-06-07 00:00:00\", date_format)\ntest_end = datetime.strptime(\"2022-12-25 23:59:59\" , date_format)\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Write training dataset to external storage</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\nexternal_storage_connector = fs.get_storage_connector(\"storage_connector_name\")\n\n# create a train-test split dataset\nversion, job = feature_view.create_train_test_split(\n    train_start=...,\n    train_end=...,\n    test_start=...,\n    test_end=...,\n    storage_connector = external_storage_connector,\n    description=...,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Warning, the following code will fail because category column contains sparse values and training dataset may not have all values available in test split.</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'category_col':['category_a','category_b','category_c','category_d'],\n    'numeric_col': [40,10,60,40]\n})\n\nfeature_group = fs.get_or_create_feature_group(\n    name='feature_group_name',\n    version=1,\n    primary_key=['category_col']\n)\n\nfeature_group.insert(df)\n\nlabel_encoder = fs.get_transformation_function(name='label_encoder')\n\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=feature_group.select_all(),\n    transformation_functions={'category_col':label_encoder}\n)\n\nfeature_view.create_train_test_split(\n    test_size=0.5\n)\n# Output: KeyError: 'category_c'\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>test_size <code>float | None</code>: size of test set.</li> <li>train_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>train_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be  formatted in one of the following ormats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"parquet\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> </ul> <p>Returns</p> <p>(td_version, <code>Job</code>): Tuple of training dataset version and job.     When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_train_validation_test_split","title":"create_train_validation_test_split","text":"<pre><code>FeatureView.create_train_validation_test_split(\n    validation_size=None,\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    validation_start=\"\",\n    validation_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    storage_connector=None,\n    location=\"\",\n    description=\"\",\n    extra_filter=None,\n    data_format=\"parquet\",\n    coalesce=False,\n    seed=None,\n    statistics_config=None,\n    write_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n)\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>. The training data is split into train, validation, and test set at random or according to time range. The training data can be retrieved by calling <code>feature_view.get_train_validation_test_split</code>.</p> <p>Create random splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    validation_size=0.3,\n    test_size=0.2,\n    description='Description of a dataset',\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as string</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-01-01 00:00:00\"\ntrain_end = \"2022-06-01 23:59:59\"\nvalidation_start = \"2022-06-02 00:00:00\"\nvalidation_end = \"2022-07-01 23:59:59\"\ntest_start = \"2022-07-02 00:00:00\"\ntest_end = \"2022-08-01 23:59:59\"\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    validation_start=validation_start,\n    validation_end=validation_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Create time series splits by specifying date as datetime object</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\ntrain_start = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\ntrain_end = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\nvalidation_start = datetime.strptime(\"2022-06-02 00:00:00\", date_format)\nvalidation_end = datetime.strptime(\"2022-07-01 23:59:59\", date_format)\ntest_start = datetime.strptime(\"2022-06-07 00:00:00\", date_format)\ntest_end = datetime.strptime(\"2022-12-25 23:59:59\", date_format)\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    validation_start=validation_start,\n    validation_end=validation_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Write training dataset to external storage</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\nexternal_storage_connector = fs.get_storage_connector(\"storage_connector_name\")\n\n# create a train-validation-test split dataset\nversion, job = feature_view.create_train_validation_test_split(\n    train_start=...,\n    train_end=...,\n    validation_start=...,\n    validation_end=...,\n    test_start=...,\n    test_end=...,\n    description=...,\n    storage_connector = external_storage_connector,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>validation_size <code>float | None</code>: size of validation set.</li> <li>test_size <code>float | None</code>: size of test set.</li> <li>train_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>train_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the validation split query, inclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the validation split query, exclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"parquet\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> </ul> <p>Returns</p> <p>(td_version, <code>Job</code>): Tuple of training dataset version and job.     When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#create_training_data","title":"create_training_data","text":"<pre><code>FeatureView.create_training_data(\n    start_time=\"\",\n    end_time=\"\",\n    storage_connector=None,\n    location=\"\",\n    description=\"\",\n    extra_filter=None,\n    data_format=\"parquet\",\n    coalesce=False,\n    seed=None,\n    statistics_config=None,\n    write_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n)\n</code></pre> <p>Create the metadata for a training dataset and save the corresponding training data into <code>location</code>. The training data can be retrieved by calling <code>feature_view.get_training_data</code>.</p> <p>Create training dataset</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    description='Description of a dataset',\n    data_format='csv',\n    # async creation in order not to wait till finish of the job\n    write_options={\"wait_for_job\": False}\n)\n</code></pre> <p>Create training data specifying date range  with dates as strings</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nstart_time = \"2022-01-01 00:00:00\"\nend_time = \"2022-06-06 23:59:59\"\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n\n# When we want to read the training data, we need to supply the training data version returned by the create_training_data method:\nX_train, X_test, y_train, y_test = feature_view.get_training_data(version)\n</code></pre> <p>Create training data specifying date range  with dates as datetime objects</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nfrom datetime import datetime\ndate_format = \"%Y-%m-%d %H:%M:%S\"\n\nstart_time = datetime.strptime(\"2022-01-01 00:00:00\", date_format)\nend_time = datetime.strptime(\"2022-06-06 23:59:59\", date_format)\n\n# create a training dataset\nversion, job = feature_view.create_training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset',\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format='csv'\n)\n</code></pre> <p>Write training dataset to external storage</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get storage connector instance\nexternal_storage_connector = fs.get_storage_connector(\"storage_connector_name\")\n\n# create a train-test split dataset\nversion, job = feature_view.create_training_data(\n    start_time=...,\n    end_time=...,\n    storage_connector = external_storage_connector,\n    description=...,\n    # you can have different data formats such as csv, tsv, tfrecord, parquet and others\n    data_format=...\n)\n</code></pre> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the training dataset query, inclusive. Optional. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the training dataset query, exclusive. Optional. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"parquet\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not. Training helper columns are a     list of feature names in the feature view, defined during its creation, that are not the part of the     model schema itself but can be used during training as a helper for extra information.     If training helper columns were not defined in the feature view then<code>training_helper_columns=True</code>     will not have any effect. Defaults to <code>False</code>, no training helper columns.</li> </ul> <p>Returns</p> <p>(td_version, <code>Job</code>): Tuple of training dataset version and job.     When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete","title":"delete","text":"<pre><code>FeatureView.delete()\n</code></pre> <p>Delete current feature view, all associated metadata and training data.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a feature view\nfeature_view.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature view and related training dataset and materialized data in HopsFS.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_all_training_datasets","title":"delete_all_training_datasets","text":"<pre><code>FeatureView.delete_all_training_datasets()\n</code></pre> <p>Delete all training datasets. This will delete both metadata and training data.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete all training datasets\nfeature_view.delete_all_training_datasets()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training datasets.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_tag","title":"delete_tag","text":"<pre><code>FeatureView.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature view.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a tag\nfeature_view.delete_tag('name_of_tag')\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_training_dataset","title":"delete_training_dataset","text":"<pre><code>FeatureView.delete_training_dataset(training_dataset_version)\n</code></pre> <p>Delete a training dataset. This will delete both metadata and training data.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete a training dataset\nfeature_view.delete_training_dataset(\n    training_dataset_version=1\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: Version of the training dataset to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#delete_training_dataset_tag","title":"delete_training_dataset_tag","text":"<pre><code>FeatureView.delete_training_dataset_tag(training_dataset_version, name)\n</code></pre> <p>Delete a tag attached to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# delete training dataset tag\nfeature_view.delete_training_dataset_tag(\n    training_dataset_version=1,\n    name='name_of_dataset'\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#find_neighbors","title":"find_neighbors","text":"<pre><code>FeatureView.find_neighbors(\n    embedding, feature=None, k=10, filter=None, external=None, return_type=\"list\"\n)\n</code></pre> <p>Finds the nearest neighbors for a given embedding in the vector database.</p> <p>If <code>filter</code> is specified, or if embedding feature is stored in default project index, the number of results returned may be less than k. Try using a large value of k and extract the top k items from the results if needed.</p> <p>Duplicate column error in Polars</p> <p>If the feature view has duplicate column names, attempting to create a polars DataFrame will raise an error. To avoid this, set <code>return_type</code> to <code>\"list\"</code> or <code>\"pandas\"</code>.</p> <p>Arguments</p> <ul> <li>embedding <code>List[int | float]</code>: The target embedding for which neighbors are to be found.</li> <li>feature <code>hsfs.feature.Feature | None</code>: The feature used to compute similarity score. Required only if there are multiple embeddings (optional).</li> <li>k <code>int | None</code>: The number of nearest neighbors to retrieve (default is 10).</li> <li>filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: A filter expression to restrict the search space (optional).</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['list', 'polars', 'pandas']</code>: <code>\"list\"</code>, <code>\"pandas\"</code> or <code>\"polars\"</code>. Defaults to <code>\"list\"</code>.</li> </ul> <p>Returns</p> <p><code>list</code>, <code>pd.DataFrame</code> or <code>polars.DataFrame</code> if <code>return type</code> is set to <code>\"list\"</code>, <code>\"pandas\"</code> or <code>\"polars\"</code> respectively. Defaults to <code>list</code>.</p> <p>Example</p> <pre><code>embedding_index = EmbeddingIndex()\nembedding_index.add_embedding(name=\"user_vector\", dimension=3)\nfg = fs.create_feature_group(\n            name='air_quality',\n            embedding_index=embedding_index,\n            version=1,\n            primary_key=['id1'],\n            online_enabled=True,\n        )\nfg.insert(data)\nfv = fs.create_feature_view(\"air_quality\", fg.select_all())\nfv.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n)\n\n# apply filter\nfg.find_neighbors(\n    [0.1, 0.2, 0.3],\n    k=5,\n    feature=fg.user_vector,  # optional\n    filter=(fg.id1 &gt; 10) &amp; (fg.id1 &lt; 30)\n)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#from_response_json","title":"from_response_json","text":"<pre><code>FeatureView.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_batch_data","title":"get_batch_data","text":"<pre><code>FeatureView.get_batch_data(\n    start_time=None,\n    end_time=None,\n    read_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    inference_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Get a batch of data from an event time interval from the offline feature store.</p> <p>Batch data for the last 24 hours</p> <pre><code>    # get feature store instance\n    fs = ...\n\n    # get feature view instance\n    feature_view = fs.get_feature_view(...)\n\n    # set up dates\n    import datetime\n    start_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\n    end_date = (datetime.datetime.now())\n\n    # get a batch of data\n    df = feature_view.get_batch_data(\n        start_time=start_date,\n        end_time=end_date\n    )\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the batch query, inclusive. Optional. Strings should be     formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the batch query, exclusive. Optional. Strings should be     formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>read_options <code>Dict[str, Any] | None</code>: User provided read options.     Dictionary of read options for python engine:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to read batch data with Hive instead of   Hopsworks Feature Query Service. Defaults to <code>{}</code>.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>inference_helper_columns <code>bool</code>: whether to include inference helper columns or not.     Inference helper columns are a list of feature names in the feature view, defined during its creation,     that may not be used in training the model itself but can be used during batch or online inference     for extra information. If inference helper columns were not defined in the feature view     <code>inference_helper_columns=True</code> will not any effect. Defaults to <code>False</code>, no helper columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data. <code>pyspark.DataFrame</code>. A Spark DataFrame. <code>pandas.DataFrame</code>. A Pandas DataFrame. <code>polars.DataFrame</code>. A Polars DataFrame. <code>numpy.ndarray</code>. A two-dimensional Numpy array. <code>list</code>. A two-dimensional Python list.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_batch_query","title":"get_batch_query","text":"<pre><code>FeatureView.get_batch_query(start_time=None, end_time=None)\n</code></pre> <p>Get a query string of the batch query.</p> <p>Batch query for the last 24 hours</p> <pre><code>    # get feature store instance\n    fs = ...\n\n    # get feature view instance\n    feature_view = fs.get_feature_view(...)\n\n    # set up dates\n    import datetime\n    start_date = (datetime.datetime.now() - datetime.timedelta(hours=24))\n    end_date = (datetime.datetime.now())\n\n    # get a query string of batch query\n    query_str = feature_view.get_batch_query(\n        start_time=start_date,\n        end_time=end_date\n    )\n    # print query string\n    print(query_str)\n</code></pre> <p>Arguments</p> <ul> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the batch query, inclusive. Optional. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the batch query, exclusive. Optional. Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>,     <code>%Y-%m-%d %H:%M:%S</code>, or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> </ul> <p>Returns</p> <p><code>str</code>: batch query</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>FeatureView.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch feature monitoring configs attached to the feature view. If no arguments is provided the method will return all feature monitoring configs attached to the feature view, meaning all feature monitoring configs that are attach to a feature in the feature view. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_view\", version=1)\n# fetch all feature monitoring configs attached to the feature view\nfm_configs = fv.get_feature_monitoring_configs()\n# fetch a single feature monitoring config by name\nfm_config = fv.get_feature_monitoring_configs(name=\"my_config\")\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fv.get_feature_monitoring_configs(feature_name=\"my_feature\")\n# fetch a single feature monitoring config with a particular id\nfm_config = fv.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>FeatureView.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature view\nfv = fs.get_feature_view(name=\"my_feature_group\", version=1)\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fv.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n# or use the config id\nfm_history = fv.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_date: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_date: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_vector","title":"get_feature_vector","text":"<pre><code>FeatureView.get_feature_vector(\n    entry,\n    passed_features=None,\n    external=None,\n    return_type=\"list\",\n    allow_missing=False,\n    force_rest_client=False,\n    force_sql_client=False,\n)\n</code></pre> <p>Returns assembled feature vector from online feature store.     Call <code>feature_view.init_serving</code> before this method if the following configurations are needed.       1. The training dataset version of the transformation statistics       2. Additional configurations of online serving engine</p> <p>Missing primary key entries</p> <p>If the provided primary key <code>entry</code> can't be found in one or more of the feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting <code>allow_missing</code> to <code>True</code> returns a feature vector with missing values.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled serving vector as a python list\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n\n# get assembled serving vector as a pandas dataframe\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    return_type = \"pandas\"\n)\n\n# get assembled serving vector as a numpy array\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    return_type = \"numpy\"\n)\n</code></pre> <p>Get feature vector with user-supplied features</p> <pre><code># get feature store instance\nfs = ...\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# the application provides a feature value 'app_attr'\napp_attr = ...\n\n# get a feature vector\nfeature_view.get_feature_vector(\n    entry = {\"pk1\": 1, \"pk2\": 2},\n    passed_features = { \"app_feature\" : app_attr }\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>Dict[str, Any]</code>: dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code>     If the required primary keys is not provided, it will look for name     of the primary key in feature group in the entry.</li> <li>passed_features <code>Dict[str, Any] | None</code>: dictionary of feature values provided by the application at runtime.     They can replace features values fetched from the feature store as well as     providing feature values which are not available in the feature store.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['list', 'polars', 'numpy', 'pandas']</code>: <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code>. Defaults to <code>\"list\"</code>.</li> <li>force_rest_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the REST client if initialised.</li> <li>force_sql_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the SQL client if initialised.</li> <li>allow_missing <code>bool</code>: Setting to <code>True</code> returns feature vectors with missing values.</li> </ul> <p>Returns</p> <p><code>list</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> if <code>return type</code> is set to <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code> respectively. Defaults to <code>list</code>. Returned <code>list</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_feature_vectors","title":"get_feature_vectors","text":"<pre><code>FeatureView.get_feature_vectors(\n    entry,\n    passed_features=None,\n    external=None,\n    return_type=\"list\",\n    allow_missing=False,\n    force_rest_client=False,\n    force_sql_client=False,\n)\n</code></pre> <p>Returns assembled feature vectors in batches from online feature store.     Call <code>feature_view.init_serving</code> before this method if the following configurations are needed.       1. The training dataset version of the transformation statistics       2. Additional configurations of online serving engine</p> <p>Missing primary key entries</p> <p>If any of the provided primary key elements in <code>entry</code> can't be found in any of the feature groups, no feature vector for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception. Alternatively, setting <code>allow_missing</code> to <code>True</code> returns feature vectors with missing values.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled serving vectors as a python list of lists\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n\n# get assembled serving vectors as a pandas dataframe\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    return_type = \"pandas\"\n)\n\n# get assembled serving vectors as a numpy array\nfeature_view.get_feature_vectors(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ],\n    return_type = \"numpy\"\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>List[Dict[str, Any]]</code>: a list of dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code>     If the required primary keys is not provided, it will look for name     of the primary key in feature group in the entry.</li> <li>passed_features <code>List[Dict[str, Any]] | None</code>: a list of dictionary of feature values provided by the application at runtime.     They can replace features values fetched from the feature store as well as     providing feature values which are not available in the feature store.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['list', 'polars', 'numpy', 'pandas']</code>: <code>\"list\"</code>, <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"numpy\"</code>. Defaults to <code>\"list\"</code>.</li> <li>force_sql_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the SQL client if initialised.</li> <li>force_rest_client <code>bool</code>: boolean, defaults to False. If set to True, reads from online feature store     using the REST client if initialised.</li> <li>allow_missing <code>bool</code>: Setting to <code>True</code> returns feature vectors with missing values.</li> </ul> <p>Returns</p> <p><code>List[list]</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> if <code>return type</code> is set to <code>\"list\",</code>\"pandas\"<code>,</code>\"polars\"<code>or</code>\"numpy\"<code>respectively. Defaults to</code>List[list]`.</p> <p>Returned <code>List[list]</code>, <code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>np.ndarray</code> contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_inference_helper","title":"get_inference_helper","text":"<pre><code>FeatureView.get_inference_helper(\n    entry, external=None, return_type=\"pandas\", force_rest_client=False, force_sql_client=False\n)\n</code></pre> <p>Returns assembled inference helper column vectors from online feature store.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled inference helper column vector\nfeature_view.get_inference_helper(\n    entry = {\"pk1\": 1, \"pk2\": 2}\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>Dict[str, Any]</code>: dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code></li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['pandas', 'dict', 'polars']</code>: <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"dict\"</code>. Defaults to <code>\"pandas\"</code>.</li> </ul> <p>Returns</p> <p><code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>dict</code>. Defaults to <code>pd.DataFrame</code>.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_inference_helpers","title":"get_inference_helpers","text":"<pre><code>FeatureView.get_inference_helpers(\n    entry, external=None, return_type=\"pandas\", force_sql_client=False, force_rest_client=False\n)\n</code></pre> <p>Returns assembled inference helper column vectors in batches from online feature store.</p> <p>Missing primary key entries</p> <p>If any of the provided primary key elements in <code>entry</code> can't be found in any of the feature groups, no inference helper column vectors for that primary key value will be returned. If it can be found in at least one but not all feature groups used by this feature view the call to this method will raise an exception.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get assembled inference helper column vectors\nfeature_view.get_inference_helpers(\n    entry = [\n        {\"pk1\": 1, \"pk2\": 2},\n        {\"pk1\": 3, \"pk2\": 4},\n        {\"pk1\": 5, \"pk2\": 6}\n    ]\n)\n</code></pre> <p>Arguments</p> <ul> <li>entry <code>List[Dict[str, Any]]</code>: a list of dictionary of feature group primary key and values provided by serving application.     Set of required primary keys is <code>feature_view.primary_keys</code></li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> <li>return_type <code>Literal['pandas', 'dict', 'polars']</code>: <code>\"pandas\"</code>, <code>\"polars\"</code> or <code>\"dict\"</code>. Defaults to <code>\"pandas\"</code>.</li> </ul> <p>Returns</p> <p><code>pd.DataFrame</code>, <code>polars.DataFrame</code> or <code>List[Dict[str, Any]]</code>.  Defaults to <code>pd.DataFrame</code>.</p> <p>Returned <code>pd.DataFrame</code> or <code>List[dict]</code>  contains feature values related to provided primary keys, ordered according to positions of this features in the feature view query.</p> <p>Raises</p> <p><code>Exception</code>. When primary key entry cannot be found in one or more of the feature groups used by this     feature view.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_last_accessed_training_dataset","title":"get_last_accessed_training_dataset","text":"<pre><code>FeatureView.get_last_accessed_training_dataset()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_models","title":"get_models","text":"<pre><code>FeatureView.get_models(training_dataset_version=None)\n</code></pre> <p>Get the generated models using this feature view, based on explicit provenance. Only the accessible models are returned. For more items use the base method - get_models_provenance</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Filter generated models based on the used training dataset version.</li> </ul> <p>Returns</p> <p>`List[Model]: List of models.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_models_provenance","title":"get_models_provenance","text":"<pre><code>FeatureView.get_models_provenance(training_dataset_version=None)\n</code></pre> <p>Get the generated models using this feature view, based on explicit provenance. These models can be accessible or inaccessible. Explicit provenance does not track deleted generated model links, so deleted will always be empty. For inaccessible models, only a minimal information is returned.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Filter generated models based on the used training dataset version.</li> </ul> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_newest_model","title":"get_newest_model","text":"<pre><code>FeatureView.get_newest_model(training_dataset_version=None)\n</code></pre> <p>Get the latest generated model using this feature view, based on explicit provenance. Search only through the accessible models. For more items use the base method - get_models_provenance</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: Filter generated models based on the used training dataset version.</li> </ul> <p>Returns</p> <p><code>Model</code>: Newest Generated Model.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>FeatureView.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature view, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_tag","title":"get_tag","text":"<pre><code>FeatureView.get_tag(name)\n</code></pre> <p>Get the tags of a feature view.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a tag of a feature view\nname = feature_view.get_tag('tag_name')\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_tags","title":"get_tags","text":"<pre><code>FeatureView.get_tags()\n</code></pre> <p>Returns all tags attached to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get tags\nlist_tags = feature_view.get_tags()\n</code></pre> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_train_test_split","title":"get_train_test_split","text":"<pre><code>FeatureView.get_train_test_split(\n    training_dataset_version,\n    read_options=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Get training data created by <code>feature_view.create_train_test_split</code> or <code>feature_view.train_test_split</code>.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.get_train_test_split(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to read training dataset   with the Hopsworks API instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code> Defaults to <code>{}</code>.</li> </ul> </li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view or during     materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have     any effect. Defaults to <code>False</code>, no training helper columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_test, y_train, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_train_validation_test_split","title":"get_train_validation_test_split","text":"<pre><code>FeatureView.get_train_validation_test_split(\n    training_dataset_version,\n    read_options=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Get training data created by <code>feature_view.create_train_validation_test_split</code> or <code>feature_view.train_validation_test_split</code>.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_splits(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>read_options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to read training dataset   with the Hopsworks API instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code> Defaults to <code>{}</code>.</li> </ul> </li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view or during     materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have     any effect. Defaults to <code>False</code>, no training helper columns.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_val, X_test, y_train, y_val, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_data","title":"get_training_data","text":"<pre><code>FeatureView.get_training_data(\n    training_dataset_version,\n    read_options=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Get training data created by <code>feature_view.create_training_data</code> or <code>feature_view.training_data</code>.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nfeatures_df, labels_df = feature_view.get_training_data(training_dataset_version=1)\n</code></pre> <p>External Storage Support</p> <p>Reading training data that was written to external storage using a Storage Connector other than S3 can currently not be read using HSFS APIs with Python as Engine, instead you will have to use the storage's native client.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>read_options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     For python engine:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to read training dataset   with the Hopsworks API instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code> Defaults to <code>{}</code>.</li> </ul> </li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view or during     materializing training dataset in the file system then<code>training_helper_columns=True</code> will not have     any effect. Defaults to <code>False</code>, no training helper columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X, y): Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_statistics","title":"get_training_dataset_statistics","text":"<pre><code>FeatureView.get_training_dataset_statistics(\n    training_dataset_version, before_transformation=False, feature_names=None\n)\n</code></pre> <p>Get statistics of a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training dataset statistics\nstatistics = feature_view.get_training_dataset_statistics(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: Training dataset version</li> <li>before_transformation <code>bool</code>: Whether the statistics were computed before transformation functions or not.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code></p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_tag","title":"get_training_dataset_tag","text":"<pre><code>FeatureView.get_training_dataset_tag(training_dataset_version, name)\n</code></pre> <p>Get the tags of a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a training dataset tag\ntag_str = feature_view.get_training_dataset_tag(\n    training_dataset_version=1,\n     name=\"tag_schema\"\n)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_dataset_tags","title":"get_training_dataset_tags","text":"<pre><code>FeatureView.get_training_dataset_tags(training_dataset_version)\n</code></pre> <p>Returns all tags attached to a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get a training dataset tags\nlist_tags = feature_view.get_training_dataset_tags(\n    training_dataset_version=1\n)\n</code></pre> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#get_training_datasets","title":"get_training_datasets","text":"<pre><code>FeatureView.get_training_datasets()\n</code></pre> <p>Returns the metadata of all training datasets created with this feature view.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get all training dataset metadata\nlist_tds_meta = feature_view.get_training_datasets()\n</code></pre> <p>Returns</p> <p><code>List[TrainingDatasetBase]</code> List of training datasets metadata.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the training datasets metadata.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#init_batch_scoring","title":"init_batch_scoring","text":"<pre><code>FeatureView.init_batch_scoring(training_dataset_version=None)\n</code></pre> <p>Initialise feature view to retrieve feature vector from offline feature store.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# initialise feature view to retrieve feature vector from offline feature store\nfeature_view.init_batch_scoring(training_dataset_version=1)\n\n# get batch data\nbatch_data = feature_view.get_batch_data(...)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: int, optional. Default to be None. Transformation statistics     are fetched from training dataset and applied to the feature vector.</li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#init_serving","title":"init_serving","text":"<pre><code>FeatureView.init_serving(\n    training_dataset_version=None,\n    external=None,\n    options=None,\n    init_sql_client=None,\n    init_rest_client=False,\n    reset_rest_client=False,\n    config_rest_client=None,\n    default_client=None,\n    **kwargs\n)\n</code></pre> <p>Initialise feature view to retrieve feature vector from online and offline feature store.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# initialise feature view to retrieve a feature vector\nfeature_view.init_serving(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int | None</code>: int, optional. Default to be 1 for online feature store.     Transformation statistics are fetched from training dataset and applied to the feature vector.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used which relies on the private IP.     Defaults to True if connection to Hopsworks is established from external environment (e.g AWS     Sagemaker or Google Colab), otherwise to False.</li> <li>init_sql_client <code>bool | None</code>: boolean, optional. By default the sql client is initialised if no     client is specified to match legacy behaviour. If set to True, this ensure the online store     sql client is initialised, otherwise if init_rest_client is set to true it will     skip initialising the sql client.</li> <li>init_rest_client <code>bool</code>: boolean, defaults to False. By default the rest client is not initialised.     If set to True, this ensure the online store rest client is initialised. Pass additional configuration     options via the rest_config parameter. Set reset_rest_client to True to reset the rest client.</li> <li>default_client <code>Literal['sql', 'rest'] | None</code>: string, optional. Which client to default to if both are initialised. Defaults to None.</li> <li>options <code>Dict[str, Any] | None</code>: Additional options as key/value pairs for configuring online serving engine.<ul> <li>key: kwargs of SqlAlchemy engine creation (See: https://docs.sqlalchemy.org/en/20/core/engines.html#sqlalchemy.create_engine).   For example: <code>{\"pool_size\": 10}</code></li> </ul> </li> <li>reset_rest_client <code>bool</code>: boolean, defaults to False. If set to True, the rest client will be reset and reinitialised with provided configuration.</li> <li>config_rest_client <code>Dict[str, Any] | None</code>: dictionary, optional. Additional configuration options for the rest client. If the client is already initialised,     this will be ignored. Options include:<ul> <li><code>host</code>: string, optional. The host of the online store. Dynamically set if not provided.</li> <li><code>port</code>: int, optional. The port of the online store. Defaults to 4406.</li> <li><code>verify_certs</code>: boolean, optional. Verify the certificates of the online store server. Defaults to True.</li> <li><code>api_key</code>: string, optional. The API key to authenticate with the online store. The api key must be     provided if initialising the rest client in an internal environment.</li> <li><code>timeout</code>: int, optional. The timeout for the rest client in seconds. Defaults to 2.</li> <li><code>use_ssl</code>: boolean, optional. Use SSL to connect to the online store. Defaults to True.</li> </ul> </li> </ul> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#json","title":"json","text":"<pre><code>FeatureView.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#purge_all_training_data","title":"purge_all_training_data","text":"<pre><code>FeatureView.purge_all_training_data()\n</code></pre> <p>Delete all training datasets (data only).</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# purge all training data\nfeature_view.purge_all_training_data()\n</code></pre> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training datasets.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#purge_training_data","title":"purge_training_data","text":"<pre><code>FeatureView.purge_training_data(training_dataset_version)\n</code></pre> <p>Delete a training dataset (data only).</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# purge training data\nfeature_view.purge_training_data(training_dataset_version=1)\n</code></pre> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: Version of the training dataset to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#recreate_training_dataset","title":"recreate_training_dataset","text":"<pre><code>FeatureView.recreate_training_dataset(\n    training_dataset_version, statistics_config=None, write_options=None, spine=None\n)\n</code></pre> <p>Recreate a training dataset.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# recreate a training dataset that has been deleted\nfeature_view.recreate_training_dataset(training_dataset_version=1)\n</code></pre> <p>Info</p> <p>If a materialised training data has deleted. Use <code>recreate_training_dataset()</code> to recreate the training data.</p> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>training_dataset_version <code>int</code>: training dataset version</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>use_spark</code> and value <code>True</code> to materialize training dataset   with Spark instead of Hopsworks Feature Query Service.</li> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#to_dict","title":"to_dict","text":"<pre><code>FeatureView.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#train_test_split","title":"train_test_split","text":"<pre><code>FeatureView.train_test_split(\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    description=\"\",\n    extra_filter=None,\n    statistics_config=None,\n    read_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train and test set at random or according to time ranges. The training data can be recreated by calling <code>feature_view.get_train_test_split</code> with the metadata created.</p> <p>Create random train/test splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    test_size=0.2\n)\n</code></pre> <p>Create time-series train/test splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\ntrain_start = \"2022-05-01 00:00:00\"\ntrain_end = \"2022-06-04 23:59:59\"\ntest_start = \"2022-07-01 00:00:00\"\ntest_end= \"2022-08-04 23:59:59\"\n# you can also pass dates as datetime objects\n\n# get training data\nX_train, X_test, y_train, y_test = feature_view.train_test_split(\n    train_start=train_start,\n    train_end=train_end,\n    test_start=test_start,\n    test_end=test_end,\n    description='Description of a dataset'\n)\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>test_size <code>float | None</code>: size of test set. Should be between 0 and 1.</li> <li>train_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>train_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, read_options can contain the     following entries:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to create in-memory training dataset   with Hive instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code></li> <li>key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_test, y_train, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#train_validation_test_split","title":"train_validation_test_split","text":"<pre><code>FeatureView.train_validation_test_split(\n    validation_size=None,\n    test_size=None,\n    train_start=\"\",\n    train_end=\"\",\n    validation_start=\"\",\n    validation_end=\"\",\n    test_start=\"\",\n    test_end=\"\",\n    description=\"\",\n    extra_filter=None,\n    statistics_config=None,\n    read_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data is split into train, validation, and test set at random or according to time ranges. The training data can be recreated by calling <code>feature_view.get_train_validation_test_split</code> with the metadata created.</p> <p>Example</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n    validation_size=0.3,\n    test_size=0.2\n)\n</code></pre> <p>Time Series split</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up dates\nstart_time_train = '2017-01-01 00:00:01'\nend_time_train = '2018-02-01 23:59:59'\n\nstart_time_val = '2018-02-02 23:59:59'\nend_time_val = '2019-02-01 23:59:59'\n\nstart_time_test = '2019-02-02 23:59:59'\nend_time_test = '2020-02-01 23:59:59'\n# you can also pass dates as datetime objects\n\n# get training data\nX_train, X_val, X_test, y_train, y_val, y_test = feature_view.train_validation_test_split(\n    train_start=start_time_train,\n    train_end=end_time_train,\n    validation_start=start_time_val,\n    validation_end=end_time_val,\n    test_start=start_time_test,\n    test_end=end_time_test\n)\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>validation_size <code>float | None</code>: size of validation set. Should be between 0 and 1.</li> <li>test_size <code>float | None</code>: size of test set. Should be between 0 and 1.</li> <li>train_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the train split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>train_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the train split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the validation split query, inclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>validation_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the validation split query, exclusive. Strings     should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_start <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the test split query, inclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>test_end <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the test split query, exclusive. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, read_options can contain the     following entries:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to create in-memory training dataset   with Hive instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code></li> <li>key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X_train, X_val, X_test, y_train, y_val, y_test):     Tuple of dataframe of features and labels</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#training_data","title":"training_data","text":"<pre><code>FeatureView.training_data(\n    start_time=None,\n    end_time=None,\n    description=\"\",\n    extra_filter=None,\n    statistics_config=None,\n    read_options=None,\n    spine=None,\n    primary_keys=False,\n    event_time=False,\n    training_helper_columns=False,\n    dataframe_type=\"default\",\n)\n</code></pre> <p>Create the metadata for a training dataset and get the corresponding training data from the offline feature store. This returns the training data in memory and does not materialise data in storage. The training data can be recreated by calling <code>feature_view.get_training_data</code> with the metadata created.</p> <p>Create random splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# get training data\nfeatures_df, labels_df  = feature_view.training_data(\n    description='Descriprion of a dataset',\n)\n</code></pre> <p>Create time-series based splits</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\n# set up a date\nstart_time = \"2022-05-01 00:00:00\"\nend_time = \"2022-06-04 23:59:59\"\n# you can also pass dates as datetime objects\n\n# get training data\nfeatures_df, labels_df = feature_view.training_data(\n    start_time=start_time,\n    end_time=end_time,\n    description='Description of a dataset'\n)\n</code></pre> <p>Spine Groups/Dataframes</p> <p>Spine groups and dataframes are currently only supported with the Spark engine and Spark dataframes.</p> <p>Arguments</p> <ul> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: Start event time for the training dataset query, inclusive. Strings should be formatted in one of the following     formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: End event time for the training dataset query, exclusive. Strings should be formatted in one of the following     formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>. Int, i.e Unix Epoch should be in seconds.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>extra_filter <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic | None</code>: Additional filters to be attached to the training dataset.     The filters will be also applied in <code>get_batch_data</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>read_options <code>Dict[Any, Any] | None</code>: Additional options as key/value pairs to pass to the execution engine.     For spark engine: Dictionary of read options for Spark.     When using the <code>python</code> engine, read_options can contain the     following entries:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to create in-memory training dataset   with Hive instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key <code>\"hive_config\"</code> to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code></li> <li>key <code>spark</code> and value an object of type   hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset. Defaults to <code>{}</code>.</li> </ul> </li> <li>spine <code>pandas.DataFrame | hsfs.feature_view.pyspark.sql.DataFrame | hsfs.feature_view.pyspark.RDD | numpy.ndarray | List[List[Any]] | hsfs.feature_view.SplineGroup | None</code>: Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features. Defaults to <code>None</code> and is only required     when feature view was created with spine group in the feature query.     It is possible to directly pass a spine group instead of a dataframe to overwrite the left side of the     feature join, however, the same features as in the original feature group that is being replaced need to     be available in the spine group.</li> <li>primary_keys <code>bool</code>: whether to include primary key features or not.  Defaults to <code>False</code>, no primary key     features.</li> <li>event_time <code>bool</code>: whether to include event time feature or not.  Defaults to <code>False</code>, no event time feature.</li> <li>training_helper_columns <code>bool</code>: whether to include training helper columns or not.     Training helper columns are a list of feature names in the feature view, defined during its creation,     that are not the part of the model schema itself but can be used during training as a helper for     extra information. If training helper columns were not defined in the feature view     then<code>training_helper_columns=True</code> will not have any effect. Defaults to <code>False</code>, no training helper     columns.</li> <li>dataframe_type <code>str | None</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p>(X, y): Tuple of dataframe of features and labels. If there are no labels, y returns <code>None</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#update","title":"update","text":"<pre><code>FeatureView.update()\n</code></pre> <p>Update the description of the feature view.</p> <p>Update the feature view with a new description.</p> <pre><code># get feature store instance\nfs = ...\n\n# get feature view instance\nfeature_view = fs.get_feature_view(...)\n\nfeature_view.description = \"new description\"\nfeature_view.update()\n\n# Description is updated in the metadata. Below should return \"new description\".\nfs.get_feature_view(\"feature_view_name\", 1).description\n</code></pre> <p>Returns</p> <p><code>FeatureView</code> Updated feature view.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>FeatureView.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/feature_view_api/#update_last_accessed_training_dataset","title":"update_last_accessed_training_dataset","text":"<pre><code>FeatureView.update_last_accessed_training_dataset(version)\n</code></pre>"},{"location":"generated/api/job/","title":"Job","text":"<p>[source]</p>"},{"location":"generated/api/job/#job_1","title":"Job","text":"<pre><code>hsfs.core.job.Job(\n    id,\n    name,\n    creation_time,\n    config,\n    job_type,\n    creator,\n    executions=None,\n    type=None,\n    job_schedule=None,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/job/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/job/#get_state","title":"get_state","text":"<pre><code>Job.get_state()\n</code></pre> <p>Get the state of the job.</p> <p>Returns</p> <p><code>state</code>. Current state of the job, which can be one of the following: <code>INITIALIZING</code>, <code>INITIALIZATION_FAILED</code>, <code>FINISHED</code>, <code>RUNNING</code>, <code>ACCEPTED</code>, <code>FAILED</code>, <code>KILLED</code>, <code>NEW</code>, <code>NEW_SAVING</code>, <code>SUBMITTED</code>, <code>AGGREGATING_LOGS</code>, <code>FRAMEWORK_FAILURE</code>, <code>STARTING_APP_MASTER</code>, <code>APP_MASTER_START_FAILED</code>, <code>GENERATING_SECURITY_MATERIAL</code>, <code>CONVERTING_NOTEBOOK</code></p> <p>[source]</p>"},{"location":"generated/api/job/#get_final_state","title":"get_final_state","text":"<pre><code>Job.get_final_state()\n</code></pre> <p>Get the final state of the job.</p> <p>Returns</p> <p><code>final_state</code>. Final state of the job, which can be one of the following: <code>UNDEFINED</code>, <code>FINISHED</code>, <code>FAILED</code>, <code>KILLED</code>, <code>FRAMEWORK_FAILURE</code>, <code>APP_MASTER_START_FAILED</code>, <code>INITIALIZATION_FAILED</code>. <code>UNDEFINED</code> indicates  that the job is still running.</p>"},{"location":"generated/api/job/#job-configuration","title":"Job Configuration","text":"<p>[source]</p>"},{"location":"generated/api/job/#jobconfiguration","title":"JobConfiguration","text":"<pre><code>hsfs.core.job_configuration.JobConfiguration(\n    am_memory=2048,\n    am_cores=1,\n    executor_memory=4096,\n    executor_cores=1,\n    executor_instances=1,\n    dynamic_allocation=True,\n    dynamic_min_executors=1,\n    dynamic_max_executors=2,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/links/","title":"Provenance Links","text":"<p>Provenance Links are objects returned by methods such as get_feature_groups_provenance, get_storage_connector_provenance, get_parent_feature_group, get_generated_feature_groups, get_generated_feature_views get_models_provenance and represent sections of the provenance graph, depending on the method invoked.</p>"},{"location":"generated/api/links/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/links/#accessible","title":"accessible","text":"<p>List of [StorageConnectors|FeatureGroups|FeatureViews|Models] objects which are part of the provenance graph requested. These entities exist in the feature store/model registry and the user has access to them.</p> <p>[source]</p>"},{"location":"generated/api/links/#deleted","title":"deleted","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent. These entities have been removed from the feature store/model registry.</p> <p>[source]</p>"},{"location":"generated/api/links/#faulty","title":"faulty","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent. These entities exist in the feature store/model registry, however they are corrupted.</p> <p>[source]</p>"},{"location":"generated/api/links/#inaccessible","title":"inaccessible","text":"<p>List of [Artifact objects] which contains minimal information (name, version) about the entities (storage connectors, feature groups, feature views, models) they represent. These entities exist in the feature store/model registry, however the user does not have access to them anymore.</p>"},{"location":"generated/api/links/#artifact","title":"Artifact","text":"<p>Artifacts objects are part of the provenance graph and contain a minimal set of information regarding the entities (feature groups, feature views) they represent. The provenance graph contains Artifact objects when the underlying entities have been deleted or they are corrupted or they are not accessible by the user.</p> <p>[source]</p>"},{"location":"generated/api/links/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the artifact is located.</p> <p>[source]</p>"},{"location":"generated/api/links/#name","title":"name","text":"<p>Name of the artifact.</p> <p>[source]</p>"},{"location":"generated/api/links/#version","title":"version","text":"<p>Version of the artifact</p>"},{"location":"generated/api/query_api/","title":"Query","text":"<p>Query objects are strictly generated by HSFS APIs called on Feature Group objects. Users will never construct a Query object using the constructor of the class. For this reason we do not provide the full documentation of the class here.</p>"},{"location":"generated/api/query_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/query_api/#append_feature","title":"append_feature","text":"<pre><code>Query.append_feature(feature)\n</code></pre> <p>Append a feature to the query.</p> <p>Arguments</p> <ul> <li>feature <code>str | hsfs.feature.Feature</code>: <code>[str, Feature]</code>. Name of the feature to append to the query.</li> </ul> <p>[source]</p>"},{"location":"generated/api/query_api/#as_of","title":"as_of","text":"<pre><code>Query.as_of(wallclock_time=None, exclude_until=None)\n</code></pre> <p>Perform time travel on the given Query.</p> <p>Pyspark/Spark Only</p> <p>Apache HUDI exclusively supports Time Travel and Incremental Query via Spark Context</p> <p>This method returns a new Query object at the specified point in time. Optionally, commits before a specified point in time can be excluded from the query. The Query can then either be read into a Dataframe or used further to perform joins or construct a training dataset.</p> <p>Reading features at a specific point in time:</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:34:11\").read().show()\n</code></pre> <p>Reading commits incrementally between specified points in time:</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:34:11\", exclude_until=\"2020-10-19 07:34:11\").read().show()\n</code></pre> <p>The first parameter is inclusive while the latter is exclusive. That means, in order to query a single commit, you need to query that commit time and exclude everything just before the commit.</p> <p>Reading only the changes from a single commit</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(\"2020-10-20 07:31:38\", exclude_until=\"2020-10-20 07:31:37\").read().show()\n</code></pre> <p>When no wallclock_time is given, the latest state of features is returned. Optionally, commits before a specified point in time can still be excluded.</p> <p>Reading the latest state of features, excluding commits before a specified point in time</p> <pre><code>fs = connection.get_feature_store();\nquery = fs.get_feature_group(\"example_feature_group\", 1).select_all()\nquery.as_of(None, exclude_until=\"2020-10-20 07:31:38\").read().show()\n</code></pre> <p>Note that the interval will be applied to all joins in the query. If you want to query different intervals for different feature groups in the query, you have to apply them in a nested fashion: <pre><code>query1.as_of(..., ...)\n    .join(query2.as_of(..., ...))\n</code></pre></p> <p>If instead you apply another <code>as_of</code> selection after the join, all joined feature groups will be queried with this interval: <pre><code>query1.as_of(..., ...)  # as_of is not applied\n    .join(query2.as_of(..., ...))  # as_of is not applied\n    .as_of(..., ...)\n</code></pre></p> <p>Warning</p> <p>This function only works for queries on feature groups with time_travel_format='HUDI'.</p> <p>Warning</p> <p>Excluding commits via exclude_until is only possible within the range of the Hudi active timeline. By default, Hudi keeps the last 20 to 30 commits in the active timeline. If you need to keep a longer active timeline, you can overwrite the options: <code>hoodie.keep.min.commits</code> and <code>hoodie.keep.max.commits</code> when calling the <code>insert()</code> method.</p> <p>Arguments</p> <ul> <li>wallclock_time <code>str | int | datetime.datetime | datetime.date | None</code>: Read data as of this point in time.     Strings should be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> <li>exclude_until <code>str | int | datetime.datetime | datetime.date | None</code>: Exclude commits until this point in time. Strings should be formatted in one of the     following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, or <code>%Y-%m-%d %H:%M:%S</code>.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied time travel condition.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#filter","title":"filter","text":"<pre><code>Query.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\nquery.filter(Feature(\"weekly_sales\") &gt; 1000)\nquery.filter(Feature(\"name\").like(\"max%\"))\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: <pre><code>query.filter(fg.feature1 == 1).show(10)\n</code></pre></p> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...): <pre><code>query.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre></p> <p>Filters are fully compatible with joins</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n    .join(fg2.select_all(), on=[\"date\", \"location_id\"])\n    .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n    .filter((fg1.location_id == 10) | (fg1.location_id == 20))\n</code></pre> <p>Filters can be applied at any point of the query</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n    .join(fg2.select_all().filter(fg2.avg_temp &gt;= 22), on=[\"date\", \"location_id\"])\n    .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n    .filter(fg1.location_id == 10)\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#from_response_json","title":"from_response_json","text":"<pre><code>Query.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/query_api/#get_feature","title":"get_feature","text":"<pre><code>Query.get_feature(feature_name)\n</code></pre> <p>Get a feature by name.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: <code>str</code>. Name of the feature to get.</li> </ul> <p>Returns</p> <p><code>Feature</code>. Feature object.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#is_cache_feature_group_only","title":"is_cache_feature_group_only","text":"<pre><code>Query.is_cache_feature_group_only()\n</code></pre> <p>Query contains only cached feature groups</p> <p>[source]</p>"},{"location":"generated/api/query_api/#is_time_travel","title":"is_time_travel","text":"<pre><code>Query.is_time_travel()\n</code></pre> <p>Query contains time travel</p> <p>[source]</p>"},{"location":"generated/api/query_api/#join","title":"join","text":"<pre><code>Query.join(sub_query, on=None, left_on=None, right_on=None, join_type=\"inner\", prefix=None)\n</code></pre> <p>Join Query with another Query.</p> <p>If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no nested joins.</p> <p>Join two feature groups</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n</code></pre> <p>More complex join</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\nfg3 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all()\n        .join(fg2.select_all(), on=[\"date\", \"location_id\"])\n        .join(fg3.select_all(), left_on=[\"location_id\"], right_on=[\"id\"], join_type=\"left\")\n</code></pre> <p>Arguments</p> <ul> <li>sub_query <code>hsfs.constructor.query.Query</code>: Right-hand side query to join.</li> <li>on <code>List[str] | None</code>: List of feature names to join on if they are available in both     feature groups. Defaults to <code>[]</code>.</li> <li>left_on <code>List[str] | None</code>: List of feature names to join on from the left feature group of the     join. Defaults to <code>[]</code>.</li> <li>right_on <code>List[str] | None</code>: List of feature names to join on from the right feature group of     the join. Defaults to <code>[]</code>.</li> <li>join_type <code>str | None</code>: Type of join to perform, can be <code>\"inner\"</code>, <code>\"outer\"</code>, <code>\"left\"</code> or     <code>\"right\"</code>. Defaults to \"inner\".</li> <li>prefix <code>str | None</code>: User provided prefix to avoid feature name clash. Prefix is applied to the right     feature group of the query. Defaults to <code>None</code>.</li> </ul> <p>Returns</p> <p><code>Query</code>: A new Query object representing the join.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#pull_changes","title":"pull_changes","text":"<pre><code>Query.pull_changes(wallclock_start_time, wallclock_end_time)\n</code></pre> <p>Deprecated</p> <p><code>pull_changes</code> method is deprecated. Use `as_of(end_wallclock_time, exclude_until=start_wallclock_time) instead.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#read","title":"read","text":"<pre><code>Query.read(online=False, dataframe_type=\"default\", read_options=None)\n</code></pre> <p>Read the specified query into a DataFrame.</p> <p>It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists).</p> <p>External Feature Group Engine Support</p> <p>Spark only</p> <p>Reading a Query containing an External Feature Group directly into a Pandas Dataframe using Python/Pandas as Engine is not supported, however, you can use the Query API to create Feature Views/Training Data containing External Feature Groups.</p> <p>Arguments</p> <ul> <li>online <code>bool</code>: Read from online storage. Defaults to <code>False</code>.</li> <li>dataframe_type <code>str</code>: DataFrame type to return. Defaults to <code>\"default\"</code>.</li> <li>read_options <code>Dict[str, Any] | None</code>: Dictionary of read options for Spark in spark engine.     Only for python engine:<ul> <li>key <code>\"use_hive\"</code> and value <code>True</code> to read query with Hive instead of   Hopsworks Feature Query Service.</li> <li>key <code>\"arrow_flight_config\"</code> to pass a dictionary of arrow flight configurations.   For example: <code>{\"arrow_flight_config\": {\"timeout\": 900}}</code></li> <li>key \"hive_config\" to pass a dictionary of hive or tez configurations.   For example: <code>{\"hive_config\": {\"hive.tez.cpu.vcores\": 2, \"tez.grouping.split-count\": \"3\"}}</code> Defaults to <code>{}</code>.</li> </ul> </li> </ul> <p>Returns</p> <p><code>DataFrame</code>: DataFrame depending on the chosen type.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#show","title":"show","text":"<pre><code>Query.show(n, online=False)\n</code></pre> <p>Show the first N rows of the Query.</p> <p>Show the first 10 rows</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n\nquery.show(10)\n</code></pre> <p>Arguments</p> <ul> <li>n <code>int</code>: Number of rows to show.</li> <li>online <code>bool</code>: Show from online storage. Defaults to <code>False</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/query_api/#to_string","title":"to_string","text":"<pre><code>Query.to_string(online=False, arrow_flight=False)\n</code></pre> <p>Example</p> <pre><code>fg1 = fs.get_feature_group(\"...\")\nfg2 = fs.get_feature_group(\"...\")\n\nquery = fg1.select_all().join(fg2.select_all())\n\nquery.to_string()\n</code></pre>"},{"location":"generated/api/query_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/query_api/#featuregroups","title":"featuregroups","text":"<p>List of feature groups used in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#features","title":"features","text":"<p>List of all features in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#filters","title":"filters","text":"<p>All filters used in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#joins","title":"joins","text":"<p>List of joins in the query</p> <p>[source]</p>"},{"location":"generated/api/query_api/#left_feature_group_end_time","title":"left_feature_group_end_time","text":"<p>End time of time travel for the left feature group.</p> <p>[source]</p>"},{"location":"generated/api/query_api/#left_feature_group_start_time","title":"left_feature_group_start_time","text":"<p>Start time of time travel for the left feature group.</p>"},{"location":"generated/api/rule_api/","title":"Rule","text":"<p>{{rule}}</p>"},{"location":"generated/api/rule_api/#properties","title":"Properties","text":"<p>{{rule_properties}}</p>"},{"location":"generated/api/rule_definition_api/","title":"Rule Definition","text":"<p>{{ruledefinition}}</p>"},{"location":"generated/api/rule_definition_api/#properties","title":"Properties","text":"<p>{{ruledefinition_properties}}</p>"},{"location":"generated/api/rule_definition_api/#retrieval","title":"Retrieval","text":"<p>{{ruledefinition_getall}}</p> <p>{{ruledefinition_get}}</p>"},{"location":"generated/api/similarity_function_type_api/","title":"SimilarityFunctionType","text":"<p>[source]</p>"},{"location":"generated/api/similarity_function_type_api/#similarityfunctiontype_1","title":"SimilarityFunctionType","text":"<pre><code>hsfs.embedding.SimilarityFunctionType()\n</code></pre> <p>Enumeration class representing different types of similarity functions.</p> <p>Attributes</p> <ul> <li>L2 (str): Represents L2 norm similarity function.</li> <li>COSINE (str): Represents cosine similarity function.</li> <li>DOT_PRODUCT (str): Represents dot product similarity function.</li> </ul>"},{"location":"generated/api/spine_group_api/","title":"SpineGroup","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#spinegroup_1","title":"SpineGroup","text":"<pre><code>hsfs.feature_group.SpineGroup(\n    storage_connector=None,\n    query=None,\n    data_format=None,\n    path=None,\n    options=None,\n    name=None,\n    version=None,\n    description=None,\n    primary_key=None,\n    featurestore_id=None,\n    featurestore_name=None,\n    created=None,\n    creator=None,\n    id=None,\n    features=None,\n    location=None,\n    statistics_config=None,\n    event_time=None,\n    expectation_suite=None,\n    online_enabled=False,\n    href=None,\n    online_topic_name=None,\n    topic_name=None,\n    spine=True,\n    dataframe=None,\n    deprecated=False,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/spine_group_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_or_create_spine_group","title":"get_or_create_spine_group","text":"<pre><code>FeatureStore.get_or_create_spine_group(\n    name,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    event_time=None,\n    features=None,\n    dataframe=None,\n)\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date',\n                    dataframe=spine_df\n                    )\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> <p>Note</p> <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring: <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre></p> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the spine group to create.</li> <li>version <code>int | None</code>: Version of the spine group to retrieve, defaults to <code>None</code> and     will create the spine group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the spine group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     spine group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this spine group. If event_time is set     the spine group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li> <p>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the spine group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ dataframe__: DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features.</p> </li> </ul> <p>Returns</p> <p><code>SpineGroup</code>. The spine group metadata object.</p>"},{"location":"generated/api/spine_group_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_or_create_spine_group_1","title":"get_or_create_spine_group","text":"<pre><code>FeatureStore.get_or_create_spine_group(\n    name,\n    version=None,\n    description=\"\",\n    primary_key=None,\n    event_time=None,\n    features=None,\n    dataframe=None,\n)\n</code></pre> <p>Create a spine group metadata object.</p> <p>Instead of using a feature group to save a label/prediction target, you can use a spine together with a dataframe containing the labels. A Spine is essentially a metadata object similar to a feature group, however, the data is not materialized in the feature store. It only containes the needed metadata such as the relevant event time column and primary key columns to perform point-in-time correct joins.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nspine_df = pd.Dataframe()\n\nspine_group = fs.get_or_create_spine_group(\n                    name=\"sales\",\n                    version=1,\n                    description=\"Physical shop sales features\",\n                    primary_key=['ss_store_sk'],\n                    event_time='sale_date',\n                    dataframe=spine_df\n                    )\n</code></pre> <p>Note that you can inspect the dataframe in the spine group, or replace the dataframe:</p> <pre><code>spine_group.dataframe.show()\n\nspine_group.dataframe = new_df\n</code></pre> <p>The spine can then be used to construct queries, with only one speciality:</p> <p>Note</p> <p>Spines can only be used on the left side of a feature join, as this is the base set of entities for which features are to be fetched and the left side of the join determines the event timestamps to compare against.</p> <p>If you want to use the query for a feature view to be used for online serving, you can only select the label or target feature from the spine. For the online lookup, the label is not required, therefore it is important to only select label from the left feature group, so that we don't need to provide a spine for online serving.</p> <p>These queries can then be used to create feature views. Since the dataframe contained in the spine is not being materialized, every time you use a feature view created with spine to read data you will have to provide a dataframe with the same structure again.</p> <p>For example, to generate training data:</p> <pre><code>X_train, X_test, y_train, y_test = feature_view_spine.train_test_split(0.2, spine=training_data_entities)\n</code></pre> <p>Or to get batches of fresh data for batch scoring: <pre><code>feature_view_spine.get_batch_data(spine=scoring_entities_df).show()\n</code></pre></p> <p>Here you have the chance to pass a different set of entities to generate the training dataset.</p> <p>Sometimes it might be handy to create a feature view with a regular feature group containing the label, but then at serving time to use a spine in order to fetch features for example only for a small set of primary key values. To do this, you can pass the spine group instead of a dataframe. Just make sure it contains the needed primary key, event time and label column.</p> <pre><code>feature_view.get_batch_data(spine=spine_group)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the spine group to create.</li> <li>version <code>int | None</code>: Version of the spine group to retrieve, defaults to <code>None</code> and     will create the spine group with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the spine group to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>primary_key <code>List[str] | None</code>: A list of feature names to be used as primary key for the     spine group. This primary key can be a composite key of multiple     features and will be used as joining key, if not specified otherwise.     Defaults to empty list <code>[]</code>, and the spine group won't have any primary key.</li> <li>event_time <code>str | None</code>: Optionally, provide the name of the feature containing the event     time for the features in this spine group. If event_time is set     the spine group can be used for point-in-time joins. Defaults to <code>None</code>.</li> <li> <p>features <code>List[hsfs.feature.Feature] | None</code>: Optionally, define the schema of the spine group manually as a     list of <code>Feature</code> objects. Defaults to empty list <code>[]</code> and will use the     schema information of the DataFrame resulting by executing the provided query     against the data source.</p> <p>Event time data type restriction</p> <p>The supported data types for the event time column are: <code>timestamp</code>, <code>date</code> and <code>bigint</code>.</p> </li> <li> <p>__ dataframe__: DataFrame, RDD, Ndarray, list. Spine dataframe with primary key, event time and     label column to use for point in time join when fetching features.</p> </li> </ul> <p>Returns</p> <p><code>SpineGroup</code>. The spine group metadata object.</p>"},{"location":"generated/api/spine_group_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#avro_schema","title":"avro_schema","text":"<p>Avro schema representation of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#dataframe","title":"dataframe","text":"<p>Spine dataframe with primary key, event time and label column to use for point in time join when fetching features.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#deprecated","title":"deprecated","text":"<p>Setting if the feature group is deprecated.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#embedding_index","title":"embedding_index","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#event_time","title":"event_time","text":"<p>Event time feature in the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#feature_store","title":"feature_store","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#features","title":"features","text":"<p>Feature Group schema (alias)</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#name","title":"name","text":"<p>Name of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#notification_topic_name","title":"notification_topic_name","text":"<p>The topic used for feature group notifications.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#path","title":"path","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#primary_key","title":"primary_key","text":"<p>List of features building the primary key.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#schema","title":"schema","text":"<p>Feature Group schema</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#storage_connector","title":"storage_connector","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#topic_name","title":"topic_name","text":"<p>The topic used for feature group data ingestion.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#version","title":"version","text":"<p>Version number of the feature group.</p>"},{"location":"generated/api/spine_group_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/spine_group_api/#add_tag","title":"add_tag","text":"<pre><code>SpineGroup.add_tag(name, value)\n</code></pre> <p>Attach a tag to a feature group.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.add_tag(name=\"example_tag\", value=\"42\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value <code>Any</code>: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#check_deprecated","title":"check_deprecated","text":"<pre><code>SpineGroup.check_deprecated()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#create_feature_monitoring","title":"create_feature_monitoring","text":"<pre><code>SpineGroup.create_feature_monitoring(\n    name,\n    feature_name,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Enable feature monitoring to compare statistics on snapshots of feature data over time.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable feature monitoring\nmy_config = fg.create_feature_monitoring(\n    name=\"my_monitoring_config\",\n    feature_name=\"my_feature\",\n    description=\"my monitoring config description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Data inserted in the last day\n    time_offset=\"1d\",\n    window_length=\"1d\",\n).with_reference_window(\n    # Data inserted last week on the same day\n    time_offset=\"1w1d\",\n    window_length=\"1d\",\n).compare_on(\n    metric=\"mean\",\n    threshold=0.5,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str</code>: Name of the feature to monitor.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#create_statistics_monitoring","title":"create_statistics_monitoring","text":"<pre><code>SpineGroup.create_statistics_monitoring(\n    name,\n    feature_name=None,\n    description=None,\n    start_date_time=None,\n    end_date_time=None,\n    cron_expression=\"0 0 12 ? * * *\",\n)\n</code></pre> <p>Run a job to compute statistics on snapshot of feature data on a schedule.</p> <p>Experimental</p> <p>Public API is subject to change, this feature is not suitable for production use-cases.</p> <p>Example</p> <pre><code># fetch feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# enable statistics monitoring\nmy_config = fg.create_statistics_monitoring(\n    name=\"my_config\",\n    start_date_time=\"2021-01-01 00:00:00\",\n    description=\"my description\",\n    cron_expression=\"0 0 12 ? * * *\",\n).with_detection_window(\n    # Statistics computed on 10% of the last week of data\n    time_offset=\"1w\",\n    row_percentage=0.1,\n).save()\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the feature monitoring configuration.     name must be unique for all configurations attached to the feature group.</li> <li>feature_name <code>str | None</code>: Name of the feature to monitor. If not specified, statistics     will be computed for all features.</li> <li>description <code>str | None</code>: Description of the feature monitoring configuration.</li> <li>start_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: Start date and time from which to start computing statistics.</li> <li>end_date_time <code>int | str | datetime.datetime | datetime.date | pandas._libs.tslibs.timestamps.Timestamp | None</code>: End date and time at which to stop computing statistics.</li> <li>cron_expression <code>str | None</code>: Cron expression to use to schedule the job. The cron expression     must be in UTC and follow the Quartz specification. Default is '0 0 12 ? * * *',     every day at 12pm UTC.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>Return</p> <p><code>FeatureMonitoringConfig</code> Configuration with minimal information about the feature monitoring.     Additional information are required before feature monitoring is enabled.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#delete","title":"delete","text":"<pre><code>SpineGroup.delete()\n</code></pre> <p>Drop the entire feature group along with its feature data.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(\n        name='bitcoin_price',\n        version=1\n        )\n\n# delete the feature group\nfg.delete()\n</code></pre> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#delete_tag","title":"delete_tag","text":"<pre><code>SpineGroup.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.delete_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#filter","title":"filter","text":"<pre><code>SpineGroup.filter(f)\n</code></pre> <p>Apply filter to the feature group.</p> <p>Selects all features and returns the resulting <code>Query</code> with the applied filter.</p> <p>Example</p> <pre><code>from hsfs.feature import Feature\n\n# connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.filter(Feature(\"weekly_sales\") &gt; 1000)\n</code></pre> <p>If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group:</p> <p>Example</p> <pre><code>fg.filter(fg.feature1 == 1).show(10)\n</code></pre> <p>Composite filters require parenthesis and symbols for logical operands (e.g. <code>&amp;</code>, <code>|</code>, ...):</p> <p>Example</p> <pre><code>fg.filter((fg.feature1 == 1) | (fg.feature2 &gt;= 2))\n</code></pre> <p>Arguments</p> <ul> <li>f <code>hsfs.constructor.filter.Filter | hsfs.constructor.filter.Logic</code>: Filter object.</li> </ul> <p>Returns</p> <p><code>Query</code>. The query object with the applied filter.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_all_statistics","title":"get_all_statistics","text":"<pre><code>SpineGroup.get_all_statistics(computation_time=None, feature_names=None)\n</code></pre> <p>Returns all the statistics metadata computed before a specific time for the current feature group.</p> <p>If <code>computation_time</code> is <code>None</code>, all the statistics metadata are returned.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_statistics = fg.get_statistics(computation_time=None)\n</code></pre> <p>Arguments</p> <ul> <li>computation_time <code>str | int | float | datetime.datetime | datetime.date | None</code>: Date and time when statistics were computed. Defaults to <code>None</code>. Strings should     be formatted in one of the following formats <code>%Y-%m-%d</code>, <code>%Y-%m-%d %H</code>, <code>%Y-%m-%d %H:%M</code>, <code>%Y-%m-%d %H:%M:%S</code>,     or <code>%Y-%m-%d %H:%M:%S.%f</code>.</li> <li>feature_names <code>List[str] | None</code>: List of feature names of which statistics are retrieved.</li> </ul> <p>Returns</p> <p><code>Statistics</code>. Statistics object.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> <code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_complex_features","title":"get_complex_features","text":"<pre><code>SpineGroup.get_complex_features()\n</code></pre> <p>Returns the names of all features with a complex data type in this feature group.</p> <p>Example</p> <pre><code>complex_dtype_features = fg.get_complex_features()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_feature","title":"get_feature","text":"<pre><code>SpineGroup.get_feature(name)\n</code></pre> <p>Retrieve a <code>Feature</code> object from the schema of the feature group.</p> <p>There are several ways to access features of a feature group:</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\n# get Feature instanse\nfg.feature1\nfg[\"feature1\"]\nfg.get_feature(\"feature1\")\n</code></pre> <p>Note</p> <p>Attribute access to features works only for non-reserved names. For example features named <code>id</code> or <code>name</code> will not be accessible via <code>fg.name</code>, instead this will return the name of the feature group itself. Fall back on using the <code>get_feature</code> method.</p> <p>Arguments:</p> <p>name: The name of the feature to retrieve</p> <p>Returns:</p> <p>Feature: The feature object</p> <p>Raises</p> <p><code>hsfs.client.exceptions.FeatureStoreException</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_feature_monitoring_configs","title":"get_feature_monitoring_configs","text":"<pre><code>SpineGroup.get_feature_monitoring_configs(name=None, feature_name=None, config_id=None)\n</code></pre> <p>Fetch all feature monitoring configs attached to the feature group, or fetch by name or feature name only. If no arguments is provided the method will return all feature monitoring configs attached to the feature group, meaning all feature monitoring configs that are attach to a feature in the feature group. If you wish to fetch a single config, provide the its name. If you wish to fetch all configs attached to a particular feature, provide the feature name.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch all feature monitoring configs attached to the feature group\nfm_configs = fg.get_feature_monitoring_configs()\n\n# fetch a single feature monitoring config by name\nfm_config = fg.get_feature_monitoring_configs(name=\"my_config\")\n\n# fetch all feature monitoring configs attached to a particular feature\nfm_configs = fg.get_feature_monitoring_configs(feature_name=\"my_feature\")\n\n# fetch a single feature monitoring config with a given id\nfm_config = fg.get_feature_monitoring_configs(config_id=1)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str | None</code>: If provided fetch only the feature monitoring config with the given name.     Defaults to None.</li> <li>feature_name <code>str | None</code>: If provided, fetch only configs attached to a particular feature.     Defaults to None.</li> <li>config_id <code>int | None</code>: If provided, fetch only the feature monitoring config with the given id.     Defaults to None.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both name and feature_name are provided. - TypeError: if name or feature_name are not string or None.</p> <p>Return</p> <p>Union[<code>FeatureMonitoringConfig</code>, List[<code>FeatureMonitoringConfig</code>], None]     A list of feature monitoring configs. If name provided,     returns either a single config or None if not found.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_feature_monitoring_history","title":"get_feature_monitoring_history","text":"<pre><code>SpineGroup.get_feature_monitoring_history(\n    config_name=None, config_id=None, start_time=None, end_time=None, with_statistics=True\n)\n</code></pre> <p>Fetch feature monitoring history for a given feature monitoring config.</p> <p>Example</p> <pre><code># fetch your feature group\nfg = fs.get_feature_group(name=\"my_feature_group\", version=1)\n\n# fetch feature monitoring history for a given feature monitoring config\nfm_history = fg.get_feature_monitoring_history(\n    config_name=\"my_config\",\n    start_time=\"2020-01-01\",\n)\n\n# fetch feature monitoring history for a given feature monitoring config id\nfm_history = fg.get_feature_monitoring_history(\n    config_id=1,\n    start_time=datetime.now() - timedelta(weeks=2),\n    end_time=datetime.now() - timedelta(weeks=1),\n    with_statistics=False,\n)\n</code></pre> <p>Arguments</p> <ul> <li>config_name <code>str | None</code>: The name of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>config_id <code>int | None</code>: The id of the feature monitoring config to fetch history for.     Defaults to None.</li> <li>start_time <code>str | int | datetime.datetime | datetime.date | None</code>: The start date of the feature monitoring history to fetch.     Defaults to None.</li> <li>end_time <code>str | int | datetime.datetime | datetime.date | None</code>: The end date of the feature monitoring history to fetch.     Defaults to None.</li> <li>with_statistics <code>bool | None</code>: Whether to include statistics in the feature monitoring history.     Defaults to True. If False, only metadata about the monitoring will be fetched.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>. - ValueError: if both config_name and config_id are provided. - TypeError: if config_name or config_id are not respectively string, int or None.</p> <p>Return</p> <p>List[<code>FeatureMonitoringResult</code>]     A list of feature monitoring results containing the monitoring metadata     as well as the computed statistics for the detection and reference window     if requested.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_fg_name","title":"get_fg_name","text":"<pre><code>SpineGroup.get_fg_name()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_generated_feature_groups","title":"get_generated_feature_groups","text":"<pre><code>SpineGroup.get_generated_feature_groups()\n</code></pre> <p>Get the generated feature groups using this feature group, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_generated_feature_views","title":"get_generated_feature_views","text":"<pre><code>SpineGroup.get_generated_feature_views()\n</code></pre> <p>Get the generated feature view using this feature group, based on explicit provenance. These feature views can be accessible or inaccessible. Explicit provenance does not track deleted generated feature view links, so deleted will always be empty. For inaccessible feature views, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_parent_feature_groups","title":"get_parent_feature_groups","text":"<pre><code>SpineGroup.get_parent_feature_groups()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are feature groups or external feature groups. These feature groups can be accessible, deleted or inaccessible. For deleted and inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ProvenanceLinks</code>: Object containing the section of provenance graph requested.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>SpineGroup.get_storage_connector()\n</code></pre> <p>Get the storage connector using this feature group, based on explicit provenance. Only the accessible storage connector is returned. For more items use the base method - get_storage_connector_provenance</p> <p>Returns</p> <p>`StorageConnector: Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_storage_connector_provenance","title":"get_storage_connector_provenance","text":"<pre><code>SpineGroup.get_storage_connector_provenance()\n</code></pre> <p>Get the parents of this feature group, based on explicit provenance. Parents are storage connectors. These storage connector can be accessible, deleted or inaccessible. For deleted and inaccessible storage connector, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the storage connector used to generated this feature group</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_tag","title":"get_tag","text":"<pre><code>SpineGroup.get_tag(name)\n</code></pre> <p>Get the tags of a feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg_tag_value = fg.get_tag(\"example_tag\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#get_tags","title":"get_tags","text":"<pre><code>SpineGroup.get_tags()\n</code></pre> <p>Retrieves all tags attached to a feature group.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#json","title":"json","text":"<pre><code>SpineGroup.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#prepare_spark_location","title":"prepare_spark_location","text":"<pre><code>SpineGroup.prepare_spark_location()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select","title":"select","text":"<pre><code>SpineGroup.select(features)\n</code></pre> <p>Select a subset of features of the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select([\"id\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature]</code>: A list of <code>Feature</code> objects or feature names as     strings to be selected.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select_all","title":"select_all","text":"<pre><code>SpineGroup.select_all(include_primary_key=True, include_event_time=True)\n</code></pre> <p>Select all features in the feature group and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instances\nfg1 = fs.get_or_create_feature_group(...)\nfg2 = fs.get_or_create_feature_group(...)\n\n# construct the query\nquery = fg1.select_all().join(fg2.select_all())\n\n# show first 5 rows\nquery.show(5)\n\n\n# select all features exclude primary key and event time\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\nquery = fg.select_all()\nquery.features\n# [Feature('id', ...), Feature('ts', ...), Feature('f1', ...), Feature('f2', ...)]\n\nquery = fg.select_all(include_primary_key=False, include_event_time=False)\nquery.features\n# [Feature('f1', ...), Feature('f2', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>include_primary_key <code>bool | None</code>: If True, include primary key of the feature group     to the feature list. Defaults to True.</li> <li>include_event_time <code>bool | None</code>: If True, include event time of the feature group     to the feature list. Defaults to True.</li> </ul> <p>Returns</p> <p><code>Query</code>. A query object with all features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#select_except","title":"select_except","text":"<pre><code>SpineGroup.select_except(features=None)\n</code></pre> <p>Select all features including primary key and event time feature of the feature group except provided <code>features</code> and return a query object.</p> <p>The query can be used to construct joins of feature groups or create a feature view with a subset of features of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfrom hsfs.feature import Feature\nfg = fs.create_feature_group(\n        \"fg\",\n        features=[\n                Feature(\"id\", type=\"string\"),\n                Feature(\"ts\", type=\"bigint\"),\n                Feature(\"f1\", type=\"date\"),\n                Feature(\"f2\", type=\"double\")\n                ],\n        primary_key=[\"id\"],\n        event_time=\"ts\")\n\n# construct query\nquery = fg.select_except([\"ts\", \"f1\"])\nquery.features\n# [Feature('id', ...), Feature('f1', ...)]\n</code></pre> <p>Arguments</p> <ul> <li>features <code>List[str | hsfs.feature.Feature] | None</code>: A list of <code>Feature</code> objects or feature names as     strings to be excluded from the selection. Defaults to [],     selecting all features.</li> </ul> <p>Returns</p> <p><code>Query</code>: A query object with the selected features of the feature group.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#to_dict","title":"to_dict","text":"<pre><code>SpineGroup.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_deprecated","title":"update_deprecated","text":"<pre><code>SpineGroup.update_deprecated(deprecate=True)\n</code></pre> <p>Deprecate the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_deprecated(deprecate=True)\n</code></pre> <p>Safe update</p> <p>This method updates the feature group safely. In case of failure your local metadata object will be kept unchanged.</p> <p>Arguments</p> <ul> <li>deprecate <code>bool</code>: Boolean value identifying if the feature group should be deprecated. Defaults to True.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_description","title":"update_description","text":"<pre><code>SpineGroup.update_description(description)\n</code></pre> <p>Update the description of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_description(description=\"Much better description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_feature_description","title":"update_feature_description","text":"<pre><code>SpineGroup.update_feature_description(feature_name, description)\n</code></pre> <p>Update the description of a single feature in this feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_feature_description(feature_name=\"min_temp\",\n                              description=\"Much better feature description.\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature description safely. In case of failure your local metadata object will keep the old description.</p> <p>Arguments</p> <ul> <li>feature_name <code>str</code>: Name of the feature to be updated.</li> <li>description <code>str</code>: New description string.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_features","title":"update_features","text":"<pre><code>SpineGroup.update_features(features)\n</code></pre> <p>Update metadata of features in this feature group.</p> <p>Currently it's only supported to update the description of a feature.</p> <p>Unsafe update</p> <p>Note that if you use an existing <code>Feature</code> object of the schema in the feature group metadata object, this might leave your metadata object in a corrupted state if the update fails.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.feature.Feature | List[hsfs.feature.Feature]</code>: <code>Feature</code> or list of features. A feature object or list thereof to     be updated.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p> <p>[source]</p>"},{"location":"generated/api/spine_group_api/#update_notification_topic_name","title":"update_notification_topic_name","text":"<pre><code>SpineGroup.update_notification_topic_name(notification_topic_name)\n</code></pre> <p>Update the notification topic name of the feature group.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nfg.update_notification_topic_name(notification_topic_name=\"notification_topic_name\")\n</code></pre> <p>Safe update</p> <p>This method updates the feature group notification topic name safely. In case of failure your local metadata object will keep the old notification topic name.</p> <p>Arguments</p> <ul> <li>notification_topic_name <code>str</code>: Name of the topic used for sending notifications when entries     are inserted or updated on the online feature store. If set to None no notifications are sent.</li> </ul> <p>Returns</p> <p><code>FeatureGroup</code>. The updated feature group object.</p>"},{"location":"generated/api/split_statistics_api/","title":"Split Statistics","text":"<p>[source]</p>"},{"location":"generated/api/split_statistics_api/#splitstatistics","title":"SplitStatistics","text":"<pre><code>hsfs.split_statistics.SplitStatistics(\n    name,\n    feature_descriptive_statistics,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    type=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/split_statistics_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/split_statistics_api/#feature_descriptive_statistics","title":"feature_descriptive_statistics","text":"<p>List of feature descriptive statistics.</p> <p>[source]</p>"},{"location":"generated/api/split_statistics_api/#name","title":"name","text":"<p>Name of the training dataset split.</p>"},{"location":"generated/api/statistics_api/","title":"Statistics","text":"<p>[source]</p>"},{"location":"generated/api/statistics_api/#statistics_1","title":"Statistics","text":"<pre><code>hsfs.statistics.Statistics(\n    computation_time,\n    row_percentage=1.0,\n    feature_descriptive_statistics=None,\n    feature_group_id=None,\n    window_start_commit_time=None,\n    window_end_commit_time=None,\n    feature_view_name=None,\n    feature_view_version=None,\n    training_dataset_version=None,\n    split_statistics=None,\n    before_transformation=False,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    type=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/statistics_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/statistics_api/#before_transformation","title":"before_transformation","text":"<p>Whether or not the statistics were computed on feature values before applying model-dependent transformations.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#computation_time","title":"computation_time","text":"<p>Time at which the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_descriptive_statistics","title":"feature_descriptive_statistics","text":"<p>List of feature descriptive statistics.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_group_id","title":"feature_group_id","text":"<p>Id of the feature group on whose data the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_view_name","title":"feature_view_name","text":"<p>Name of the feature view whose query was used to retrieve the data on which the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#feature_view_version","title":"feature_view_version","text":"<p>Id of the feature view whose query was used to retrieve the data on which the statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#row_percentage","title":"row_percentage","text":"<p>Percentage of data on which statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#split_statistics","title":"split_statistics","text":"<p>List of statistics computed on each split of a training dataset.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#training_dataset_version","title":"training_dataset_version","text":"<p>Version of the training dataset on which statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#window_end_commit_time","title":"window_end_commit_time","text":"<p>End time of the window of data on which statistics were computed.</p> <p>[source]</p>"},{"location":"generated/api/statistics_api/#window_start_commit_time","title":"window_start_commit_time","text":"<p>Start time of the window of data on which statistics were computed.</p>"},{"location":"generated/api/statistics_config_api/","title":"StatisticsConfig","text":"<p>[source]</p>"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","title":"StatisticsConfig","text":"<pre><code>hsfs.statistics_config.StatisticsConfig(\n    enabled=True,\n    correlations=False,\n    histograms=False,\n    exact_uniqueness=False,\n    columns=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/statistics_config_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/statistics_config_api/#columns","title":"columns","text":"<p>Specify a subset of columns to compute statistics for.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#correlations","title":"correlations","text":"<p>Enable correlations as an additional statistic to be computed for each feature pair.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#enabled","title":"enabled","text":"<p>Enable statistics, by default this computes only descriptive statistics.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#exact_uniqueness","title":"exact_uniqueness","text":"<p>Enable exact uniqueness as an additional statistic to be computed for each feature.</p> <p>[source]</p>"},{"location":"generated/api/statistics_config_api/#histograms","title":"histograms","text":"<p>Enable histograms as an additional statistic to be computed for each feature.</p>"},{"location":"generated/api/storage_connector_api/","title":"Storage Connector","text":""},{"location":"generated/api/storage_connector_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_storage_connector","title":"get_storage_connector","text":"<pre><code>FeatureStore.get_storage_connector(name)\n</code></pre> <p>Get a previously created storage connector from the feature store.</p> <p>Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.</p> <p>If you want to connect to the online feature store, see the <code>get_online_storage_connector</code> method to get the JDBC connector for the Online Feature Store.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nsc = fs.get_storage_connector(\"demo_fs_meb10000_Training_Datasets\")\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the storage connector to retrieve.</li> </ul> <p>Returns</p> <p><code>StorageConnector</code>. Storage connector object.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","title":"get_online_storage_connector","text":"<pre><code>FeatureStore.get_online_storage_connector()\n</code></pre> <p>Get the storage connector for the Online Feature Store of the respective project's feature store.</p> <p>The returned storage connector depends on the project that you are connected to.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\nonline_storage_connector = fs.get_online_storage_connector()\n</code></pre> <p>Returns</p> <p><code>StorageConnector</code>. JDBC storage connector to the Online Feature Store.</p>"},{"location":"generated/api/storage_connector_api/#hopsfs","title":"HopsFS","text":""},{"location":"generated/api/storage_connector_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name","title":"name","text":"<p>Name of the storage connector.</p>"},{"location":"generated/api/storage_connector_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options","title":"connector_options","text":"<pre><code>HopsFSConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups","title":"get_feature_groups","text":"<pre><code>HopsFSConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance","title":"get_feature_groups_provenance","text":"<pre><code>HopsFSConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark","title":"prepare_spark","text":"<pre><code>HopsFSConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read","title":"read","text":"<pre><code>HopsFSConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a query or a path into a dataframe using the storage connector.</p> <p>Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: By default, the storage connector will read the table configured together     with the connector, if any. It's possible to overwrite this by passing a SQL     query here. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: When reading from object stores such as S3, HopsFS and ADLS, specify     the file format to be read, e.g. <code>csv</code>, <code>parquet</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the connector.</li> <li>path <code>str | None</code>: Path to be read from within the bucket of the storage connector. Not relevant     for JDBC or database based connectors such as Snowflake, JDBC or Redshift.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch","title":"refetch","text":"<pre><code>HopsFSConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options","title":"spark_options","text":"<pre><code>HopsFSConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict","title":"to_dict","text":"<pre><code>HopsFSConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>HopsFSConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#jdbc","title":"JDBC","text":""},{"location":"generated/api/storage_connector_api/#properties_1","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments","title":"arguments","text":"<p>Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the <code>driver</code> argument to <code>com.mysql.cj.jdbc.Driver</code> when creating the Storage Connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connection_string","title":"connection_string","text":"<p>JDBC connection string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_1","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_1","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_1","title":"name","text":"<p>Name of the storage connector.</p>"},{"location":"generated/api/storage_connector_api/#methods_1","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_1","title":"connector_options","text":"<pre><code>JdbcConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_1","title":"get_feature_groups","text":"<pre><code>JdbcConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_1","title":"get_feature_groups_provenance","text":"<pre><code>JdbcConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_1","title":"prepare_spark","text":"<pre><code>JdbcConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_1","title":"read","text":"<pre><code>JdbcConnector.read(query, data_format=None, options=None, path=None, dataframe_type=\"default\")\n</code></pre> <p>Reads a query into a dataframe using the storage connector.</p> <p>Arguments</p> <ul> <li>query <code>str</code>: A SQL query to be read.</li> <li>data_format <code>str | None</code>: Not relevant for JDBC based connectors.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the JDBC connector.</li> <li>path <code>str | None</code>: Not relevant for JDBC based connectors.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_1","title":"refetch","text":"<pre><code>JdbcConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_1","title":"spark_options","text":"<pre><code>JdbcConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_1","title":"to_dict","text":"<pre><code>JdbcConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_1","title":"update_from_response_json","text":"<pre><code>JdbcConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#s3","title":"S3","text":""},{"location":"generated/api/storage_connector_api/#properties_2","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#access_key","title":"access_key","text":"<p>Access key.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments_1","title":"arguments","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#bucket","title":"bucket","text":"<p>Return the bucket for S3 connectors.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_2","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#iam_role","title":"iam_role","text":"<p>IAM role.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_2","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_2","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#path","title":"path","text":"<p>If the connector refers to a path (e.g. S3) - return the path of the connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#region","title":"region","text":"<p>Return the region for S3 connectors.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#secret_key","title":"secret_key","text":"<p>Secret key.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","title":"server_encryption_algorithm","text":"<p>Encryption algorithm if server-side S3 bucket encryption is enabled.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#server_encryption_key","title":"server_encryption_key","text":"<p>Encryption key if server-side S3 bucket encryption is enabled.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#session_token","title":"session_token","text":"<p>Session token.</p>"},{"location":"generated/api/storage_connector_api/#methods_2","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_2","title":"connector_options","text":"<pre><code>S3Connector.connector_options()\n</code></pre> <p>Return options to be passed to an external S3 connector library</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_2","title":"get_feature_groups","text":"<pre><code>S3Connector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_2","title":"get_feature_groups_provenance","text":"<pre><code>S3Connector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_2","title":"prepare_spark","text":"<pre><code>S3Connector.prepare_spark(path=None)\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\n\nspark.read.format(\"json\").load(\"s3a://[bucket]/path\")\n\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"s3a://[bucket]/path\"))\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str | None</code>: Path to prepare for reading from cloud storage. Defaults to <code>None</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_2","title":"read","text":"<pre><code>S3Connector.read(query=None, data_format=None, options=None, path=\"\", dataframe_type=\"default\")\n</code></pre> <p>Reads a query or a path into a dataframe using the storage connector.</p> <p>Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: Not relevant for S3 connectors.</li> <li>data_format <code>str | None</code>: The file format of the files to be read, e.g. <code>csv</code>, <code>parquet</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the S3 connector.</li> <li>path <code>str</code>: Path within the bucket to be read.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_2","title":"refetch","text":"<pre><code>S3Connector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_2","title":"spark_options","text":"<pre><code>S3Connector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_2","title":"to_dict","text":"<pre><code>S3Connector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_2","title":"update_from_response_json","text":"<pre><code>S3Connector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#redshift","title":"Redshift","text":""},{"location":"generated/api/storage_connector_api/#properties_3","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments_2","title":"arguments","text":"<p>Additional JDBC, REDSHIFT, or Snowflake arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#auto_create","title":"auto_create","text":"<p>Database username for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#cluster_identifier","title":"cluster_identifier","text":"<p>Cluster identifier for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_driver","title":"database_driver","text":"<p>Database endpoint for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_endpoint","title":"database_endpoint","text":"<p>Database endpoint for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_group","title":"database_group","text":"<p>Database username for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_name","title":"database_name","text":"<p>Database name for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_password","title":"database_password","text":"<p>Database password for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_port","title":"database_port","text":"<p>Database port for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database_user_name","title":"database_user_name","text":"<p>Database username for redshift cluster.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_3","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#expiration","title":"expiration","text":"<p>Cluster temporary credential expiration time.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#iam_role_1","title":"iam_role","text":"<p>IAM role.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_3","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_3","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#table_name","title":"table_name","text":"<p>Table name for redshift cluster.</p>"},{"location":"generated/api/storage_connector_api/#methods_3","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_3","title":"connector_options","text":"<pre><code>RedshiftConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_3","title":"get_feature_groups","text":"<pre><code>RedshiftConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_3","title":"get_feature_groups_provenance","text":"<pre><code>RedshiftConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_3","title":"prepare_spark","text":"<pre><code>RedshiftConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_3","title":"read","text":"<pre><code>RedshiftConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a table or query into a dataframe using the storage connector.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: By default, the storage connector will read the table configured together     with the connector, if any. It's possible to overwrite this by passing a SQL     query here. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: Not relevant for JDBC based connectors such as Redshift.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the JDBC connector.</li> <li>path <code>str | None</code>: Not relevant for JDBC based connectors such as Redshift.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_3","title":"refetch","text":"<pre><code>RedshiftConnector.refetch()\n</code></pre> <p>Refetch storage connector in order to retrieve updated temporary credentials.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_3","title":"spark_options","text":"<pre><code>RedshiftConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_3","title":"to_dict","text":"<pre><code>RedshiftConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_3","title":"update_from_response_json","text":"<pre><code>RedshiftConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#azure-data-lake-storage","title":"Azure Data Lake Storage","text":""},{"location":"generated/api/storage_connector_api/#properties_4","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#account_name","title":"account_name","text":"<p>Account name of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#application_id","title":"application_id","text":"<p>Application ID of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#container_name","title":"container_name","text":"<p>Container name of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_4","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#directory_id","title":"directory_id","text":"<p>Directory ID of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#generation","title":"generation","text":"<p>Generation of the ADLS storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_4","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_4","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#path_1","title":"path","text":"<p>If the connector refers to a path (e.g. ADLS) - return the path of the connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#service_credential","title":"service_credential","text":"<p>Service credential of the ADLS storage connector</p>"},{"location":"generated/api/storage_connector_api/#methods_4","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_4","title":"connector_options","text":"<pre><code>AdlsConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_4","title":"get_feature_groups","text":"<pre><code>AdlsConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_4","title":"get_feature_groups_provenance","text":"<pre><code>AdlsConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_4","title":"prepare_spark","text":"<pre><code>AdlsConnector.prepare_spark(path=None)\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\n\nspark.read.format(\"json\").load(\"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\")\n\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\"))\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str | None</code>: Path to prepare for reading from cloud storage. Defaults to <code>None</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_4","title":"read","text":"<pre><code>AdlsConnector.read(\n    query=None, data_format=None, options=None, path=\"\", dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a path into a dataframe using the storage connector. Arguments</p> <ul> <li>query <code>str | None</code>: Not relevant for ADLS connectors.</li> <li>data_format <code>str | None</code>: The file format of the files to be read, e.g. <code>csv</code>, <code>parquet</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the ADLS connector.</li> <li>path <code>str</code>: Path within the bucket to be read. For example, path=<code>path</code> will read directly from the container specified on connector by constructing the URI as 'abfss://[container-name]@[account_name].dfs.core.windows.net/[path]'. If no path is specified default container path will be used from connector.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_4","title":"refetch","text":"<pre><code>AdlsConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_4","title":"spark_options","text":"<pre><code>AdlsConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_4","title":"to_dict","text":"<pre><code>AdlsConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_4","title":"update_from_response_json","text":"<pre><code>AdlsConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#snowflake","title":"Snowflake","text":""},{"location":"generated/api/storage_connector_api/#properties_5","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#account","title":"account","text":"<p>Account of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#application","title":"application","text":"<p>Application of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#database","title":"database","text":"<p>Database of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_5","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_5","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_5","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#options","title":"options","text":"<p>Additional options for the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#password","title":"password","text":"<p>Password of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#role","title":"role","text":"<p>Role of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#schema","title":"schema","text":"<p>Schema of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#table","title":"table","text":"<p>Table of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#token","title":"token","text":"<p>OAuth token of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#url","title":"url","text":"<p>URL of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#user","title":"user","text":"<p>User of the Snowflake storage connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#warehouse","title":"warehouse","text":"<p>Warehouse of the Snowflake storage connector</p>"},{"location":"generated/api/storage_connector_api/#methods_5","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_5","title":"connector_options","text":"<pre><code>SnowflakeConnector.connector_options()\n</code></pre> <p>In order to use the <code>snowflake.connector</code> Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database.</p> <pre><code>import snowflake.connector\n\nsc = fs.get_storage_connector(\"snowflake_conn\")\nctx = snowflake.connector.connect(**sc.connector_options())\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_5","title":"get_feature_groups","text":"<pre><code>SnowflakeConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_5","title":"get_feature_groups_provenance","text":"<pre><code>SnowflakeConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_5","title":"prepare_spark","text":"<pre><code>SnowflakeConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_5","title":"read","text":"<pre><code>SnowflakeConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads a table or query into a dataframe using the storage connector.</p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: By default, the storage connector will read the table configured together     with the connector, if any. It's possible to overwrite this by passing a SQL     query here. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: Not relevant for Snowflake connectors.</li> <li>options <code>Dict[str, Any] | None</code>: Any additional key/value options to be passed to the engine.</li> <li>path <code>str | None</code>: Not relevant for Snowflake connectors.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_5","title":"refetch","text":"<pre><code>SnowflakeConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#snowflake_connector_options","title":"snowflake_connector_options","text":"<pre><code>SnowflakeConnector.snowflake_connector_options()\n</code></pre> <p>Alias for <code>connector_options</code></p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_5","title":"spark_options","text":"<pre><code>SnowflakeConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_5","title":"to_dict","text":"<pre><code>SnowflakeConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_5","title":"update_from_response_json","text":"<pre><code>SnowflakeConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>This storage connector provides integration to Google Cloud Storage (GCS). Once you create a connector in FeatureStore, you can transact data from a GCS bucket into a spark dataframe by calling the <code>read</code> API.</p> <p>Authentication to GCP is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</p> <p>The connector also supports the optional encryption method <code>Customer Supplied Encryption Key</code> by Google. The encryption details are stored as <code>Secrets</code> in the FeatureStore for keeping it secure. Read more about encryption on Google Documentation.</p> <p>The storage connector uses the Google <code>gcs-connector-hadoop</code> behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop</p>"},{"location":"generated/api/storage_connector_api/#properties_6","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#algorithm","title":"algorithm","text":"<p>Encryption Algorithm</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#bucket_1","title":"bucket","text":"<p>GCS Bucket</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_6","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#encryption_key","title":"encryption_key","text":"<p>Encryption Key</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#encryption_key_hash","title":"encryption_key_hash","text":"<p>Encryption Key Hash</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_6","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#key_path","title":"key_path","text":"<p>JSON keyfile for service account</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_6","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#path_2","title":"path","text":"<p>the path of the connector along with gs file system prefixed</p>"},{"location":"generated/api/storage_connector_api/#methods_6","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_6","title":"connector_options","text":"<pre><code>GcsConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_6","title":"get_feature_groups","text":"<pre><code>GcsConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_6","title":"get_feature_groups_provenance","text":"<pre><code>GcsConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_6","title":"prepare_spark","text":"<pre><code>GcsConnector.prepare_spark(path=None)\n</code></pre> <p>Prepare Spark to use this Storage Connector.</p> <pre><code>conn.prepare_spark()\nspark.read.format(\"json\").load(\"gs://bucket/path\")\n# or\nspark.read.format(\"json\").load(conn.prepare_spark(\"gs://bucket/path\"))\n</code></pre> <p>Arguments</p> <ul> <li>path <code>str | None</code>: Path to prepare for reading from Google cloud storage. Defaults to <code>None</code>.</li> </ul> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_6","title":"read","text":"<pre><code>GcsConnector.read(\n    query=None, data_format=None, options=None, path=\"\", dataframe_type=\"default\"\n)\n</code></pre> <p>Reads GCS path into a dataframe using the storage connector.</p> <p>To read directly from the default bucket, you can omit the path argument: <pre><code>conn.read(data_format='spark_formats')\n</code></pre> Or to read objects from default bucket provide the object path without gsUtil URI schema. For example, following will read from a path gs://bucket_on_connector/Path/object : <pre><code>conn.read(data_format='spark_formats', paths='Path/object')\n</code></pre> Or to read with full gsUtil URI path, <pre><code>conn.read(data_format='spark_formats',path='gs://BUCKET/DATA')\n</code></pre> Arguments</p> <ul> <li>query <code>str | None</code>: Not relevant for GCS connectors.</li> <li>data_format <code>str | None</code>: Spark data format. Defaults to <code>None</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Spark options. Defaults to <code>None</code>.</li> <li>path <code>str</code>: GCS path. Defaults to <code>None</code>.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: Malformed arguments.</li> </ul> <p>Returns</p> <p><code>Dataframe</code>: A Spark dataframe.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_6","title":"refetch","text":"<pre><code>GcsConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_6","title":"spark_options","text":"<pre><code>GcsConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_6","title":"to_dict","text":"<pre><code>GcsConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_6","title":"update_from_response_json","text":"<pre><code>GcsConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#bigquery","title":"BigQuery","text":"<p>The BigQuery storage connector provides integration to Google Cloud BigQuery. You can use it to run bigquery on your GCP cluster and load results into spark dataframe by calling the <code>read</code> API.</p> <p>Authentication to GCP is handled by uploading the <code>JSON keyfile for service account</code> to the Hopsworks Project. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.</p> <p>The storage connector uses the Google <code>spark-bigquery-connector</code> behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery.</p>"},{"location":"generated/api/storage_connector_api/#properties_7","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#arguments_3","title":"arguments","text":"<p>Additional spark options</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#dataset","title":"dataset","text":"<p>BigQuery dataset (The dataset containing the table)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_7","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_7","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#key_path_1","title":"key_path","text":"<p>JSON keyfile for service account</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#materialization_dataset","title":"materialization_dataset","text":"<p>BigQuery materialization dataset (The dataset where the materialized view is going to be created, used in case of query)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_7","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#parent_project","title":"parent_project","text":"<p>BigQuery parent project (Google Cloud Project ID of the table to bill for the export)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#query_project","title":"query_project","text":"<p>BigQuery project (The Google Cloud Project ID of the table)</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#query_table","title":"query_table","text":"<p>BigQuery table name</p>"},{"location":"generated/api/storage_connector_api/#methods_7","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_7","title":"connector_options","text":"<pre><code>BigQueryConnector.connector_options()\n</code></pre> <p>Return options to be passed to an external BigQuery connector library</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_7","title":"get_feature_groups","text":"<pre><code>BigQueryConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_7","title":"get_feature_groups_provenance","text":"<pre><code>BigQueryConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_7","title":"prepare_spark","text":"<pre><code>BigQueryConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_7","title":"read","text":"<pre><code>BigQueryConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>Reads results from BigQuery into a spark dataframe using the storage connector.</p> <p>Reading from bigquery is done via either specifying the BigQuery table or BigQuery query.   For example, to read from a BigQuery table, set the BigQuery project, dataset and table on storage connector   and read directly from the corresponding path.     <pre><code>conn.read()\n</code></pre>   OR, to read results from a BigQuery query, set <code>Materialization Dataset</code> on storage connector,    and pass your SQL to <code>query</code> argument.     <pre><code>conn.read(query='SQL')\n</code></pre>   Optionally, passing <code>query</code> argument will take priority at runtime if the table options were also set   on the storage connector. This allows user to run from both a query or table with same connector, assuming   all fields were set.   Also, user can set the <code>path</code> argument to a bigquery table path to read at runtime,    if table options were not set initially while creating the connector.     <pre><code>conn.read(path='project.dataset.table')\n</code></pre></p> <p>Arguments</p> <ul> <li>query <code>str | None</code>: BigQuery query. Defaults to <code>None</code>.</li> <li>data_format <code>str | None</code>: Spark data format. Defaults to <code>None</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Spark options. Defaults to <code>None</code>.</li> <li>path <code>str | None</code>: BigQuery table path. Defaults to <code>None</code>.</li> <li>dataframe_type <code>str</code>: str, optional. The type of the returned dataframe.     Possible values are <code>\"default\"</code>, <code>\"spark\"</code>,<code>\"pandas\"</code>, <code>\"polars\"</code>, <code>\"numpy\"</code> or <code>\"python\"</code>.     Defaults to \"default\", which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Python engine.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: Malformed arguments.</li> </ul> <p>Returns</p> <p><code>Dataframe</code>: A Spark dataframe.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_7","title":"refetch","text":"<pre><code>BigQueryConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_7","title":"spark_options","text":"<pre><code>BigQueryConnector.spark_options()\n</code></pre> <p>Return spark options to be set for BigQuery spark connector</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_7","title":"to_dict","text":"<pre><code>BigQueryConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_7","title":"update_from_response_json","text":"<pre><code>BigQueryConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/storage_connector_api/#kafka","title":"Kafka","text":""},{"location":"generated/api/storage_connector_api/#properties_8","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#bootstrap_servers","title":"bootstrap_servers","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#description_8","title":"description","text":"<p>User provided description of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#id_8","title":"id","text":"<p>Id of the storage connector uniquely identifying it in the Feature store.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#name_8","title":"name","text":"<p>Name of the storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#options_1","title":"options","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#security_protocol","title":"security_protocol","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#ssl_endpoint_identification_algorithm","title":"ssl_endpoint_identification_algorithm","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#ssl_keystore_location","title":"ssl_keystore_location","text":"<p>Bootstrap servers string.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#ssl_truststore_location","title":"ssl_truststore_location","text":"<p>Bootstrap servers string.</p>"},{"location":"generated/api/storage_connector_api/#methods_8","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/storage_connector_api/#confluent_options","title":"confluent_options","text":"<pre><code>KafkaConnector.confluent_options()\n</code></pre> <p>Return prepared options to be passed to confluent_kafka, based on the provided apache spark configuration. Right now only producer values with Importance &gt;= medium are implemented. https://docs.confluent.io/platform/current/clients/librdkafka/html/md_CONFIGURATION.html</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#connector_options_8","title":"connector_options","text":"<pre><code>KafkaConnector.connector_options()\n</code></pre> <p>Return prepared options to be passed to an external connector library. Not implemented for this connector type.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_8","title":"get_feature_groups","text":"<pre><code>KafkaConnector.get_feature_groups()\n</code></pre> <p>Get the feature groups using this storage connector, based on explicit provenance. Only the accessible feature groups are returned. For more items use the base method - get_feature_groups_provenance</p> <p>Returns</p> <p>`List[FeatureGroup]: List of feature groups.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#get_feature_groups_provenance_8","title":"get_feature_groups_provenance","text":"<pre><code>KafkaConnector.get_feature_groups_provenance()\n</code></pre> <p>Get the generated feature groups using this storage connector, based on explicit provenance. These feature groups can be accessible or inaccessible. Explicit provenance does not track deleted generated feature group links, so deleted will always be empty. For inaccessible feature groups, only a minimal information is returned.</p> <p>Returns</p> <p><code>ExplicitProvenance.Links</code>: the feature groups generated using this storage connector</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#kafka_options","title":"kafka_options","text":"<pre><code>KafkaConnector.kafka_options()\n</code></pre> <p>Return prepared options to be passed to kafka, based on the additional arguments. https://kafka.apache.org/documentation/</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#prepare_spark_8","title":"prepare_spark","text":"<pre><code>KafkaConnector.prepare_spark(path=None)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_8","title":"read","text":"<pre><code>KafkaConnector.read(\n    query=None, data_format=None, options=None, path=None, dataframe_type=\"default\"\n)\n</code></pre> <p>NOT SUPPORTED.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#read_stream","title":"read_stream","text":"<pre><code>KafkaConnector.read_stream(\n    topic,\n    topic_pattern=False,\n    message_format=\"avro\",\n    schema=None,\n    options=None,\n    include_metadata=False,\n)\n</code></pre> <p>Reads a Kafka stream from a topic or multiple topics into a Dataframe.</p> <p>Engine Support</p> <p>Spark only</p> <p>Reading from data streams using Pandas/Python as engine is currently not supported. Python/Pandas has no notion of streaming.</p> <p>Arguments</p> <ul> <li>topic <code>str</code>: Name or pattern of the topic(s) to subscribe to.</li> <li>topic_pattern <code>bool</code>: Flag to indicate if <code>topic</code> string is a pattern.     Defaults to <code>False</code>.</li> <li>message_format <code>str</code>: The format of the messages to use for decoding.     Can be <code>\"avro\"</code> or <code>\"json\"</code>. Defaults to <code>\"avro\"</code>.</li> <li>schema <code>str | None</code>: Optional schema, to use for decoding, can be an Avro schema string for     <code>\"avro\"</code> message format, or for JSON encoding a Spark StructType schema,     or a DDL formatted string. Defaults to <code>None</code>.</li> <li>options <code>Dict[str, Any] | None</code>: Additional options as key/value string pairs to be passed to Spark.     Defaults to <code>{}</code>.</li> <li>include_metadata <code>bool</code>: Indicate whether to return additional metadata fields from     messages in the stream. Otherwise, only the decoded value fields are     returned. Defaults to <code>False</code>.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: Malformed arguments.</li> </ul> <p>Returns</p> <p><code>StreamingDataframe</code>: A Spark streaming dataframe.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#refetch_8","title":"refetch","text":"<pre><code>KafkaConnector.refetch()\n</code></pre> <p>Refetch storage connector.</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#spark_options_8","title":"spark_options","text":"<pre><code>KafkaConnector.spark_options()\n</code></pre> <p>Return prepared options to be passed to Spark, based on the additional arguments. This is done by just adding 'kafka.' prefix to kafka_options. https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations</p> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#to_dict_8","title":"to_dict","text":"<pre><code>KafkaConnector.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/storage_connector_api/#update_from_response_json_8","title":"update_from_response_json","text":"<pre><code>KafkaConnector.update_from_response_json(json_dict)\n</code></pre>"},{"location":"generated/api/training_dataset_api/","title":"Training Dataset","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#trainingdataset","title":"TrainingDataset","text":"<pre><code>hsfs.training_dataset.TrainingDataset(\n    name,\n    version,\n    data_format,\n    featurestore_id,\n    location=\"\",\n    event_start_time=None,\n    event_end_time=None,\n    coalesce=False,\n    description=None,\n    storage_connector=None,\n    splits=None,\n    validation_size=None,\n    test_size=None,\n    train_start=None,\n    train_end=None,\n    validation_start=None,\n    validation_end=None,\n    test_start=None,\n    test_end=None,\n    seed=None,\n    created=None,\n    creator=None,\n    features=None,\n    statistics_config=None,\n    featurestore_name=None,\n    id=None,\n    inode_id=None,\n    training_dataset_type=None,\n    from_query=None,\n    querydto=None,\n    label=None,\n    transformation_functions=None,\n    train_split=None,\n    time_split_size=None,\n    extra_filter=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/training_dataset_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#create_training_dataset","title":"create_training_dataset","text":"<pre><code>FeatureStore.create_training_dataset(\n    name,\n    version=None,\n    description=\"\",\n    data_format=\"tfrecords\",\n    coalesce=False,\n    storage_connector=None,\n    splits=None,\n    location=\"\",\n    seed=None,\n    statistics_config=None,\n    label=None,\n    transformation_functions=None,\n    train_split=None,\n)\n</code></pre> <p>Create a training dataset metadata object.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. From version 3.0 training datasets created with this API are not visibile in the API anymore.</p> <p>Lazy</p> <p>This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the <code>save()</code> method with a <code>DataFrame</code> or <code>Query</code>.</p> <p>Data Formats</p> <p>The feature store currently supports the following data formats for training datasets:</p> <ol> <li>tfrecord</li> <li>csv</li> <li>tsv</li> <li>parquet</li> <li>avro</li> <li>orc</li> </ol> <p>Currently not supported petastorm, hdf5 and npy file formats.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to create.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and     will create the training dataset with incremented version from the last     version in the feature store.</li> <li>description <code>str | None</code>: A string describing the contents of the training dataset to     improve discoverability for Data Scientists, defaults to empty string     <code>\"\"</code>.</li> <li>data_format <code>str | None</code>: The data format used to save the training dataset,     defaults to <code>\"tfrecords\"</code>-format.</li> <li>coalesce <code>bool | None</code>: If true the training dataset data will be coalesced into     a single partition before writing. The resulting training dataset     will be a single file per split. Default False.</li> <li>storage_connector <code>hsfs.StorageConnector | None</code>: Storage connector defining the sink location for the     training dataset, defaults to <code>None</code>, and materializes training dataset     on HopsFS.</li> <li>splits <code>Dict[str, float] | None</code>: A dictionary defining training dataset splits to be created. Keys in     the dictionary define the name of the split as <code>str</code>, values represent     percentage of samples in the split as <code>float</code>. Currently, only random     splits are supported. Defaults to empty dict<code>{}</code>, creating only a single     training dataset without splits.</li> <li>location <code>str | None</code>: Path to complement the sink storage connector with, e.g if the     storage connector points to an S3 bucket, this path can be used to     define a sub-directory inside the bucket to place the training dataset.     Defaults to <code>\"\"</code>, saving the training dataset at the root defined by the     storage connector.</li> <li>seed <code>int | None</code>: Optionally, define a seed to create the random splits with, in order     to guarantee reproducability, defaults to <code>None</code>.</li> <li>statistics_config <code>hsfs.StatisticsConfig | bool | dict | None</code>: A configuration object, or a dictionary with keys     \"<code>enabled</code>\" to generally enable descriptive statistics computation for     this feature group, <code>\"correlations</code>\" to turn on feature correlation     computation and <code>\"histograms\"</code> to compute feature value frequencies. The     values should be booleans indicating the setting. To fully turn off     statistics computation pass <code>statistics_config=False</code>. Defaults to     <code>None</code> and will compute only descriptive statistics.</li> <li>label <code>List[str] | None</code>: A list of feature names constituting the prediction label/feature of     the training dataset. When replaying a <code>Query</code> during model inference,     the label features can be omitted from the feature vector retrieval.     Defaults to <code>[]</code>, no label.</li> <li>transformation_functions <code>Dict[str, hsfs.transformation_function.TransformationFunction] | None</code>: A dictionary mapping tansformation functions to     to the features they should be applied to before writing out the     training data and at inference time. Defaults to <code>{}</code>, no     transformations.</li> <li>train_split <code>str | None</code>: If <code>splits</code> is set, provide the name of the split that is going     to be used for training. The statistics of this split will be used for     transformation functions if necessary. Defaults to <code>None</code>.</li> </ul> <p>Returns:</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p>"},{"location":"generated/api/training_dataset_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_training_dataset","title":"get_training_dataset","text":"<pre><code>FeatureStore.get_training_dataset(name, version=None)\n</code></pre> <p>Get a training dataset entity from the feature store.</p> <p>Deprecated</p> <p><code>TrainingDataset</code> is deprecated, use <code>FeatureView</code> instead. You can still retrieve old training datasets using this method, but after upgrading the old training datasets will also be available under a Feature View with the same name and version.</p> <p>It is recommended to use this method only for old training datasets that have been created directly from Dataframes and not with Query objects.</p> <p>Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the training dataset to get.</li> <li>version <code>int | None</code>: Version of the training dataset to retrieve, defaults to <code>None</code> and will     return the <code>version=1</code>.</li> </ul> <p>Returns</p> <p><code>TrainingDataset</code>: The training dataset metadata object.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: If unable to retrieve training dataset from the feature store.</li> </ul>"},{"location":"generated/api/training_dataset_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#coalesce","title":"coalesce","text":"<p>If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#data_format","title":"data_format","text":"<p>File format of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#description","title":"description","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#event_end_time","title":"event_end_time","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#event_start_time","title":"event_start_time","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#extra_filter","title":"extra_filter","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#feature_store_id","title":"feature_store_id","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#feature_store_name","title":"feature_store_name","text":"<p>Name of the feature store in which the feature group is located.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#id","title":"id","text":"<p>Training dataset id.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#label","title":"label","text":"<p>The label/prediction feature of the training dataset.</p> <p>Can be a composite of multiple features.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#location","title":"location","text":"<p>Path to the training dataset location. Can be an empty string if e.g. the training dataset is in-memory.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#name","title":"name","text":"<p>Name of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#query","title":"query","text":"<p>Query to generate this training dataset from online feature store.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#schema","title":"schema","text":"<p>Training dataset schema.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#seed","title":"seed","text":"<p>Seed used to perform random split, ensure reproducibility of the random split at a later date.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#serving_keys","title":"serving_keys","text":"<p>Set of primary key names that is used as keys in input dict object for <code>get_serving_vector</code> method.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#splits","title":"splits","text":"<p>Training dataset splits. <code>train</code>, <code>test</code> or <code>eval</code> and corresponding percentages.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#statistics","title":"statistics","text":"<p>Get computed statistics for the training dataset.</p> <p>Returns</p> <p><code>Statistics</code>. Object with statistics information.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#statistics_config","title":"statistics_config","text":"<p>Statistics configuration object defining the settings for statistics computation of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#storage_connector","title":"storage_connector","text":"<p>Storage connector.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#test_end","title":"test_end","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#test_size","title":"test_size","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#test_start","title":"test_start","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#train_end","title":"train_end","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#train_split","title":"train_split","text":"<p>Set name of training dataset split that is used for training.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#train_start","title":"train_start","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#training_dataset_type","title":"training_dataset_type","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#transformation_functions","title":"transformation_functions","text":"<p>Set transformation functions.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#validation_end","title":"validation_end","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#validation_size","title":"validation_size","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#validation_start","title":"validation_start","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#version","title":"version","text":"<p>Version number of the training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#write_options","title":"write_options","text":"<p>User provided options to write training dataset.</p>"},{"location":"generated/api/training_dataset_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/training_dataset_api/#add_tag","title":"add_tag","text":"<pre><code>TrainingDataset.add_tag(name, value)\n</code></pre> <p>Attach a tag to a training dataset.</p> <p>A tag consists of a  pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be added.</li> <li>value: Value of the tag to be added.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to add the tag.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#compute_statistics","title":"compute_statistics","text":"<pre><code>TrainingDataset.compute_statistics()\n</code></pre> <p>Compute the statistics for the training dataset and save them to the feature store.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#delete","title":"delete","text":"<pre><code>TrainingDataset.delete()\n</code></pre> <p>Delete training dataset and all associated metadata.</p> <p>Drops only HopsFS data</p> <p>Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store.</p> <p>Potentially dangerous operation</p> <p>This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#delete_tag","title":"delete_tag","text":"<pre><code>TrainingDataset.delete_tag(name)\n</code></pre> <p>Delete a tag attached to a training dataset.</p> <p>Arguments</p> <ul> <li>name <code>str</code>: Name of the tag to be removed.</li> </ul> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to delete the tag.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#from_response_json","title":"from_response_json","text":"<pre><code>TrainingDataset.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#from_response_json_single","title":"from_response_json_single","text":"<pre><code>TrainingDataset.from_response_json_single(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_query","title":"get_query","text":"<pre><code>TrainingDataset.get_query(online=True, with_label=False)\n</code></pre> <p>Returns the query used to generate this training dataset</p> <p>Arguments</p> <ul> <li>online <code>bool</code>: boolean, optional. Return the query for the online storage, else     for offline storage, defaults to <code>True</code> - for online storage.</li> <li>with_label <code>bool</code>: Indicator whether the query should contain features which were     marked as prediction label/feature when the training dataset was     created, defaults to <code>False</code>.</li> </ul> <p>Returns</p> <p><code>str</code>. Query string for the chosen storage used to generate this training     dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_serving_vector","title":"get_serving_vector","text":"<pre><code>TrainingDataset.get_serving_vector(entry, external=None)\n</code></pre> <p>Returns assembled serving vector from online feature store.</p> <p>Arguments</p> <ul> <li>entry <code>Dict[str, Any]</code>: dictionary of training dataset feature group primary key names as keys and values provided by     serving application.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>Returns</p> <p><code>list</code> List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_serving_vectors","title":"get_serving_vectors","text":"<pre><code>TrainingDataset.get_serving_vectors(entry, external=None)\n</code></pre> <p>Returns assembled serving vectors in batches from online feature store.</p> <p>Arguments</p> <ul> <li>entry <code>Dict[str, List[Any]]</code>: dict of feature group primary key names as keys and value as list of primary keys provided by     serving application.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>Returns</p> <p><code>List[list]</code> List of lists of feature values related to provided primary keys, ordered according to positions of this features in training dataset query.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_tag","title":"get_tag","text":"<pre><code>TrainingDataset.get_tag(name)\n</code></pre> <p>Get the tags of a training dataset.</p> <p>Arguments</p> <ul> <li>name: Name of the tag to get.</li> </ul> <p>Returns</p> <p>tag value</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tag.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#get_tags","title":"get_tags","text":"<pre><code>TrainingDataset.get_tags()\n</code></pre> <p>Returns all tags attached to a training dataset.</p> <p>Returns</p> <p><code>Dict[str, obj]</code> of tags.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code> in case the backend fails to retrieve the tags.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#init_prepared_statement","title":"init_prepared_statement","text":"<pre><code>TrainingDataset.init_prepared_statement(batch=None, external=None)\n</code></pre> <p>Initialise and cache parametrized prepared statement to    retrieve feature vector from online feature store.</p> <p>Arguments</p> <ul> <li>batch <code>bool | None</code>: boolean, optional. If set to True, prepared statements will be     initialised for retrieving serving vectors as a batch.</li> <li>external <code>bool | None</code>: boolean, optional. If set to True, the connection to the     online feature store is established using the same host as     for the <code>host</code> parameter in the <code>hsfs.connection()</code> method.     If set to False, the online feature store storage connector is used     which relies on the private IP. Defaults to True if connection to Hopsworks is established from     external environment (e.g AWS Sagemaker or Google Colab), otherwise to False.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#insert","title":"insert","text":"<pre><code>TrainingDataset.insert(features, overwrite, write_options=None)\n</code></pre> <p>Insert additional feature data into the training dataset.</p> <p>Deprecated</p> <p><code>insert</code> method is deprecated.</p> <p>This method appends data to the training dataset either from a Feature Store <code>Query</code>, a Spark or Pandas <code>DataFrame</code>, a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation.</p> <p>This can also be used to overwrite all data in an existing training dataset.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.constructor.query.Query | pandas.DataFrame | hsfs.training_dataset.pyspark.sql.DataFrame | hsfs.training_dataset.pyspark.RDD | numpy.ndarray | List[list]</code>: Feature data to be materialized.</li> <li>overwrite <code>bool</code>: Whether to overwrite the entire data in the training dataset.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> </ul> </li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: Unable to create training dataset metadata.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#json","title":"json","text":"<pre><code>TrainingDataset.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#read","title":"read","text":"<pre><code>TrainingDataset.read(split=None, read_options=None)\n</code></pre> <p>Read the training dataset into a dataframe.</p> <p>It is also possible to read only a specific split.</p> <p>Arguments</p> <ul> <li>split: Name of the split to read, defaults to <code>None</code>, reading the entire     training dataset. If the training dataset has split, the <code>split</code> parameter     is mandatory.</li> <li>read_options: Additional read options as key/value pairs, defaults to <code>{}</code>.</li> </ul> <p>Returns</p> <p><code>DataFrame</code>: The spark dataframe containing the feature data of the     training dataset.</p> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#save","title":"save","text":"<pre><code>TrainingDataset.save(features, write_options=None)\n</code></pre> <p>Materialize the training dataset to storage.</p> <p>This method materializes the training dataset either from a Feature Store <code>Query</code>, a Spark or Pandas <code>DataFrame</code>, a Spark RDD, two-dimensional Python lists or Numpy ndarrays. From v2.5 onward, filters are saved along with the <code>Query</code>.</p> <p>Engine Support</p> <p>Creating Training Datasets from Dataframes is only supported using Spark as Engine.</p> <p>Arguments</p> <ul> <li>features <code>hsfs.constructor.query.Query | pandas.DataFrame | hsfs.training_dataset.pyspark.sql.DataFrame | hsfs.training_dataset.pyspark.RDD | numpy.ndarray | List[list]</code>: Feature data to be materialized.</li> <li>write_options <code>Dict[Any, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to compute the training dataset.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the save call should return only   after the Hopsworks Job has finished. By default it waits.</li> </ul> </li> </ul> <p>Returns</p> <p><code>Job</code>: When using the <code>python</code> engine, it returns the Hopsworks Job     that was launched to create the training dataset.</p> <p>Raises</p> <ul> <li><code>hsfs.client.exceptions.RestAPIError</code>: Unable to create training dataset metadata.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#show","title":"show","text":"<pre><code>TrainingDataset.show(n, split=None)\n</code></pre> <p>Show the first <code>n</code> rows of the training dataset.</p> <p>You can specify a split from which to retrieve the rows.</p> <p>Arguments</p> <ul> <li>n <code>int</code>: Number of rows to show.</li> <li>split <code>str | None</code>: Name of the split to show, defaults to <code>None</code>, showing the first rows     when taking all splits together.</li> </ul> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#to_dict","title":"to_dict","text":"<pre><code>TrainingDataset.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#update_from_response_json","title":"update_from_response_json","text":"<pre><code>TrainingDataset.update_from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/training_dataset_api/#update_statistics_config","title":"update_statistics_config","text":"<pre><code>TrainingDataset.update_statistics_config()\n</code></pre> <p>Update the statistics configuration of the training dataset.</p> <p>Change the <code>statistics_config</code> object and persist the changes by calling this method.</p> <p>Returns</p> <p><code>TrainingDataset</code>. The updated metadata object of the training dataset.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p>"},{"location":"generated/api/transformation_functions_api/","title":"Transformation Function","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#transformationfunction","title":"TransformationFunction","text":"<pre><code>hsfs.transformation_function.TransformationFunction(\n    featurestore_id,\n    transformation_fn=None,\n    version=None,\n    name=None,\n    source_code_content=None,\n    builtin_source_code=None,\n    output_type=None,\n    id=None,\n    type=None,\n    items=None,\n    count=None,\n    href=None,\n    **kwargs\n)\n</code></pre>"},{"location":"generated/api/transformation_functions_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#id","title":"id","text":"<p>Training dataset id.</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#name","title":"name","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#output_type","title":"output_type","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#source_code_content","title":"source_code_content","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#transformation_fn","title":"transformation_fn","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#transformer_code","title":"transformer_code","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#version","title":"version","text":""},{"location":"generated/api/transformation_functions_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#delete","title":"delete","text":"<pre><code>TransformationFunction.delete()\n</code></pre> <p>Delete transformation function from backend.</p> <p>Example</p> <pre><code># define function\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        output_type=int,\n        version=1\n    )\n# persist transformation function in backend\nplus_one_meta.save()\n\n# retrieve transformation function\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n\n# delete transformation function from backend\nplus_one_fn.delete()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#save","title":"save","text":"<pre><code>TransformationFunction.save()\n</code></pre> <p>Persist transformation function in backend.</p> <p>Example</p> <pre><code># define function\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        output_type=int,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre>"},{"location":"generated/api/transformation_functions_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#create_transformation_function","title":"create_transformation_function","text":"<pre><code>FeatureStore.create_transformation_function(transformation_function, output_type, version=None)\n</code></pre> <p>Create a transformation function metadata object.</p> <p>Example</p> <pre><code># define function\ndef plus_one(value):\n    return value + 1\n\n# create transformation function\nplus_one_meta = fs.create_transformation_function(\n        transformation_function=plus_one,\n        output_type=int,\n        version=1\n    )\n\n# persist transformation function in backend\nplus_one_meta.save()\n</code></pre> <p>Lazy</p> <p>This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the <code>save()</code> method of the transformation function metadata object.</p> <p>Arguments</p> <ul> <li>transformation_function <code>callable</code>: callable object.</li> <li>output_type <code>str | bytes | int | numpy.int8 | numpy.int16 | numpy.int32 | numpy.int64 | float | numpy.float64 | datetime.datetime | numpy.datetime64 | datetime.date | bool</code>: python or numpy output type that will be inferred as pyspark.sql.types type.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p>"},{"location":"generated/api/transformation_functions_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#get_transformation_function","title":"get_transformation_function","text":"<pre><code>FeatureStore.get_transformation_function(name, version=None)\n</code></pre> <p>Get  transformation function metadata object.</p> <p>Get transformation function by name. This will default to version 1</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nplus_one_fn = fs.get_transformation_function(name=\"plus_one\")\n</code></pre> <p>Get built-in transformation function min max scaler</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler_fn = fs.get_transformation_function(name=\"min_max_scaler\")\n</code></pre> <p>Get transformation function by name and version</p> <pre><code># get feature store instance\nfs = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=2)\n</code></pre> <p>You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function instance. Then the transformation functions are applied when you read training data, get batch data, or get feature vector(s).</p> <p>Attach transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# get transformation function metadata object\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\", version=1)\n\n# attach transformation functions\nfeature_view = fs.create_feature_view(\n    name='feature_view_name',\n    query=query,\n    labels=[\"target_column\"],\n    transformation_functions={\n        \"column_to_transform\": min_max_scaler\n    }\n)\n</code></pre> <p>Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for <code>min_max_scaler</code>; mean and standard deviation for <code>standard_scaler</code> etc.</p> <p>Attach built-in transformation functions to the feature view</p> <pre><code># get feature store instance\nfs = ...\n\n# define query object\nquery = ...\n\n# retrieve transformation functions\nmin_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\nstandard_scaler = fs.get_transformation_function(name=\"standard_scaler\")\nrobust_scaler = fs.get_transformation_function(name=\"robust_scaler\")\nlabel_encoder = fs.get_transformation_function(name=\"label_encoder\")\n\n# attach built-in transformation functions while creating feature view\nfeature_view = fs.create_feature_view(\n    name='transactions_view',\n    query=query,\n    labels=[\"fraud_label\"],\n    transformation_functions = {\n        \"category_column\": label_encoder,\n        \"weight\": robust_scaler,\n        \"age\": min_max_scaler,\n        \"salary\": standard_scaler\n    }\n)\n</code></pre> <p>Arguments</p> <ul> <li>name <code>str</code>: name of transformation function.</li> <li>version <code>int | None</code>: version of transformation function. Optional, if not provided all functions that match to provided     name will be retrieved.</li> </ul> <p>Returns:</p> <p><code>TransformationFunction</code>: The TransformationFunction metadata object.</p> <p>[source]</p>"},{"location":"generated/api/transformation_functions_api/#get_transformation_functions","title":"get_transformation_functions","text":"<pre><code>FeatureStore.get_transformation_functions()\n</code></pre> <p>Get  all transformation functions metadata objects.</p> <p>Get all transformation functions</p> <pre><code># get feature store instance\nfs = ...\n\n# get all transformation functions\nlist_transformation_fns = fs.get_transformation_functions()\n</code></pre> <p>Returns:</p> <p><code>List[TransformationFunction]</code>. List of transformation function instances.</p>"},{"location":"generated/api/validation_api/","title":"Validation","text":"<p>{{validation_result}}</p>"},{"location":"generated/api/validation_api/#properties","title":"Properties","text":"<p>{{validation_result_properties}}</p>"},{"location":"generated/api/validation_api/#methods","title":"Methods","text":"<p>{{expectation_methods}}</p>"},{"location":"generated/api/validation_api/#validate-a-dataframe","title":"Validate a dataframe","text":"<p>{{validate}}</p>"},{"location":"generated/api/validation_api/#retrieval","title":"Retrieval","text":"<p>{{validation_result_get}}</p>"},{"location":"generated/api/validation_report_api/","title":"Validation Report","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#validationreport","title":"ValidationReport","text":"<pre><code>hsfs.validation_report.ValidationReport(\n    success,\n    results,\n    meta,\n    statistics,\n    evaluation_parameters=None,\n    id=None,\n    full_report_path=None,\n    featurestore_id=None,\n    featuregroup_id=None,\n    href=None,\n    expand=None,\n    items=None,\n    count=None,\n    type=None,\n    validation_time=None,\n    ingestion_result=\"UNKNOWN\",\n    **kwargs\n)\n</code></pre> <p>Metadata object representing a validation report generated by Great Expectations in the Feature Store.</p>"},{"location":"generated/api/validation_report_api/#creation","title":"Creation","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#validate","title":"validate","text":"<pre><code>FeatureGroup.validate(\n    dataframe=None,\n    expectation_suite=None,\n    save_report=False,\n    validation_options=None,\n    ingestion_result=\"UNKNOWN\",\n    ge_type=True,\n)\n</code></pre> <p>Run validation based on the attached expectations.</p> <p>Runs any expectation attached with Deequ. But also runs attached Great Expectation Suites.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get feature group instance\nfg = fs.get_or_create_feature_group(...)\n\nge_report = fg.validate(df, save_report=False)\n</code></pre> <p>Arguments</p> <ul> <li>dataframe <code>pandas.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | None</code>: The dataframe to run the data validation expectations against.</li> <li>expectation_suite <code>hsfs.expectation_suite.ExpectationSuite | None</code>: Optionally provide an Expectation Suite to override the     one that is possibly attached to the feature group. This is useful for     testing new Expectation suites. When an extra suite is provided, the results     will never be persisted. Defaults to <code>None</code>.</li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> </ul> </li> <li>ingestion_result <code>str</code>: Specify the fate of the associated data, defaults     to \"UNKNOWN\". Supported options are  \"UNKNOWN\", \"INGESTED\", \"REJECTED\",     \"EXPERIMENT\", \"FG_DATA\". Use \"INGESTED\" or \"REJECTED\" for validation     of DataFrames to be inserted in the Feature Group. Use \"EXPERIMENT\"     for testing and development and \"FG_DATA\" when validating data     already in the Feature Group.</li> <li>save_report <code>bool | None</code>: Whether to save the report to the backend. This is only possible if the Expectation suite     is initialised and attached to the Feature Group. Defaults to False.</li> <li>ge_type <code>bool</code>: Whether to return a Great Expectations object or Hopsworks own abstraction. Defaults to True.</li> </ul> <p>Returns</p> <p>A Validation Report produced by Great Expectations.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#insert","title":"insert","text":"<pre><code>FeatureGroup.insert(\n    features,\n    overwrite=False,\n    operation=\"upsert\",\n    storage=None,\n    write_options=None,\n    validation_options=None,\n    save_code=True,\n    wait=False,\n)\n</code></pre> <p>Persist the metadata and materialize the feature group to the feature store or insert data from a dataframe into the existing feature group.</p> <p>Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is <code>online_enabled=True</code>.</p> <p>The <code>features</code> dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, a Polars DataFrame or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is <code>HUDI</code> then <code>operation</code> argument can be either <code>insert</code> or <code>upsert</code>.</p> <p>If feature group doesn't exist the insert method will create the necessary metadata the first time it is invoked and writes the specified <code>features</code> dataframe as feature group to the online/offline feature store.</p> <p>Changed in 3.3.0</p> <p><code>insert</code> and <code>save</code> methods are now async by default in non-spark clients. To achieve the old behaviour, set <code>wait</code> argument to <code>True</code>.</p> <p>Upsert new feature data with time travel format <code>HUDI</code></p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg = fs.get_or_create_feature_group(\n    name='bitcoin_price',\n    description='Bitcoin price aggregated for days',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n\nfg.insert(df_bitcoin_processed)\n</code></pre> <p>Async insert</p> <pre><code># connect to the Feature Store\nfs = ...\n\nfg1 = fs.get_or_create_feature_group(\n    name='feature_group_name1',\n    description='Description of the first FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\n# async insertion in order not to wait till finish of the job\nfg.insert(df_for_fg1, write_options={\"wait_for_job\" : False})\n\nfg2 = fs.get_or_create_feature_group(\n    name='feature_group_name2',\n    description='Description of the second FG',\n    version=1,\n    primary_key=['unix'],\n    online_enabled=True,\n    event_time='unix'\n)\nfg.insert(df_for_fg2)\n</code></pre> <p>Arguments</p> <ul> <li>features <code>pandas.DataFrame | polars.dataframe.frame.DataFrame | hsfs.feature_group.pyspark.sql.DataFrame | hsfs.feature_group.pyspark.RDD | numpy.ndarray | List[list]</code>: Pandas DataFrame, Polars DataFrame, RDD, Ndarray, list. Features to be saved.</li> <li>overwrite <code>bool</code>: Drop all data in the feature group before     inserting new data. This does not affect metadata, defaults to False.</li> <li>operation <code>str | None</code>: Apache Hudi operation type <code>\"insert\"</code> or <code>\"upsert\"</code>.     Defaults to <code>\"upsert\"</code>.</li> <li>storage <code>str | None</code>: Overwrite default behaviour, write to offline     storage only with <code>\"offline\"</code> or online only with <code>\"online\"</code>, defaults     to <code>None</code> (If the streaming APIs are enabled, specifying the storage option is not supported).</li> <li>write_options <code>Dict[str, Any] | None</code>: Additional write options as key-value pairs, defaults to <code>{}</code>.     When using the <code>python</code> engine, write_options can contain the     following entries:<ul> <li>key <code>spark</code> and value an object of type hsfs.core.job_configuration.JobConfiguration   to configure the Hopsworks Job used to write data into the   feature group.</li> <li>key <code>wait_for_job</code> and value <code>True</code> or <code>False</code> to configure   whether or not to the insert call should return only   after the Hopsworks Job has finished. By default it waits.</li> <li>key <code>start_offline_backfill</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. <code>start_offline_backfill</code> is deprecated. Use <code>start_offline_materialization</code> instead.</li> <li>key <code>start_offline_materialization</code> and value <code>True</code> or <code>False</code> to configure   whether or not to start the materialization job to write data to the offline   storage. By default the materialization job gets started immediately.</li> <li>key <code>kafka_producer_config</code> and value an object of type properties   used to configure the Kafka client. To optimize for throughput in high latency connection consider   changing producer properties.</li> <li>key <code>internal_kafka</code> and value <code>True</code> or <code>False</code> in case you established   connectivity from you Python environment to the internal advertised   listeners of the Hopsworks Kafka Cluster. Defaults to <code>False</code> and   will use external listeners when connecting from outside of Hopsworks.</li> </ul> </li> <li>validation_options <code>Dict[str, Any] | None</code>: Additional validation options as key-value pairs, defaults to <code>{}</code>.<ul> <li>key <code>run_validation</code> boolean value, set to <code>False</code> to skip validation temporarily on ingestion.</li> <li>key <code>save_report</code> boolean value, set to <code>False</code> to skip upload of the validation report to Hopsworks.</li> <li>key <code>ge_validate_kwargs</code> a dictionary containing kwargs for the validate method of Great Expectations.</li> <li>key <code>fetch_expectation_suite</code> a boolean value, by default <code>True</code>, to control whether the expectation    suite of the feature group should be fetched before every insert.</li> </ul> </li> <li>save_code <code>bool | None</code>: When running HSFS on Hopsworks or Databricks, HSFS can save the code/notebook used to create     the feature group or used to insert data to it. When calling the <code>insert</code> method repeatedly     with small batches of data, this can slow down the writes. Use this option to turn off saving     code. Defaults to <code>True</code>.</li> <li>wait <code>bool</code>: Wait for job to finish before returning, defaults to <code>False</code>.     Shortcut for read_options <code>{\"wait_for_job\": False}</code>.</li> </ul> <p>Returns</p> <p>(<code>Job</code>, <code>ValidationReport</code>) A tuple with job information if python engine is used and the validation report if validation is enabled.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. e.g fail to create feature group, dataframe schema does not match     existing feature group schema, etc. <code>hsfs.client.exceptions.DataValidationException</code>. If data validation fails and the expectation     suite <code>validation_ingestion_policy</code> is set to <code>STRICT</code>. Data is NOT ingested.</p>"},{"location":"generated/api/validation_report_api/#retrieval","title":"Retrieval","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#get_latest_validation_report","title":"get_latest_validation_report","text":"<pre><code>FeatureGroup.get_latest_validation_report(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the Feature Group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nlatest_val_report = fg.get_latest_validation_report()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p><code>ValidationReport</code>. The latest validation report attached to the Feature Group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#get_all_validation_reports","title":"get_all_validation_reports","text":"<pre><code>FeatureGroup.get_all_validation_reports(ge_type=True)\n</code></pre> <p>Return the latest validation report attached to the feature group if it exists.</p> <p>Example</p> <pre><code># connect to the Feature Store\nfs = ...\n\n# get the Feature Group instance\nfg = fs.get_or_create_feature_group(...)\n\nval_reports = fg.get_all_validation_reports()\n</code></pre> <p>Arguments</p> <ul> <li>ge_type <code>bool</code>: If <code>True</code> returns a native Great Expectation type, Hopsworks     custom type otherwise. Conversion can be performed via the <code>to_ge_type()</code>     method on hopsworks type. Defaults to <code>True</code>.</li> </ul> <p>Returns</p> <p>Union[List[<code>ValidationReport</code>], <code>ValidationReport</code>]. All validation reports attached to the feature group.</p> <p>Raises</p> <p><code>hsfs.client.exceptions.RestAPIError</code>. <code>hsfs.client.exceptions.FeatureStoreException</code>.</p>"},{"location":"generated/api/validation_report_api/#properties","title":"Properties","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#evaluation_parameters","title":"evaluation_parameters","text":"<p>Evaluation parameters field of the validation report which store kwargs of the validation.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#id","title":"id","text":"<p>Id of the validation report, set by backend.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#ingestion_result","title":"ingestion_result","text":"<p>Overall success of the validation run together with the ingestion validation policy. Indicating if dataframe was ingested or rejected.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#meta","title":"meta","text":"<p>Meta field of the validation report to store additional informations.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#results","title":"results","text":"<p>List of expectation results obtained after validation.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#statistics","title":"statistics","text":"<p>Statistics field of the validation report which store overall statistics about the validation result, e.g number of failing/successful expectations.</p> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#success","title":"success","text":"<p>Overall success of the validation step</p>"},{"location":"generated/api/validation_report_api/#methods","title":"Methods","text":"<p>[source]</p>"},{"location":"generated/api/validation_report_api/#from_response_json","title":"from_response_json","text":"<pre><code>ValidationReport.from_response_json(json_dict)\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#json","title":"json","text":"<pre><code>ValidationReport.json()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#to_dict","title":"to_dict","text":"<pre><code>ValidationReport.to_dict()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#to_ge_type","title":"to_ge_type","text":"<pre><code>ValidationReport.to_ge_type()\n</code></pre> <p>[source]</p>"},{"location":"generated/api/validation_report_api/#to_json_dict","title":"to_json_dict","text":"<pre><code>ValidationReport.to_json_dict()\n</code></pre>"}]}