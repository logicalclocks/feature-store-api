{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Feature Store # HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) Feed the training dataset to a TensorFlow model: tf_data_object = training_dataset . tf_data ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = tf_data_object . tf_record_dataset ( batch_size = 32 , num_epochs = 5 , process = True ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository. Documentation # Documentation is available at Hopsworks Feature Store Documentation . Issues # For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Introduction"},{"location":"#hopsworks-feature-store","text":"HSFS is the library to interact with the Hopsworks Feature Store. The library makes creating new features, feature groups and training datasets easy. The library is environment independent and can be used in two modes: Spark mode: For data engineering jobs that create and write features into the feature store or generate training datasets. It requires a Spark environment such as the one provided in the Hopsworks platform or Databricks. In Spark mode, HSFS provides bindings both for Python and JVM languages. Python mode: For data science jobs to explore the features available in the feature store, generate training datasets and feed them in a training pipeline. Python mode requires just a Python interpreter and can be used both in Hopsworks from Python Jobs/Jupyter Kernels, Amazon SageMaker or KubeFlow. The library automatically configures itself based on the environment it is run. However, to connect from an external environment such as Databricks or AWS Sagemaker, additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Feature Store"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project feature store handler import hsfs connection = hsfs . connection () fs = connection . get_feature_store () Create a new feature group fg = fs . create_feature_group ( \"rain\" , version = 1 , description = \"Rain features\" , primary_key = [ 'date' , 'location_id' ], online_enabled = True ) fg . save ( dataframe ) Upsert new data in to the feature group with time_travel_format=\"HUDI\" \". fg . insert ( upsert_df ) Retrieve commit timeline metdata of the feature group with time_travel_format=\"HUDI\" \". fg . commit_details () \"Reading feature group as of specific point in time\". fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) . show () Read updates that occurred between specified points in time. fg = fs . get_feature_group ( \"rain\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Join features together feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) feature_join . show ( 5 ) join feature groups that correspond to specific point in time feature_join = rain_fg . select_all () . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) . join ( location_fg . select_all ()) . as_of ( \"2020-10-31\" ) feature_join . show ( 5 ) join feature groups that correspond to different time rain_fg_q = rain_fg . select_all () . as_of ( \"2020-10-20 07:41:43\" ) temperature_fg_q = temperature_fg . select_all () . as_of ( \"2020-10-20 07:32:33\" ) location_fg_q = location_fg . select_all () . as_of ( \"2020-10-20 07:33:08\" ) joined_features_q = rain_fg_q . join ( temperature_fg_q ) . join ( location_fg_q ) Use the query object to create a training dataset: td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , data_format = \"tfrecords\" , description = \"A test training dataset saved in TfRecords format\" , splits = { 'train' : 0.7 , 'test' : 0.2 , 'validate' : 0.1 }) td . save ( feature_join ) Feed the training dataset to a TensorFlow model: tf_data_object = training_dataset . tf_data ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = tf_data_object . tf_record_dataset ( batch_size = 32 , num_epochs = 5 , process = True ) A short introduction to the Scala API: import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build () val fs = connection . getFeatureStore (); val attendances_features_fg = fs . getFeatureGroup ( \"games_features\" , 1 ); attendances_features_fg . show ( 1 ) You can find more examples on how to use the library in our hops-examples repository.","title":"Getting Started On Hopsworks"},{"location":"#documentation","text":"Documentation is available at Hopsworks Feature Store Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks and the Feature Store please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[hive,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ hive,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Option 1: Build only current version of docs # Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve Option 2 (Preferred): Build multi-version doc with mike # Versioning on docs.hopsworks.ai # On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 . Build Instructions # For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ] Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[hive,dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Feature Store uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsfs black hsfs","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Feature store entity engine methods (e.g. FeatureGroupEngine etc.) only require a single line docstring. REST Api implementations (e.g. FeatureGroupApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/moritzmeister/keras-autodoc@split-tags-properties Install HSFS with docs extras: pip install -e . [ hive,dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","text":"Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve","title":"Option 1: Build only current version of docs"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","text":"","title":"Option 2 (Preferred): Build multi-version doc with mike"},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","text":"On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsfs corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 .","title":"Versioning on docs.hopsworks.ai"},{"location":"CONTRIBUTING/#build-instructions","text":"For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ]","title":"Build Instructions"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsfs.connection.Connection.connection\" , \"hsfs.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"overview/","text":"Concept Overview # Project-Based Multi-tenancy # Hopsworks implements a dynamic role-based access control model through a project-based multi-tenant security model . Inspired by GDPR, in Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Every Project has an owner with full read-write privileges and zero or more members. %% change to github link An important aspect of Project based multi-tenancy is that assets can be shared between projects. The current assets that can be shared between projects are: files/directories in HopsFS, Hive databases, feature stores , and Kafka topics. Important Sharing assets does not mean that data is duplicated. The Hopsworks Feature Store # The Hopsworks Feature Store is a tool for curating and serving machine learning (ML)features. The Feature Store is a central and unified API between Data Engineers and Data Scientists. Benefits of the Feature Store Manage feature data to profive unified access to machine learning features from small teams to large enterprises. Enable discovery, documentation, sharing and insights into your features through rich metadata. Make feature data available in a performant and scalable way for model training and model inference. Allow point-in-time correct and consistent access to feature data (time travel). Feature Store Concepts # Entities in the Feature Store Entities within the Feature Store are organized hierarchically. On the most granular level are the features itself. Data Engineers ingest the feature data within their organization through the creation of feature groups . Data Scientists are then able to read selected features from the feature groups to create training datasets for model training, run batch inference with deployed models or perform inference from online models by scoring single feature vectors . Feature Vector A Feature Vector is a single row of feature values associated with a primary key. Feature Groups # Feature Groups are entities that contain both metadata about the grouped features, as well as information of the jobs used to ingest the data contained in a feature group and also the actual location of the data (HopsFS or externally, such as S3). Typically, feature groups represent a logical set of features coming from the same data source sharing a common primary key. Feature groups also contain the schema and type information of the features, for the user to know how to interpret the data. Feature groups can also be used to compute Statistics over features, or to define Data Validation Rules using the statistics and schema information. In order to enable online serving for features of a feature group, the feature group needs to be made available as an online feature group. Training Datasets # In order to be able to train machine learning models efficiently, the feature data needs to be materialized as a Training Dataset in the file format most suitable for the ML framework used. For example, when training models with TensorFlow the ideal file format is TensorFlow's tfrecord format. Training datasets can be created with features from any number of feature groups, as long as the feature groups can be joined in a meaningful way. Users are able to compute Statistics also for training datasets, which will make it easy to understand a dataset's characteristics also in the future. The Hopsworks Feature Store has support for writing training datasets either to the distributed file system of Hopsworks - HopsFS - or to external storage such as S3. Offline vs. Online Feature Store # The Feature Store is a dual database-system, to cover all machine learning use cases it consists of high throughput offline storage layer, and additionally a low-latency online storage. The offline storage is mainly used to generate large batches of feature data, for example to be exported as training datasets. Additionally, the offline storage can be used to score large amounts of data with a machine learning model in regular intervals, so called batch inference . The online storage on the other hand is required for online applications, where the goal is to retrieve a single feature vector with the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. An example for online inference would be an e-commerce business, which would like to predict the credit score of a client when he is about to checkout his shopping cart. A client-id will be sent to the online feature store to retrieve the historic features for this customer, which can then be enriched by real time features like the value of his shopping cart, and will then be passed to the machine learning model for inference. Offline vs. Online Feature Store There is no database fullfilling both requirements of very low latency and and high throughput. Therefore, the Hopsworks Feature Store builds on Apache Hive with Apache Hudi as offline storage layer and MySQL Cluster (NDB) as online storage.","title":"Overview"},{"location":"overview/#concept-overview","text":"","title":"Concept Overview"},{"location":"overview/#project-based-multi-tenancy","text":"Hopsworks implements a dynamic role-based access control model through a project-based multi-tenant security model . Inspired by GDPR, in Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Every Project has an owner with full read-write privileges and zero or more members. %% change to github link An important aspect of Project based multi-tenancy is that assets can be shared between projects. The current assets that can be shared between projects are: files/directories in HopsFS, Hive databases, feature stores , and Kafka topics. Important Sharing assets does not mean that data is duplicated.","title":"Project-Based Multi-tenancy"},{"location":"overview/#the-hopsworks-feature-store","text":"The Hopsworks Feature Store is a tool for curating and serving machine learning (ML)features. The Feature Store is a central and unified API between Data Engineers and Data Scientists. Benefits of the Feature Store Manage feature data to profive unified access to machine learning features from small teams to large enterprises. Enable discovery, documentation, sharing and insights into your features through rich metadata. Make feature data available in a performant and scalable way for model training and model inference. Allow point-in-time correct and consistent access to feature data (time travel).","title":"The Hopsworks Feature Store"},{"location":"overview/#feature-store-concepts","text":"Entities in the Feature Store Entities within the Feature Store are organized hierarchically. On the most granular level are the features itself. Data Engineers ingest the feature data within their organization through the creation of feature groups . Data Scientists are then able to read selected features from the feature groups to create training datasets for model training, run batch inference with deployed models or perform inference from online models by scoring single feature vectors . Feature Vector A Feature Vector is a single row of feature values associated with a primary key.","title":"Feature Store Concepts"},{"location":"overview/#feature-groups","text":"Feature Groups are entities that contain both metadata about the grouped features, as well as information of the jobs used to ingest the data contained in a feature group and also the actual location of the data (HopsFS or externally, such as S3). Typically, feature groups represent a logical set of features coming from the same data source sharing a common primary key. Feature groups also contain the schema and type information of the features, for the user to know how to interpret the data. Feature groups can also be used to compute Statistics over features, or to define Data Validation Rules using the statistics and schema information. In order to enable online serving for features of a feature group, the feature group needs to be made available as an online feature group.","title":"Feature Groups"},{"location":"overview/#training-datasets","text":"In order to be able to train machine learning models efficiently, the feature data needs to be materialized as a Training Dataset in the file format most suitable for the ML framework used. For example, when training models with TensorFlow the ideal file format is TensorFlow's tfrecord format. Training datasets can be created with features from any number of feature groups, as long as the feature groups can be joined in a meaningful way. Users are able to compute Statistics also for training datasets, which will make it easy to understand a dataset's characteristics also in the future. The Hopsworks Feature Store has support for writing training datasets either to the distributed file system of Hopsworks - HopsFS - or to external storage such as S3.","title":"Training Datasets"},{"location":"overview/#offline-vs-online-feature-store","text":"The Feature Store is a dual database-system, to cover all machine learning use cases it consists of high throughput offline storage layer, and additionally a low-latency online storage. The offline storage is mainly used to generate large batches of feature data, for example to be exported as training datasets. Additionally, the offline storage can be used to score large amounts of data with a machine learning model in regular intervals, so called batch inference . The online storage on the other hand is required for online applications, where the goal is to retrieve a single feature vector with the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. An example for online inference would be an e-commerce business, which would like to predict the credit score of a client when he is about to checkout his shopping cart. A client-id will be sent to the online feature store to retrieve the historic features for this customer, which can then be enriched by real time features like the value of his shopping cart, and will then be passed to the machine learning model for inference. Offline vs. Online Feature Store There is no database fullfilling both requirements of very low latency and and high throughput. Therefore, the Hopsworks Feature Store builds on Apache Hive with Apache Hudi as offline storage layer and MySQL Cluster (NDB) as online storage.","title":"Offline vs. Online Feature Store"},{"location":"quickstart/","text":"Quickstart Guide # The Hopsworks feature store is a centralized repository, within an organization, to manage machine learning features. A feature is a measurable property of a phenomenon. It could be a simple value such as the age of a customer, or it could be an aggregated value, such as the number of transactions made by a customer in the last 30 days. A feature is not restricted to an numeric value, it could be a string representing an address, or an image. The Hopsworks Feature Store A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models. In this Quickstart Guide we are going to focus on the left side of the picture above. In particular how data engeneers can create features and push them to the Hopsworks feature store so that they are available to the data scientists HSFS library # The Hopsworks feature feature store library is called hsfs ( H opswork s F eature S tore). The library is Apache V2 licensed and available here . The library is currently available for Python and JVM languages such as Scala and Java. If you want to connect to the Feature Store from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Feature Store. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Feature Store. In fact, the Feature Store itself is also represented by an object. Furthermore, these objects have methods to save data along with the entities in the feature store. This data can be materialized from Spark or Pandas DataFrames, or the HSFS - Query abstraction . Guide Notebooks # This guide is based on a series of notebooks , which is available in the Feature Store Demo Tour Project on Hopsworks. Connection, Project and Feature Store # The first step is to establish a connection with your Hopsworks Feature Store instance and retrieve the object that represents the Feature Store you'll be working with. By default connection.get_feature_store() returns the feature store of the project you are working with. However, it accepts also a project name as parameter to select a different feature store. Python import hsfs # Create a connection connection = hsfs . connection () # Get the feature store handle for the project's feature store fs = connection . get_feature_store () Scala import com . logicalclocks . hsfs . _ import scala . collection . JavaConverters . _ // Create a connection val connection = HopsworksConnection . builder (). build (); // Get the feature store handle for the project's feature store val fs = connection . getFeatureStore (); You can inspect the Feature Store's meta data by accessing its attributes: Python print ( fs . name ) print ( fs . description ) Scala println ( fs . getName ) println ( fs . getDescription ) Example Data # In order to use the example data, you need to unzip the archive.zip file which is located in /Jupyter/hsfs/ when you are running the Quickstart from the Feature Store Demo Tour project. To do so, head to the Data Sets tab in Hopsworks, open the /Jupyter/hsfs directory, mark the archive.zip -file and click the extract button. The Data Sets browser Of course you can also use your own data if you read it into a Spark DataFrame. Feature Groups # Assuming you have done some feature engineering on the raw data, having produced a DataFrame with Features, these can now be saved to the Feature Store. For examples of feature engineering on the provided Sales data, see the example notebook . Creation # Create a feature group named store_fg . The store is the primary key uniquely identifying all the remaining features in this feature group. As you can see, you have the possibility to make settings on the Feature Group, such as the version number, or the statistics which should be computed. The Feature Group Guide guides through the full configuration of Feature Groups. Python store_fg_meta = fs . create_feature_group ( name = \"store_fg\" , version = 1 , primary_key = [ \"store\" ], description = \"Store related features\" , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True }) Scala val storeFgMeta = ( fs . createFeatureGroup () . name ( \"store_fg\" ) . description ( \"Store related features\" ) . version ( 1 ) . primaryKeys ( Seq ( \"store\" ). asJava ) . statisticsEnabled ( True ) . histograms ( True ) . correlations ( True ) . build ()) Up to this point we have just created the metadata object representing the feature group. However, we haven't saved the feature group in the feature store yet. To do so, we can call the method save on the metadata object created in the cell above. Python store_fg_meta . save ( store_dataframe ) Scala storeFgMeta . save ( store_dataframe ) Retrieval # If there were feature groups previously created in your Feature Store, or you want to pick up where you left off before, you can retrieve and read feature groups in a similar fashion as creating them: Using the Feature Store object, you can retrieve handles to the entities, such as feature groups, in the Feature Store. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. This is necessary, in order to make the code reproducible, as version changes indicate breaking schema changes. Python exogenous_fg_meta = fs . get_feature_group ( 'exogenous_fg' , version = 1 ) # Read the data, by default selecting all features exogenous_df = exogenous_fg_meta . read () # Select a subset of features and read into dataframe exogenous_df_subset = exogenous_fg_meta . select ([ \"store\" , \"fuel_price\" , \"is_holiday\" ]) . read () Scala val exogenousFgMeta = fs . getFeatureGroup ( \"exogenous_fg\" , 1 ) // Read the data, by default selecting all features val exogenousDf = exogenousFgMeta . read () // Select a subset of features and read into dataframe val exogenousDfSubset = exogenousFgMeta . select ( Seq ( \"store\" , \"fuel_price\" , \"is_holiday\" ). asJava ). read () Joining # HSFS provides an API similar to Pandas to join feature groups together and to select features from different feature groups. The easies query you can write is by selecting all the features from a feature group and join them with all the features of another feature group. You can use the select_all() method of a feature group to select all its features. HSFS relies on the Hopsworks feature store to identify which features of the two feature groups to use as joining condition. If you don't specify anything, Hopsworks will use the largest matching subset of primary keys with the same name. In the example below, sales_fg has store , dept and date as composite primary key while exogenous_fg has only store and date . So Hopsworks will set as joining condition store and date . Python sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () . join ( exogenous_fg . select_all ()) # print first 5 rows of the query query . show ( 5 ) Scala val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) val query = salesFg . selectAll (). join ( exogenousFg . selectAll ()) // print first 5 rows of the query query . show ( 5 ) For a more complex joins, and details about overwriting the join keys and join type, the programming interface guide explains the Query interface as well as Training Datasets # Once a Data Scientist has found the features she needs for her model, she can create a training dataset to materialize the features in the desired file format. The Hopsworks Feature Store supports a variety of file formats, matching the Data Scientists' favourite Machine Learning Frameworks. Creation # You can either create a training dataset from a Query object or directly from a Spark or Pandas DataFrame. Spark and Pandas give you more flexibility, but it has drawbacks for reproducability at inference time, when the Feature Vector needs to be reconstructed. The idea of the Feature Store is to have ready-engineered features available for Data Scientists to be selected for training datasets. With this assumption, it should not be necessary to perform additional engineering, but instead joining, filtering and point in time querying should be enough to generate training datasets. Python store_fg = fs . get_feature_group ( \"store_fg\" ) sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () \\ . join ( store_fg . select_all ()) \\ . join ( exogenous_fg . select ([ 'fuel_price' , 'unemployment' , 'cpi' ])) td = fs . create_training_dataset ( name = \"sales_model\" , description = \"Dataset to train the sales model\" , data_format = \"tfrecord\" , splits = { \"train\" : 0.7 , \"test\" : 0.2 , \"validate\" : 0.1 }, version = 1 ) td . save ( query ) Scala val storeFg = fs . getFeatureGroup ( \"store_fg\" ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) query = ( salesFg . selectAll () . join ( storeFg . selectAll ()) . join ( exogenousFg . select ( Seq ( \"fuel_price\" , \"unemployment\" , \"cpi\" ). asJava ))) val td = ( fs . createTrainingDataset () . name ( \"sales_model\" ) . description ( \"Dataset to train the sales model\" ) . version ( 1 ) . dataFormat ( DataFormat . TFRECORD ) . splits ( Map ( \"train\" -> Double . box ( 0.7 ), \"test\" -> Double . box ( 0.2 ), \"validate\" -> Double . box ( 0.1 )) . build ()) td . save ( query ) Retrieval # If you want to use a previously created training dataset to train a machine learning model, you can get the training dataset similarly to how you get a feature group. Python td = fs . get_training_dataset ( \"sales_model\" ) df = td . read ( split = \"train\" ) Scala val td = fs . getTrainingDataset ( \"sales_model\" ) val df = td . read ( \"train\" ) Either you read the data into a DataFrame again, or you use the provided utility methods, to instantiate for example a tf.data.Dataset , which can directly be passed to a TensorFlow model. Python train_input_feeder = training_dataset . feed ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = train_input_feeder . tf_record_dataset () Scala This functionality is only available in the Python API.","title":"Quickstart"},{"location":"quickstart/#quickstart-guide","text":"The Hopsworks feature store is a centralized repository, within an organization, to manage machine learning features. A feature is a measurable property of a phenomenon. It could be a simple value such as the age of a customer, or it could be an aggregated value, such as the number of transactions made by a customer in the last 30 days. A feature is not restricted to an numeric value, it could be a string representing an address, or an image. The Hopsworks Feature Store A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models. In this Quickstart Guide we are going to focus on the left side of the picture above. In particular how data engeneers can create features and push them to the Hopsworks feature store so that they are available to the data scientists","title":"Quickstart Guide"},{"location":"quickstart/#hsfs-library","text":"The Hopsworks feature feature store library is called hsfs ( H opswork s F eature S tore). The library is Apache V2 licensed and available here . The library is currently available for Python and JVM languages such as Scala and Java. If you want to connect to the Feature Store from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Feature Store. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Feature Store. In fact, the Feature Store itself is also represented by an object. Furthermore, these objects have methods to save data along with the entities in the feature store. This data can be materialized from Spark or Pandas DataFrames, or the HSFS - Query abstraction .","title":"HSFS library"},{"location":"quickstart/#guide-notebooks","text":"This guide is based on a series of notebooks , which is available in the Feature Store Demo Tour Project on Hopsworks.","title":"Guide Notebooks"},{"location":"quickstart/#connection-project-and-feature-store","text":"The first step is to establish a connection with your Hopsworks Feature Store instance and retrieve the object that represents the Feature Store you'll be working with. By default connection.get_feature_store() returns the feature store of the project you are working with. However, it accepts also a project name as parameter to select a different feature store. Python import hsfs # Create a connection connection = hsfs . connection () # Get the feature store handle for the project's feature store fs = connection . get_feature_store () Scala import com . logicalclocks . hsfs . _ import scala . collection . JavaConverters . _ // Create a connection val connection = HopsworksConnection . builder (). build (); // Get the feature store handle for the project's feature store val fs = connection . getFeatureStore (); You can inspect the Feature Store's meta data by accessing its attributes: Python print ( fs . name ) print ( fs . description ) Scala println ( fs . getName ) println ( fs . getDescription )","title":"Connection, Project and Feature Store"},{"location":"quickstart/#example-data","text":"In order to use the example data, you need to unzip the archive.zip file which is located in /Jupyter/hsfs/ when you are running the Quickstart from the Feature Store Demo Tour project. To do so, head to the Data Sets tab in Hopsworks, open the /Jupyter/hsfs directory, mark the archive.zip -file and click the extract button. The Data Sets browser Of course you can also use your own data if you read it into a Spark DataFrame.","title":"Example Data"},{"location":"quickstart/#feature-groups","text":"Assuming you have done some feature engineering on the raw data, having produced a DataFrame with Features, these can now be saved to the Feature Store. For examples of feature engineering on the provided Sales data, see the example notebook .","title":"Feature Groups"},{"location":"quickstart/#creation","text":"Create a feature group named store_fg . The store is the primary key uniquely identifying all the remaining features in this feature group. As you can see, you have the possibility to make settings on the Feature Group, such as the version number, or the statistics which should be computed. The Feature Group Guide guides through the full configuration of Feature Groups. Python store_fg_meta = fs . create_feature_group ( name = \"store_fg\" , version = 1 , primary_key = [ \"store\" ], description = \"Store related features\" , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True }) Scala val storeFgMeta = ( fs . createFeatureGroup () . name ( \"store_fg\" ) . description ( \"Store related features\" ) . version ( 1 ) . primaryKeys ( Seq ( \"store\" ). asJava ) . statisticsEnabled ( True ) . histograms ( True ) . correlations ( True ) . build ()) Up to this point we have just created the metadata object representing the feature group. However, we haven't saved the feature group in the feature store yet. To do so, we can call the method save on the metadata object created in the cell above. Python store_fg_meta . save ( store_dataframe ) Scala storeFgMeta . save ( store_dataframe )","title":"Creation"},{"location":"quickstart/#retrieval","text":"If there were feature groups previously created in your Feature Store, or you want to pick up where you left off before, you can retrieve and read feature groups in a similar fashion as creating them: Using the Feature Store object, you can retrieve handles to the entities, such as feature groups, in the Feature Store. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. This is necessary, in order to make the code reproducible, as version changes indicate breaking schema changes. Python exogenous_fg_meta = fs . get_feature_group ( 'exogenous_fg' , version = 1 ) # Read the data, by default selecting all features exogenous_df = exogenous_fg_meta . read () # Select a subset of features and read into dataframe exogenous_df_subset = exogenous_fg_meta . select ([ \"store\" , \"fuel_price\" , \"is_holiday\" ]) . read () Scala val exogenousFgMeta = fs . getFeatureGroup ( \"exogenous_fg\" , 1 ) // Read the data, by default selecting all features val exogenousDf = exogenousFgMeta . read () // Select a subset of features and read into dataframe val exogenousDfSubset = exogenousFgMeta . select ( Seq ( \"store\" , \"fuel_price\" , \"is_holiday\" ). asJava ). read ()","title":"Retrieval"},{"location":"quickstart/#joining","text":"HSFS provides an API similar to Pandas to join feature groups together and to select features from different feature groups. The easies query you can write is by selecting all the features from a feature group and join them with all the features of another feature group. You can use the select_all() method of a feature group to select all its features. HSFS relies on the Hopsworks feature store to identify which features of the two feature groups to use as joining condition. If you don't specify anything, Hopsworks will use the largest matching subset of primary keys with the same name. In the example below, sales_fg has store , dept and date as composite primary key while exogenous_fg has only store and date . So Hopsworks will set as joining condition store and date . Python sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () . join ( exogenous_fg . select_all ()) # print first 5 rows of the query query . show ( 5 ) Scala val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) val query = salesFg . selectAll (). join ( exogenousFg . selectAll ()) // print first 5 rows of the query query . show ( 5 ) For a more complex joins, and details about overwriting the join keys and join type, the programming interface guide explains the Query interface as well as","title":"Joining"},{"location":"quickstart/#training-datasets","text":"Once a Data Scientist has found the features she needs for her model, she can create a training dataset to materialize the features in the desired file format. The Hopsworks Feature Store supports a variety of file formats, matching the Data Scientists' favourite Machine Learning Frameworks.","title":"Training Datasets"},{"location":"quickstart/#creation_1","text":"You can either create a training dataset from a Query object or directly from a Spark or Pandas DataFrame. Spark and Pandas give you more flexibility, but it has drawbacks for reproducability at inference time, when the Feature Vector needs to be reconstructed. The idea of the Feature Store is to have ready-engineered features available for Data Scientists to be selected for training datasets. With this assumption, it should not be necessary to perform additional engineering, but instead joining, filtering and point in time querying should be enough to generate training datasets. Python store_fg = fs . get_feature_group ( \"store_fg\" ) sales_fg = fs . get_feature_group ( 'sales_fg' ) exogenous_fg = fs . get_feature_group ( 'exogenous_fg' ) query = sales_fg . select_all () \\ . join ( store_fg . select_all ()) \\ . join ( exogenous_fg . select ([ 'fuel_price' , 'unemployment' , 'cpi' ])) td = fs . create_training_dataset ( name = \"sales_model\" , description = \"Dataset to train the sales model\" , data_format = \"tfrecord\" , splits = { \"train\" : 0.7 , \"test\" : 0.2 , \"validate\" : 0.1 }, version = 1 ) td . save ( query ) Scala val storeFg = fs . getFeatureGroup ( \"store_fg\" ) val exogenousFg = fs . getFeatureGroup ( \"exogenous_fg\" ) val salesFg = fs . getFeatureGroup ( \"sales_fg\" ) query = ( salesFg . selectAll () . join ( storeFg . selectAll ()) . join ( exogenousFg . select ( Seq ( \"fuel_price\" , \"unemployment\" , \"cpi\" ). asJava ))) val td = ( fs . createTrainingDataset () . name ( \"sales_model\" ) . description ( \"Dataset to train the sales model\" ) . version ( 1 ) . dataFormat ( DataFormat . TFRECORD ) . splits ( Map ( \"train\" -> Double . box ( 0.7 ), \"test\" -> Double . box ( 0.2 ), \"validate\" -> Double . box ( 0.1 )) . build ()) td . save ( query )","title":"Creation"},{"location":"quickstart/#retrieval_1","text":"If you want to use a previously created training dataset to train a machine learning model, you can get the training dataset similarly to how you get a feature group. Python td = fs . get_training_dataset ( \"sales_model\" ) df = td . read ( split = \"train\" ) Scala val td = fs . getTrainingDataset ( \"sales_model\" ) val df = td . read ( \"train\" ) Either you read the data into a DataFrame again, or you use the provided utility methods, to instantiate for example a tf.data.Dataset , which can directly be passed to a TensorFlow model. Python train_input_feeder = training_dataset . feed ( target_name = \"label\" , split = \"train\" , is_training = True ) train_input = train_input_feeder . tf_record_dataset () Scala This functionality is only available in the Python API.","title":"Retrieval"},{"location":"setup/","text":"Integrations # Hopsworks # If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide . Storage Connectors # Storage connectors encapsulate the configuration information needed for a Spark or Python execution engine to securely read and write to a specific storage. The storage connector guide explains step by step how to configure different data sources (such as S3, Azure Data Lake, Redshift, Snowflake, any JDBC data source) and how they can be used to ingest data and define external (on-demand) Feature Groups. Databricks # Connecting to the Feature Store from Databricks requires setting up a Feature Store API Key for Databricks and installing one of the HSFS client libraries on your Databricks cluster. The Databricks integration guide explains step by step how to connect to the Feature Store from Databricks. AWS Sagemaker # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API Key for SageMaker and installing the HSFS Python client library on SageMaker. The AWS SageMaker integration guide explains step by step how to connect to the Feature Store from SageMaker. Python (Local or KubeFlow) # Connecting to the Feature Store from any Python environment, such as your local environment or KubeFlow, requires setting up a Feature Store API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment. Spark Cluster # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Overview"},{"location":"setup/#integrations","text":"","title":"Integrations"},{"location":"setup/#hopsworks","text":"If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started Guide .","title":"Hopsworks"},{"location":"setup/#storage-connectors","text":"Storage connectors encapsulate the configuration information needed for a Spark or Python execution engine to securely read and write to a specific storage. The storage connector guide explains step by step how to configure different data sources (such as S3, Azure Data Lake, Redshift, Snowflake, any JDBC data source) and how they can be used to ingest data and define external (on-demand) Feature Groups.","title":"Storage Connectors"},{"location":"setup/#databricks","text":"Connecting to the Feature Store from Databricks requires setting up a Feature Store API Key for Databricks and installing one of the HSFS client libraries on your Databricks cluster. The Databricks integration guide explains step by step how to connect to the Feature Store from Databricks.","title":"Databricks"},{"location":"setup/#aws-sagemaker","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API Key for SageMaker and installing the HSFS Python client library on SageMaker. The AWS SageMaker integration guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS Sagemaker"},{"location":"setup/#python-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment, such as your local environment or KubeFlow, requires setting up a Feature Store API Key and installing the HSFS Python client library. The Python integration guide explains step by step how to connect to the Feature Store from any Python environment.","title":"Python (Local or KubeFlow)"},{"location":"setup/#spark-cluster","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. The Spark integration guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Cluster"},{"location":"generated/feature/","text":"Feature # Features are the most granular entity in the feature store and are logically grouped by feature groups . The storage location of a single feature is determined by the feature group . Hence, enabling a feature group for online storage will make a feature available as an online feature. New features can be appended to feature groups , however, to drop features, a new feature group version has to be created. When appending features it is possible to specify a default value which is used for existing feature vectors in the feature group for the new feature. [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Feature Types # Each features requires at least an offline type to be specified for the creation of the meta data of the feature group in the offline storage, even if the feature group is going to be a purely online feature group with no data in the offline storage. Offline Storage # The offline storage is based on Apache Hive and hence, any Hive Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential offline types are: \"None\" , \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\" Online Storage # The online storage is based on MySQL Cluster (NDB) and hence, any MySQL Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(25)\" , \"VARCHAR(125)\" , \"VARCHAR(225)\" , \"VARCHAR(500)\" , \"VARCHAR(1000)\" , \"VARCHAR(2000)\" , \"VARCHAR(5000)\" , \"VARCHAR(10000)\" , \"BINARY\" , \"VARBINARY(100)\" , \"VARBINARY(500)\" , \"VARBINARY(1000)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\" Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] is_complex # Feature . is_complex () Returns true if the feature has a complex type. [source] json # Feature . json ()","title":"Feature"},{"location":"generated/feature/#feature","text":"Features are the most granular entity in the feature store and are logically grouped by feature groups . The storage location of a single feature is determined by the feature group . Hence, enabling a feature group for online storage will make a feature available as an online feature. New features can be appended to feature groups , however, to drop features, a new feature group version has to be created. When appending features it is possible to specify a default value which is used for existing feature vectors in the feature group for the new feature. [source]","title":"Feature"},{"location":"generated/feature/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/feature/#feature-types","text":"Each features requires at least an offline type to be specified for the creation of the meta data of the feature group in the offline storage, even if the feature group is going to be a purely online feature group with no data in the offline storage.","title":"Feature Types"},{"location":"generated/feature/#offline-storage","text":"The offline storage is based on Apache Hive and hence, any Hive Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential offline types are: \"None\" , \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\"","title":"Offline Storage"},{"location":"generated/feature/#online-storage","text":"The online storage is based on MySQL Cluster (NDB) and hence, any MySQL Data Type can be leveraged. Type Inference When creating a feature group from a Spark DataFrame, without providing a schema manually, the feature store will infer the schema with feature types from the DataFrame. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(25)\" , \"VARCHAR(125)\" , \"VARCHAR(225)\" , \"VARCHAR(500)\" , \"VARCHAR(1000)\" , \"VARCHAR(2000)\" , \"VARCHAR(5000)\" , \"VARCHAR(10000)\" , \"BINARY\" , \"VARBINARY(100)\" , \"VARBINARY(500)\" , \"VARBINARY(1000)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\"","title":"Online Storage"},{"location":"generated/feature/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/feature/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/feature/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/feature/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/feature/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/feature/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/feature/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/feature/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature/#is_complex","text":"Feature . is_complex () Returns true if the feature has a complex type. [source]","title":"is_complex"},{"location":"generated/feature/#json","text":"Feature . json ()","title":"json"},{"location":"generated/feature_group/","text":"Feature Group # A Feature Groups is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The Feature Group lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them. Generally, the features in a feature froup are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, feature groups provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in. Combine features from any number of feature groups Feature groups are logical groupings of features, usually based on the data source and ingestion job from which they originate. It is important to note that feature groups are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets. Versioning # Feature groups can be versioned. Data Engineers should use the version to indicate to a Data Scientist that the schema or the feature engineering logic of the features in this group has changed. Breaking feature group schema changes In order to guarantee reproducability, the schema of a feature group should be immutable, because deleting features could lead to failing model pipelines downstream. Hence, in order to modify a schema, a new version of a feature group has to be created. In contrary, appending features to feature groups is considered a non-breaking change, since the feature store makes all selections explicit and because the namespace within a feature group is flat, it is not possible to append a new feature with an already existing name to a feature group. Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] expectations_names # The names of expectations attached to this feature group. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] time_travel_format # Setting of the feature group time travel format. [source] validation_type # Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source] attach_expectation # FeatureGroup . attach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source] compute_statistics # FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] detach_expectation # FeatureGroup . detach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] get_complex_features # FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source] get_expectation # FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source] get_expectations # FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns FeatureGroup . Updated feature group metadata object. [source] insert_stream # FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to Fals e. timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source] save # FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"Feature Group"},{"location":"generated/feature_group/#feature-group","text":"A Feature Groups is a logical grouping of features, and experience has shown, that this grouping generally originates from the features being derived from the same data source. The Feature Group lets you save metadata along features, which defines how the Feature Store interprets them, combines them and reproduces training datasets created from them. Generally, the features in a feature froup are engineered together in an ingestion job. However, it is possible to have additional jobs to append features to an existing feature group. Furthermore, feature groups provide a way of defining a namespace for features, such that you can define features with the same name multiple times, but uniquely identified by the group they are contained in. Combine features from any number of feature groups Feature groups are logical groupings of features, usually based on the data source and ingestion job from which they originate. It is important to note that feature groups are not groupings of features for immediate training of Machine Learning models. Instead, to ensure reusability of features, it is possible to combine features from any number of groups into training datasets.","title":"Feature Group"},{"location":"generated/feature_group/#versioning","text":"Feature groups can be versioned. Data Engineers should use the version to indicate to a Data Scientist that the schema or the feature engineering logic of the features in this group has changed. Breaking feature group schema changes In order to guarantee reproducability, the schema of a feature group should be immutable, because deleting features could lead to failing model pipelines downstream. Hence, in order to modify a schema, a new version of a feature group has to be created. In contrary, appending features to feature groups is considered a non-breaking change, since the feature store makes all selections explicit and because the namespace within a feature group is flat, it is not possible to append a new feature with an already existing name to a feature group.","title":"Versioning"},{"location":"generated/feature_group/#creation","text":"[source]","title":"Creation"},{"location":"generated/feature_group/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object.","title":"create_feature_group"},{"location":"generated/feature_group/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_group/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/feature_group/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_group/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/feature_group/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/feature_group/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/feature_group/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/feature_group/#expectations_names","text":"The names of expectations attached to this feature group. [source]","title":"expectations_names"},{"location":"generated/feature_group/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/feature_group/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/feature_group/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/feature_group/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/feature_group/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/feature_group/#location","text":"[source]","title":"location"},{"location":"generated/feature_group/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/feature_group/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/feature_group/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/feature_group/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/feature_group/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/feature_group/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/feature_group/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/feature_group/#validation_type","text":"Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source]","title":"validation_type"},{"location":"generated/feature_group/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/feature_group/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_group/#add_tag","text":"FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/feature_group/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source]","title":"append_features"},{"location":"generated/feature_group/#attach_expectation","text":"FeatureGroup . attach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"attach_expectation"},{"location":"generated/feature_group/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/feature_group/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"commit_details"},{"location":"generated/feature_group/#compute_statistics","text":"FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/feature_group/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/feature_group/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/feature_group/#detach_expectation","text":"FeatureGroup . detach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"detach_expectation"},{"location":"generated/feature_group/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/feature_group/#get_complex_features","text":"FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source]","title":"get_complex_features"},{"location":"generated/feature_group/#get_expectation","text":"FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"get_expectation"},{"location":"generated/feature_group/#get_expectations","text":"FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source]","title":"get_expectations"},{"location":"generated/feature_group/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/feature_group/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/feature_group/#get_tag","text":"FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/feature_group/#get_tags","text":"FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/feature_group/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source]","title":"get_validations"},{"location":"generated/feature_group/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/feature_group/#insert_stream","text":"FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to Fals e. timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source]","title":"insert_stream"},{"location":"generated/feature_group/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/feature_group/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"read_changes"},{"location":"generated/feature_group/#save","text":"FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/feature_group/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/feature_group/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/feature_group/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/feature_group/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/feature_group/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/feature_group/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/feature_group/#validate","text":"FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/feature_store/","text":"Feature Store # Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object. [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_groups # FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Args: query (str): The SQL query to execute. dataframe_type (Optional[str], optional): The type of the returned dataframe. Defaults to \"default\" which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online (Optional[bool], optional): Set to true to execute the query against the online feature store. Defaults to False. read_options (Optional[dict], optional): Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns: DataFrame : DataFrame depending on the chosen type.","title":"Feature Store"},{"location":"generated/feature_store/#feature-store","text":"","title":"Feature Store"},{"location":"generated/feature_store/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_store/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/feature_store/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_store/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/feature_store/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/feature_store/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/feature_store/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/feature_store/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/feature_store/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/feature_store/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/feature_store/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/feature_store/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/feature_store/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/feature_store/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_store/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object. [source]","title":"create_expectation"},{"location":"generated/feature_store/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/feature_store/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/feature_store/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/feature_store/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"create_transformation_function"},{"location":"generated/feature_store/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source]","title":"get_expectation"},{"location":"generated/feature_store/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source]","title":"get_expectations"},{"location":"generated/feature_store/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/feature_store/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/feature_store/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/feature_store/#get_on_demand_feature_groups","text":"FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_groups"},{"location":"generated/feature_store/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/feature_store/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/feature_store/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/feature_store/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_datasets"},{"location":"generated/feature_store/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/feature_store/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source]","title":"get_transformation_functions"},{"location":"generated/feature_store/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Args: query (str): The SQL query to execute. dataframe_type (Optional[str], optional): The type of the returned dataframe. Defaults to \"default\" which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online (Optional[bool], optional): Set to true to execute the query against the online feature store. Defaults to False. read_options (Optional[dict], optional): Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns: DataFrame : DataFrame depending on the chosen type.","title":"sql"},{"location":"generated/feature_validation/","text":"Feature Validation with the Hopsworks Feature Store # Correct feature data is essential for developing accurate machine learning models. Raw data being ingested into the feature store maybe suffer from incorrect or corrupt values, may need to be validated against certain features depending on the domain. For example, a feature representing the customer's age should not be a negative number and should always have a value set. HSFS provides an API to define expectations on data being inserted into feature groups and also view results over time of these expectations in the form of feature validations. Feature validations can therefore be easily integrated with existing feature ingestion pipelines. HSFS utilizes the Deequ open source library and support is currently being added for working with the Great Expectations . Below we describe the different API components of the hsfs feature validation API and we walk you through Feature validation is part of the HSFS Java/Scala and Python API for working with Feature Groups. Users work with the abstractions: Rule definitions # Rule definitions is a set of pre-defined and immutable rules ( RuleDefiniton ) that are unique by name and are used for creating validation rules ( Rule ) and expectations ( Expectation ) applied on a dataframe that is ingested into a Feature Group. The following table describes all the supported rule definitions (code examples are shown in the section below). Name: The name of the rule. Predicate: The type of value this rule accepts. For example, when using HAS_MIN you need to set the value with the value parameter. Accepted type: The data type of the value users set for this rule. For example, the value of the HAS_MIN predicate must be a fractional number. Feature type: The data type of the feature this rule can be applied to. For example, the HAS_MIN rule can only be applied on numerical features. If a rule is to be applied to an incompatible feature type, an error will be thrown when the expectation is attached on the feature group. If a rule does not have a feature type, then it can be applied on both numerical and categorical features. Description: A short description of what the rule validates. If an expectation contains a rule that can be applied Name Predicate Accepted type Feature type Description HAS_APPROX_COUNT_DISTINCT VALUE Fractional Assert on the approximate count distinct of a feature. HAS_APPROX_QUANTILE VALUE Fractional Numerical Assert on the approximate quantile of a feature. HAS_COMPLETENESS VALUE Fractional Assert on the uniqueness of a single or combined set of features. HAS_CORRELATION VALUE Fractional Numerical Assert on the pearson correleation between two features. HAS_DATATYPE ACCEPTED_TYPE String Assert on the fraction of rows that conform to the given data type. HAS_DISTINCTNESS VALUE Fractional Assert on the distincness of a single or combined set of features. HAS_ENTROPY VALUE Fractional Assert on the entropy of a feature. HAS_MAX VALUE Fractional Numerical Assert on the max of a feature. HAS_MAX_LENGTH VALUE String Categorical Assert on the maximum length of the feature value. HAS_MEAN VALUE Fractional Numerical Assert on the mean of a feature. HAS_MIN VALUE Fractional Numerical Assert on the min of a feature. HAS_MIN_LENGTH VALUE String Categorical Assert on the minimum length of the feature value. HAS_MUTUAL_INFORMATION VALUE Fractional Assert on the mutual information between two features. HAS_NUMBER_OF_DISTINCT_VALUES VALUE Integral Assert on the number of distinct values of a feature. HAS_PATTERN PATTERN String Categorical Assert on the average compliance of the feature to the regular expression. HAS_SIZE VALUE Integral Assert on the number of rows of the dataframe. HAS_STANDARD_DEVIATION VALUE Fractional Numerical Assert on the standard deviation of a feature. HAS_SUM VALUE Fractional Numerical Assert on the sum of a feature. HAS_UNIQUENESS VALUE Fractional Assert on the uniqueness of a single or combined set of features. HAS_UNIQUE_VALUE_RATIO VALUE Fractional Assert on the unique value ratio of of a single or combined set of features. IS_CONTAINED_IN LEGAL_VALUES String Assert that every non-null value of feature is contained in a set of predefined values. IS_GREATER_THAN VALUE Fractional Assert on feature A values being greater than feature B. IS_GREATER_THAN_OR_EQUAL_TO VALUE Fractional Assert on feature A values being greater than or equal to those of feature B. IS_LESS_THAN VALUE Fractional Assert on feature A values being less that feature B. IS_LESS_THAN_OR_EQUAL_TO VALUE Fractional Assert on feature A values being less or equal to those of feature B. IS_NON_NEGATIVE VALUE Fractional Assert on feature containing non negative values. IS_POSITIVE VALUE Boolean Assert on a feature containing non negative values. Retrieval # [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. For example, to get all available rule definitions in hsfs: Python import hsfs connection = hsfs . connection () rules = connection . get_rules () Scala import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules () and to get a rule definition by name: Python import hsfs connection = hsfs . connection () rules = connection . get_rules () Scala import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules () Properties # [source] RuleDefinition # hsfs . ruledefinition . RuleDefinition ( name , predicate , accepted_type , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified. [source] accepted_type # The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source] description # [source] feature_type # The type of the feature, one of \"Numerical\", \"Categorical\". [source] name # Name of the rule definition. Unique across all features stores. [source] predicate # Predicate of the rule definition, one of \"VALUE\", \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\". Rules # Used as part of expectations that are applied on ingested features. Rule names correspond to the names of the rule definitions (see section above) and you can set the severity level and the actual values that the feature should respect. Defining expectation rules # In general, rule values can be an exact value or a range of values. For example, if you need a feature to be ingested if its minimum value is below zero, then you can set min(0) and max(0) but if you want the minimum to fall within a range of 0 and 1000 then you need to set min(0) and max(1000) . See section Expectations below for a detailed example. Rules that operate on tuples of features, for example HAS_CORRELEATION , are applied on the first two features as defined in the expectation (as ordered within the expectation). Examples # Python rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 10 )] # the mininum value of the feature needs to be at least 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , max = 10 )] # the minimum value of the feature needs to be at most 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 , max = 10 )] # the minimum value of the feature needs to be between 0 and 10 rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 )] # At least 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.5 )] # 10-50% of all instances of the feature need to of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"IS_CONTAINED_IN\" , level = \"ERROR\" , legal_values = [ \"a\" , \"b\" ], min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be contained in the legal_values list rules = [ Rule ( name = \"HAS_PATTERN\" , level = \"ERROR\" , pattern = \"a+\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to match the given pattern Scala Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). level ( Level . WARNING ). build () // the mininum value of the feature needs to be at least 10 Rule . createRule ( RuleName . HAS_MIN ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be at most 10 Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be between 0 and 10 Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). level ( Level . ERROR ). build () // At least 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). max ( 10 ). level ( Level . ERROR ). build () // At most 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . IS_CONTAINED_IN ). legalValues ( Seq ( \"a\" , \"b\" )). min ( 0.1 ). max ( 0.1 ). level ( Level . ERROR ). build () // # Exactly 10% of all instances of the feature need to be contained in the legal_values list Rule . createRule ( RuleName . HAS_PATTERN ). pattern ( \"a+\" ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to match the given pattern Properties # [source] Rule # hsfs . rule . Rule ( name , level , min = None , max = None , value = None , pattern = None , accepted_type = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only. [source] accepted_type # Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source] legal_values # List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source] level # Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source] max # The upper bound of the value range this feature should fall into. [source] min # The lower bound of the value range this feature should fall into. [source] name # Name of the rule as found in rule definitions. [source] pattern # Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule. [source] value # The upper bound of the value range this feature should fall into. Expectations # A set of rule instances that are applied on a set of features. Expectations are created at the feature store level and can be attached to multiple feature groups. An expectation contains a list of featues it is applied to. If the feature group the expectation is attached to, does not contain all the expectations features, the expectation will not be met. Creation # Create an expectation with two rules for ensuring the min and max of the features are valid: Python expectation_sales = fs . create_expectation ( \"sales\" , description = \"min and max sales limits\" , features = [ \"salary\" , \"commission\" ], rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 ), Rule ( name = \"HAS_MAX\" , level = \"ERROR\" , max = 1000000 )]) expectation_sales . save () Scala // Create an expectation for the \"salary\" and \"commissio\" features so that their min value is \"10\" and their max is \"100\" val expectationSales = ( fs . createExpectation () . rules ( Seq ( Rule . createRule ( RuleName . HAS_MIN ). min ( 0 ). level ( Level . WARNING ). build (), //Set rule by name Rule . createRule ( ruleMax ). max ( 1000000 ). level ( Level . ERROR ). build ())) //Set rule by passing the RuleDefinition metadata . name ( \"sales\" ) . description ( \"min and max sales limits\" ) . features ( Seq ( \"salary\" , \"commission\" )) . build ()) expectationSales . save () [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object. Retrieval # [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. Properties # [source] Expectation # hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store. [source] description # Description of the expectation. [source] features # List of features this expectation is applied to. [source] name # Name of the expectation, unique per feature store (project). [source] rules # List of rules applied to the features of the expectation. Methods # [source] save # Expectation . save () Persist the expectation metadata object to the feature store. Validations # The results of expectations against the ingested dataframe are assigned a validation time and are persisted within the Feature Store. Users can then retrieve validation results by validation time and by commit time for time-travel enabled feature groups. Validation Type # Feature Validation is disabled by default, by having the validation_type feature group attribute set to NONE. The list of allowed validation types are: STRICT: Data validation is performed and feature group is updated only if validation status is \"Success\" WARNING: Data validation is performed and feature group is updated only if validation status is \"Warning\" or lower ALL: Data validation is performed and feature group is updated only if validation status is \"Failure\" or lower NONE: Data validation not performed on feature group For example, to update the validation type to all: Python fg . validation_type = \"ALL\" Scala import com . logicalclocks . hsfs . metadata . validation . _ fg . updateValidationType ( ValidationType . ALL ) Validate # You can also apply the expectations on a dataframe without inserting data, that can be helpful for debugging. [source] validate # FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object. Retrieval # [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. Properties # [source] ValidationResult # hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group. [source] features # Feature of the validation result on which the rule was applied. [source] message # Message describing the outcome of applying the rule against the feature. [source] rule # Feature of the validation result on which the rule was applied. [source] status # [source] value # The computed value of the feature according to the rule.","title":"Feature Validation"},{"location":"generated/feature_validation/#feature-validation-with-the-hopsworks-feature-store","text":"Correct feature data is essential for developing accurate machine learning models. Raw data being ingested into the feature store maybe suffer from incorrect or corrupt values, may need to be validated against certain features depending on the domain. For example, a feature representing the customer's age should not be a negative number and should always have a value set. HSFS provides an API to define expectations on data being inserted into feature groups and also view results over time of these expectations in the form of feature validations. Feature validations can therefore be easily integrated with existing feature ingestion pipelines. HSFS utilizes the Deequ open source library and support is currently being added for working with the Great Expectations . Below we describe the different API components of the hsfs feature validation API and we walk you through Feature validation is part of the HSFS Java/Scala and Python API for working with Feature Groups. Users work with the abstractions:","title":"Feature Validation with the Hopsworks Feature Store"},{"location":"generated/feature_validation/#rule-definitions","text":"Rule definitions is a set of pre-defined and immutable rules ( RuleDefiniton ) that are unique by name and are used for creating validation rules ( Rule ) and expectations ( Expectation ) applied on a dataframe that is ingested into a Feature Group. The following table describes all the supported rule definitions (code examples are shown in the section below). Name: The name of the rule. Predicate: The type of value this rule accepts. For example, when using HAS_MIN you need to set the value with the value parameter. Accepted type: The data type of the value users set for this rule. For example, the value of the HAS_MIN predicate must be a fractional number. Feature type: The data type of the feature this rule can be applied to. For example, the HAS_MIN rule can only be applied on numerical features. If a rule is to be applied to an incompatible feature type, an error will be thrown when the expectation is attached on the feature group. If a rule does not have a feature type, then it can be applied on both numerical and categorical features. Description: A short description of what the rule validates. If an expectation contains a rule that can be applied Name Predicate Accepted type Feature type Description HAS_APPROX_COUNT_DISTINCT VALUE Fractional Assert on the approximate count distinct of a feature. HAS_APPROX_QUANTILE VALUE Fractional Numerical Assert on the approximate quantile of a feature. HAS_COMPLETENESS VALUE Fractional Assert on the uniqueness of a single or combined set of features. HAS_CORRELATION VALUE Fractional Numerical Assert on the pearson correleation between two features. HAS_DATATYPE ACCEPTED_TYPE String Assert on the fraction of rows that conform to the given data type. HAS_DISTINCTNESS VALUE Fractional Assert on the distincness of a single or combined set of features. HAS_ENTROPY VALUE Fractional Assert on the entropy of a feature. HAS_MAX VALUE Fractional Numerical Assert on the max of a feature. HAS_MAX_LENGTH VALUE String Categorical Assert on the maximum length of the feature value. HAS_MEAN VALUE Fractional Numerical Assert on the mean of a feature. HAS_MIN VALUE Fractional Numerical Assert on the min of a feature. HAS_MIN_LENGTH VALUE String Categorical Assert on the minimum length of the feature value. HAS_MUTUAL_INFORMATION VALUE Fractional Assert on the mutual information between two features. HAS_NUMBER_OF_DISTINCT_VALUES VALUE Integral Assert on the number of distinct values of a feature. HAS_PATTERN PATTERN String Categorical Assert on the average compliance of the feature to the regular expression. HAS_SIZE VALUE Integral Assert on the number of rows of the dataframe. HAS_STANDARD_DEVIATION VALUE Fractional Numerical Assert on the standard deviation of a feature. HAS_SUM VALUE Fractional Numerical Assert on the sum of a feature. HAS_UNIQUENESS VALUE Fractional Assert on the uniqueness of a single or combined set of features. HAS_UNIQUE_VALUE_RATIO VALUE Fractional Assert on the unique value ratio of of a single or combined set of features. IS_CONTAINED_IN LEGAL_VALUES String Assert that every non-null value of feature is contained in a set of predefined values. IS_GREATER_THAN VALUE Fractional Assert on feature A values being greater than feature B. IS_GREATER_THAN_OR_EQUAL_TO VALUE Fractional Assert on feature A values being greater than or equal to those of feature B. IS_LESS_THAN VALUE Fractional Assert on feature A values being less that feature B. IS_LESS_THAN_OR_EQUAL_TO VALUE Fractional Assert on feature A values being less or equal to those of feature B. IS_NON_NEGATIVE VALUE Fractional Assert on feature containing non negative values. IS_POSITIVE VALUE Boolean Assert on a feature containing non negative values.","title":"Rule definitions"},{"location":"generated/feature_validation/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/feature_validation/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/feature_validation/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. For example, to get all available rule definitions in hsfs: Python import hsfs connection = hsfs . connection () rules = connection . get_rules () Scala import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules () and to get a rule definition by name: Python import hsfs connection = hsfs . connection () rules = connection . get_rules () Scala import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val rules = connection . getRules ()","title":"get_rule"},{"location":"generated/feature_validation/#properties","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#ruledefinition","text":"hsfs . ruledefinition . RuleDefinition ( name , predicate , accepted_type , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified. [source]","title":"RuleDefinition"},{"location":"generated/feature_validation/#accepted_type","text":"The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source]","title":"accepted_type"},{"location":"generated/feature_validation/#description","text":"[source]","title":"description"},{"location":"generated/feature_validation/#feature_type","text":"The type of the feature, one of \"Numerical\", \"Categorical\". [source]","title":"feature_type"},{"location":"generated/feature_validation/#name","text":"Name of the rule definition. Unique across all features stores. [source]","title":"name"},{"location":"generated/feature_validation/#predicate","text":"Predicate of the rule definition, one of \"VALUE\", \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\".","title":"predicate"},{"location":"generated/feature_validation/#rules","text":"Used as part of expectations that are applied on ingested features. Rule names correspond to the names of the rule definitions (see section above) and you can set the severity level and the actual values that the feature should respect.","title":"Rules"},{"location":"generated/feature_validation/#defining-expectation-rules","text":"In general, rule values can be an exact value or a range of values. For example, if you need a feature to be ingested if its minimum value is below zero, then you can set min(0) and max(0) but if you want the minimum to fall within a range of 0 and 1000 then you need to set min(0) and max(1000) . See section Expectations below for a detailed example. Rules that operate on tuples of features, for example HAS_CORRELEATION , are applied on the first two features as defined in the expectation (as ordered within the expectation).","title":"Defining expectation rules"},{"location":"generated/feature_validation/#examples","text":"Python rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 10 )] # the mininum value of the feature needs to be at least 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , max = 10 )] # the minimum value of the feature needs to be at most 10 rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 , max = 10 )] # the minimum value of the feature needs to be between 0 and 10 rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 )] # At least 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.5 )] # 10-50% of all instances of the feature need to of type String rules = [ Rule ( name = \"HAS_DATATYPE\" , level = \"ERROR\" , accepted_type = \"String\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be of type String rules = [ Rule ( name = \"IS_CONTAINED_IN\" , level = \"ERROR\" , legal_values = [ \"a\" , \"b\" ], min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to be contained in the legal_values list rules = [ Rule ( name = \"HAS_PATTERN\" , level = \"ERROR\" , pattern = \"a+\" , min = 0.1 , max = 0.1 )] # Exactly 10% of all instances of the feature need to match the given pattern Scala Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). level ( Level . WARNING ). build () // the mininum value of the feature needs to be at least 10 Rule . createRule ( RuleName . HAS_MIN ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be at most 10 Rule . createRule ( RuleName . HAS_MIN ). min ( 10 ). max ( 10 ). level ( Level . WARNING ). build () // the minimum value of the feature needs to be between 0 and 10 Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). level ( Level . ERROR ). build () // At least 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). max ( 10 ). level ( Level . ERROR ). build () // At most 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . HAS_DATATYPE ). acceptedType ( AcceptedType . String ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to be of type String Rule . createRule ( RuleName . IS_CONTAINED_IN ). legalValues ( Seq ( \"a\" , \"b\" )). min ( 0.1 ). max ( 0.1 ). level ( Level . ERROR ). build () // # Exactly 10% of all instances of the feature need to be contained in the legal_values list Rule . createRule ( RuleName . HAS_PATTERN ). pattern ( \"a+\" ). min ( 10 ). max ( 10 ). level ( Level . ERROR ). build () // Exactly 10% of all instances of the feature need to match the given pattern","title":"Examples"},{"location":"generated/feature_validation/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#rule","text":"hsfs . rule . Rule ( name , level , min = None , max = None , value = None , pattern = None , accepted_type = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only. [source]","title":"Rule"},{"location":"generated/feature_validation/#accepted_type_1","text":"Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source]","title":"accepted_type"},{"location":"generated/feature_validation/#legal_values","text":"List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source]","title":"legal_values"},{"location":"generated/feature_validation/#level","text":"Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source]","title":"level"},{"location":"generated/feature_validation/#max","text":"The upper bound of the value range this feature should fall into. [source]","title":"max"},{"location":"generated/feature_validation/#min","text":"The lower bound of the value range this feature should fall into. [source]","title":"min"},{"location":"generated/feature_validation/#name_1","text":"Name of the rule as found in rule definitions. [source]","title":"name"},{"location":"generated/feature_validation/#pattern","text":"Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule. [source]","title":"pattern"},{"location":"generated/feature_validation/#value","text":"The upper bound of the value range this feature should fall into.","title":"value"},{"location":"generated/feature_validation/#expectations","text":"A set of rule instances that are applied on a set of features. Expectations are created at the feature store level and can be attached to multiple feature groups. An expectation contains a list of featues it is applied to. If the feature group the expectation is attached to, does not contain all the expectations features, the expectation will not be met.","title":"Expectations"},{"location":"generated/feature_validation/#creation","text":"Create an expectation with two rules for ensuring the min and max of the features are valid: Python expectation_sales = fs . create_expectation ( \"sales\" , description = \"min and max sales limits\" , features = [ \"salary\" , \"commission\" ], rules = [ Rule ( name = \"HAS_MIN\" , level = \"WARNING\" , min = 0 ), Rule ( name = \"HAS_MAX\" , level = \"ERROR\" , max = 1000000 )]) expectation_sales . save () Scala // Create an expectation for the \"salary\" and \"commissio\" features so that their min value is \"10\" and their max is \"100\" val expectationSales = ( fs . createExpectation () . rules ( Seq ( Rule . createRule ( RuleName . HAS_MIN ). min ( 0 ). level ( Level . WARNING ). build (), //Set rule by name Rule . createRule ( ruleMax ). max ( 1000000 ). level ( Level . ERROR ). build ())) //Set rule by passing the RuleDefinition metadata . name ( \"sales\" ) . description ( \"min and max sales limits\" ) . features ( Seq ( \"salary\" , \"commission\" )) . build ()) expectationSales . save () [source]","title":"Creation"},{"location":"generated/feature_validation/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object.","title":"create_expectation"},{"location":"generated/feature_validation/#retrieval_1","text":"[source]","title":"Retrieval"},{"location":"generated/feature_validation/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source]","title":"get_expectation"},{"location":"generated/feature_validation/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store.","title":"get_expectations"},{"location":"generated/feature_validation/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#expectation","text":"hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store. [source]","title":"Expectation"},{"location":"generated/feature_validation/#description_1","text":"Description of the expectation. [source]","title":"description"},{"location":"generated/feature_validation/#features","text":"List of features this expectation is applied to. [source]","title":"features"},{"location":"generated/feature_validation/#name_2","text":"Name of the expectation, unique per feature store (project). [source]","title":"name"},{"location":"generated/feature_validation/#rules_1","text":"List of rules applied to the features of the expectation.","title":"rules"},{"location":"generated/feature_validation/#methods","text":"[source]","title":"Methods"},{"location":"generated/feature_validation/#save","text":"Expectation . save () Persist the expectation metadata object to the feature store.","title":"save"},{"location":"generated/feature_validation/#validations","text":"The results of expectations against the ingested dataframe are assigned a validation time and are persisted within the Feature Store. Users can then retrieve validation results by validation time and by commit time for time-travel enabled feature groups.","title":"Validations"},{"location":"generated/feature_validation/#validation-type","text":"Feature Validation is disabled by default, by having the validation_type feature group attribute set to NONE. The list of allowed validation types are: STRICT: Data validation is performed and feature group is updated only if validation status is \"Success\" WARNING: Data validation is performed and feature group is updated only if validation status is \"Warning\" or lower ALL: Data validation is performed and feature group is updated only if validation status is \"Failure\" or lower NONE: Data validation not performed on feature group For example, to update the validation type to all: Python fg . validation_type = \"ALL\" Scala import com . logicalclocks . hsfs . metadata . validation . _ fg . updateValidationType ( ValidationType . ALL )","title":"Validation Type"},{"location":"generated/feature_validation/#validate","text":"You can also apply the expectations on a dataframe without inserting data, that can be helpful for debugging. [source]","title":"Validate"},{"location":"generated/feature_validation/#validate_1","text":"FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/feature_validation/#retrieval_2","text":"[source]","title":"Retrieval"},{"location":"generated/feature_validation/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"get_validations"},{"location":"generated/feature_validation/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/feature_validation/#validationresult","text":"hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group. [source]","title":"ValidationResult"},{"location":"generated/feature_validation/#features_1","text":"Feature of the validation result on which the rule was applied. [source]","title":"features"},{"location":"generated/feature_validation/#message","text":"Message describing the outcome of applying the rule against the feature. [source]","title":"message"},{"location":"generated/feature_validation/#rule_1","text":"Feature of the validation result on which the rule was applied. [source]","title":"rule"},{"location":"generated/feature_validation/#status","text":"[source]","title":"status"},{"location":"generated/feature_validation/#value_1","text":"The computed value of the feature according to the rule.","title":"value"},{"location":"generated/on_demand_feature_group/","text":"On-Demand (External) Feature Groups # On-demand (External) Feature Groups are Feature Groups for which the data is stored on an external storage system (e.g. Data Warehouse, S3, ADLS). From an API perspective, on-demand feature groups can be used in the same way as regular feature groups. Users can pick features from on-demand feature groups to create training datasets. On-demand feature groups can be also used as data source to create derived features, meaning features on which additional feature engineering is applied. On-demand feature groups rely on Storage Connectors to identify the location and to authenticate with the external storage. When the on-demand feature group is defined on top of an external database capabale of running SQL statements (i.e. when using the JDBC, Redshift or Snowflake connectors), the on-demand feature group needs to be defined as a SQL statement. SQL statements can contain feature engineering transformations, when reading the on-demand feature group, the SQL statement is pushed down to the storage for execution. Python Define a SQL based on-demand feature group # Retrieve the storage connector defined before redshift_conn = fs . get_storage_connector ( \"telco_redshift_cluster\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_redshift\" , version = 1 , query = \"select * from telco\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = redshift_conn , statistics_config = True ) telco_on_dmd . save () Scala Connecting from Hopsworks val redshiftConn = fs . getRedshiftConnector ( \"telco_redshift_cluster\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_redshift_scala\" ) . version ( 2 ) . query ( \"select * from telco\" ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( redshiftConn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save () When defining an on-demand feature group on top of a object store/external filesystem (i.e. when using the S3 or the ADLS connector) the underlying data is required to have a schema. The underlying data can be stored in ORC, Parquet, Delta, Hudi or Avro, and the schema for the feature group will be extracted by the files metadata. Python Define a SQL based on-demand feature group # Retrieve the storage connector defined before s3_conn = fs . get_storage_connector ( \"telco_s3_bucket\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_s3\" , version = 1 , data_format = \"parquet\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = s3_conn , statistics_config = True ) telco_on_dmd . save () Scala Connecting from Hopsworks val s3Conn = fs . getS3Connector ( \"telco_s3_bucket\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_s3\" ) . version ( 1 ) . dataFormat ( OnDemandDataFormat . PARQUET ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( s3Conn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save () Use cases # There are two use cases in which a user can benefit from on-demand feature groups: Existing feature engineering pipelines : in case users have recently migrated to Hopsworks Feature Store and they have existing feature engineering pipelines in production. Users can register the output of the existing pipelines as on-demand feature groups in Hopsworks, and immediately use their features to build training datasets. With on-demand feature groups, users do not have to modify the existing pipelines to write to the Hopsworks Feature Store. Data Ingestion : on-demand feature groups can be used as a data source. The benefit of using on-demand feature groups to ingest data from external sources is that the Hopsworks Feature Store keeps track of where the data is located and how to authenticate with the external storage system. In addition to that, the Hopsworks Feature Store tracks also the schema of the underlying data and will make sure that, if something changes in the underlying schema, the ingestion pipeline fails with a clear error. Limitations # Hopsworks Feature Store does not support time-travel capabilities for on-demand feature groups. Moreover, as the data resides on external systems, on-demand feature groups cannot be made available online for low latency serving. To make data from an on-demand feature group available online, users need to define an online enabled feature group and hava a job that periodically reads data from the on-demand feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on on-demand feature groups. Likewise it is not possibile to call the read() or show() methods on queries containing on-demand feature groups. Nevertheless, on-demand feature groups can be used from a Python engine to create training datasets. Creation # [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. Retrieval # [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] created # [source] creator # [source] data_format # [source] description # [source] expectations_names # The names of expectations attached to this feature group. [source] features # [source] id # [source] name # [source] options # [source] path # [source] query # [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] storage_connector # [source] validation_type # Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source] version # Methods # [source] add_tag # OnDemandFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] attach_expectation # OnDemandFeatureGroup . attach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] compute_statistics # OnDemandFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # OnDemandFeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # OnDemandFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] detach_expectation # OnDemandFeatureGroup . detach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] filter # OnDemandFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] get_expectation # OnDemandFeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source] get_expectations # OnDemandFeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source] get_feature # OnDemandFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # OnDemandFeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # OnDemandFeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # OnDemandFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_validations # OnDemandFeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source] read # OnDemandFeatureGroup . read ( dataframe_type = \"default\" ) Get the feature group as a DataFrame. [source] save # OnDemandFeatureGroup . save () [source] select # OnDemandFeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # OnDemandFeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # OnDemandFeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # OnDemandFeatureGroup . show ( n ) Show the first n rows of the feature group. [source] update_description # OnDemandFeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source] update_statistics_config # OnDemandFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # OnDemandFeatureGroup . validate () Run validation based on the attached expectations Returns FeatureGroupValidation . The feature group validation metadata object.","title":"On-demand (External) Feature Group"},{"location":"generated/on_demand_feature_group/#on-demand-external-feature-groups","text":"On-demand (External) Feature Groups are Feature Groups for which the data is stored on an external storage system (e.g. Data Warehouse, S3, ADLS). From an API perspective, on-demand feature groups can be used in the same way as regular feature groups. Users can pick features from on-demand feature groups to create training datasets. On-demand feature groups can be also used as data source to create derived features, meaning features on which additional feature engineering is applied. On-demand feature groups rely on Storage Connectors to identify the location and to authenticate with the external storage. When the on-demand feature group is defined on top of an external database capabale of running SQL statements (i.e. when using the JDBC, Redshift or Snowflake connectors), the on-demand feature group needs to be defined as a SQL statement. SQL statements can contain feature engineering transformations, when reading the on-demand feature group, the SQL statement is pushed down to the storage for execution. Python Define a SQL based on-demand feature group # Retrieve the storage connector defined before redshift_conn = fs . get_storage_connector ( \"telco_redshift_cluster\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_redshift\" , version = 1 , query = \"select * from telco\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = redshift_conn , statistics_config = True ) telco_on_dmd . save () Scala Connecting from Hopsworks val redshiftConn = fs . getRedshiftConnector ( \"telco_redshift_cluster\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_redshift_scala\" ) . version ( 2 ) . query ( \"select * from telco\" ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( redshiftConn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save () When defining an on-demand feature group on top of a object store/external filesystem (i.e. when using the S3 or the ADLS connector) the underlying data is required to have a schema. The underlying data can be stored in ORC, Parquet, Delta, Hudi or Avro, and the schema for the feature group will be extracted by the files metadata. Python Define a SQL based on-demand feature group # Retrieve the storage connector defined before s3_conn = fs . get_storage_connector ( \"telco_s3_bucket\" ) telco_on_dmd = fs . create_on_demand_feature_group ( name = \"telco_s3\" , version = 1 , data_format = \"parquet\" , description = \"On-demand feature group for telecom customer data\" , storage_connector = s3_conn , statistics_config = True ) telco_on_dmd . save () Scala Connecting from Hopsworks val s3Conn = fs . getS3Connector ( \"telco_s3_bucket\" ) val telcoOnDmd = ( fs . createOnDemandFeatureGroup () . name ( \"telco_s3\" ) . version ( 1 ) . dataFormat ( OnDemandDataFormat . PARQUET ) . description ( \"On-demand feature group for telecom customer data\" ) . storageConnector ( s3Conn ) . statisticsEnabled ( true ) . build ()) telcoOnDmd . save ()","title":"On-Demand (External) Feature Groups"},{"location":"generated/on_demand_feature_group/#use-cases","text":"There are two use cases in which a user can benefit from on-demand feature groups: Existing feature engineering pipelines : in case users have recently migrated to Hopsworks Feature Store and they have existing feature engineering pipelines in production. Users can register the output of the existing pipelines as on-demand feature groups in Hopsworks, and immediately use their features to build training datasets. With on-demand feature groups, users do not have to modify the existing pipelines to write to the Hopsworks Feature Store. Data Ingestion : on-demand feature groups can be used as a data source. The benefit of using on-demand feature groups to ingest data from external sources is that the Hopsworks Feature Store keeps track of where the data is located and how to authenticate with the external storage system. In addition to that, the Hopsworks Feature Store tracks also the schema of the underlying data and will make sure that, if something changes in the underlying schema, the ingestion pipeline fails with a clear error.","title":"Use cases"},{"location":"generated/on_demand_feature_group/#limitations","text":"Hopsworks Feature Store does not support time-travel capabilities for on-demand feature groups. Moreover, as the data resides on external systems, on-demand feature groups cannot be made available online for low latency serving. To make data from an on-demand feature group available online, users need to define an online enabled feature group and hava a job that periodically reads data from the on-demand feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on on-demand feature groups. Likewise it is not possibile to call the read() or show() methods on queries containing on-demand feature groups. Nevertheless, on-demand feature groups can be used from a Python engine to create training datasets.","title":"Limitations"},{"location":"generated/on_demand_feature_group/#creation","text":"[source]","title":"Creation"},{"location":"generated/on_demand_feature_group/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object.","title":"create_on_demand_feature_group"},{"location":"generated/on_demand_feature_group/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/on_demand_feature_group/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_on_demand_feature_group"},{"location":"generated/on_demand_feature_group/#properties","text":"[source]","title":"Properties"},{"location":"generated/on_demand_feature_group/#created","text":"[source]","title":"created"},{"location":"generated/on_demand_feature_group/#creator","text":"[source]","title":"creator"},{"location":"generated/on_demand_feature_group/#data_format","text":"[source]","title":"data_format"},{"location":"generated/on_demand_feature_group/#description","text":"[source]","title":"description"},{"location":"generated/on_demand_feature_group/#expectations_names","text":"The names of expectations attached to this feature group. [source]","title":"expectations_names"},{"location":"generated/on_demand_feature_group/#features","text":"[source]","title":"features"},{"location":"generated/on_demand_feature_group/#id","text":"[source]","title":"id"},{"location":"generated/on_demand_feature_group/#name","text":"[source]","title":"name"},{"location":"generated/on_demand_feature_group/#options","text":"[source]","title":"options"},{"location":"generated/on_demand_feature_group/#path","text":"[source]","title":"path"},{"location":"generated/on_demand_feature_group/#query","text":"[source]","title":"query"},{"location":"generated/on_demand_feature_group/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/on_demand_feature_group/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/on_demand_feature_group/#storage_connector","text":"[source]","title":"storage_connector"},{"location":"generated/on_demand_feature_group/#validation_type","text":"Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source]","title":"validation_type"},{"location":"generated/on_demand_feature_group/#version","text":"","title":"version"},{"location":"generated/on_demand_feature_group/#methods","text":"[source]","title":"Methods"},{"location":"generated/on_demand_feature_group/#add_tag","text":"OnDemandFeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/on_demand_feature_group/#attach_expectation","text":"OnDemandFeatureGroup . attach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"attach_expectation"},{"location":"generated/on_demand_feature_group/#compute_statistics","text":"OnDemandFeatureGroup . compute_statistics () Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/on_demand_feature_group/#delete","text":"OnDemandFeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/on_demand_feature_group/#delete_tag","text":"OnDemandFeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/on_demand_feature_group/#detach_expectation","text":"OnDemandFeatureGroup . detach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"detach_expectation"},{"location":"generated/on_demand_feature_group/#filter","text":"OnDemandFeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/on_demand_feature_group/#get_expectation","text":"OnDemandFeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"get_expectation"},{"location":"generated/on_demand_feature_group/#get_expectations","text":"OnDemandFeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source]","title":"get_expectations"},{"location":"generated/on_demand_feature_group/#get_feature","text":"OnDemandFeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/on_demand_feature_group/#get_statistics","text":"OnDemandFeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/on_demand_feature_group/#get_tag","text":"OnDemandFeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/on_demand_feature_group/#get_tags","text":"OnDemandFeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/on_demand_feature_group/#get_validations","text":"OnDemandFeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source]","title":"get_validations"},{"location":"generated/on_demand_feature_group/#read","text":"OnDemandFeatureGroup . read ( dataframe_type = \"default\" ) Get the feature group as a DataFrame. [source]","title":"read"},{"location":"generated/on_demand_feature_group/#save","text":"OnDemandFeatureGroup . save () [source]","title":"save"},{"location":"generated/on_demand_feature_group/#select","text":"OnDemandFeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/on_demand_feature_group/#select_all","text":"OnDemandFeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/on_demand_feature_group/#select_except","text":"OnDemandFeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/on_demand_feature_group/#show","text":"OnDemandFeatureGroup . show ( n ) Show the first n rows of the feature group. [source]","title":"show"},{"location":"generated/on_demand_feature_group/#update_description","text":"OnDemandFeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/on_demand_feature_group/#update_statistics_config","text":"OnDemandFeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/on_demand_feature_group/#validate","text":"OnDemandFeatureGroup . validate () Run validation based on the attached expectations Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/project/","text":"Project/Connection # In Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Feature Store. However, it is possible to share Feature Stores among projects. When working with the Feature Store from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Feature Stores simultaneously. A connection to a Hopsworks instance is represented by a Connection object . Its main purpose is to retrieve the API Key if you are connecting from an external environment and subsequently to retrieve the needed certificates to communicate with the Feature Store services. The handle can then be used to retrieve a reference to the Feature Store you want to operate on. Examples # Python Connecting from Hopsworks import hsfs conn = hsfs . connection () fs = conn . get_feature_store () Connecting from Databricks In order to connect from Databricks, follow the integration guide . You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: Azure Use this method when working with Hopsworks on Azure. import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from AWS SageMaker In order to connect from SageMaker, follow the integration guide to setup the API Key. You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from Python environment To connect from a simple Python environment, you can provide the API Key as a file as shown in the SageMaker example above, or you provide the value directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_value = ( \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" \"xNnAzJ7RV6H\" ) ) fs = conn . get_feature_store () Scala Connecting from Hopsworks import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val fs = connection . getFeatureStore (); Connecting from Databricks TBD Connecting from AWS SageMaker The Scala client version of hsfs is not supported on AWS SageMaker, please use the Python client. Sharing a Feature Store # Connections are on a project-level, however, it is possible to share feature stores among projects, so even if you have a connection to one project, you can retireve a handle to any feature store shared with that project. To share a feature store, you can follow these steps: Sharing a Feature Store Open the project of the feature store that you would like to share on Hopsworks. Go to the Data Set browser and right click the Featurestore.db entry. Click Share with , then select Project and choose the project you wish to share the feature store with. Select the permissions level that the project user members should have on the feature store and click Share . Open the project you just shared the feature store with. Go to the Data Sets browser and there you should see the shared feature store as [project_name_of_shared_feature_store]::Featurestore.db . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this feature store from the other project. Sharing a feature store between projects Accepting a shared feature store from a project Connection Handle # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"Project/Connection"},{"location":"generated/project/#projectconnection","text":"In Hopsworks a Project is a sandboxed set of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Feature Store. However, it is possible to share Feature Stores among projects. When working with the Feature Store from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Feature Stores simultaneously. A connection to a Hopsworks instance is represented by a Connection object . Its main purpose is to retrieve the API Key if you are connecting from an external environment and subsequently to retrieve the needed certificates to communicate with the Feature Store services. The handle can then be used to retrieve a reference to the Feature Store you want to operate on.","title":"Project/Connection"},{"location":"generated/project/#examples","text":"Python Connecting from Hopsworks import hsfs conn = hsfs . connection () fs = conn . get_feature_store () Connecting from Databricks In order to connect from Databricks, follow the integration guide . You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: Azure Use this method when working with Hopsworks on Azure. import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from AWS SageMaker In order to connect from SageMaker, follow the integration guide to setup the API Key. You can then simply connect by using your chosen way of retrieving the API Key: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , secrets_store = \"secretsmanager\" ) fs = conn . get_feature_store () Alternatively you can pass the API Key as a file or directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_file = \"featurestore.key\" ) fs = conn . get_feature_store () Connecting from Python environment To connect from a simple Python environment, you can provide the API Key as a file as shown in the SageMaker example above, or you provide the value directly: import hsfs conn = hsfs . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_fs_admin000\" , hostname_verification = False , api_key_value = ( \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" \"xNnAzJ7RV6H\" ) ) fs = conn . get_feature_store () Scala Connecting from Hopsworks import com . logicalclocks . hsfs . _ val connection = HopsworksConnection . builder (). build (); val fs = connection . getFeatureStore (); Connecting from Databricks TBD Connecting from AWS SageMaker The Scala client version of hsfs is not supported on AWS SageMaker, please use the Python client.","title":"Examples"},{"location":"generated/project/#sharing-a-feature-store","text":"Connections are on a project-level, however, it is possible to share feature stores among projects, so even if you have a connection to one project, you can retireve a handle to any feature store shared with that project. To share a feature store, you can follow these steps: Sharing a Feature Store Open the project of the feature store that you would like to share on Hopsworks. Go to the Data Set browser and right click the Featurestore.db entry. Click Share with , then select Project and choose the project you wish to share the feature store with. Select the permissions level that the project user members should have on the feature store and click Share . Open the project you just shared the feature store with. Go to the Data Sets browser and there you should see the shared feature store as [project_name_of_shared_feature_store]::Featurestore.db . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this feature store from the other project. Sharing a feature store between projects Accepting a shared feature store from a project","title":"Sharing a Feature Store"},{"location":"generated/project/#connection-handle","text":"[source]","title":"Connection Handle"},{"location":"generated/project/#connection","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/project/#methods","text":"[source]","title":"Methods"},{"location":"generated/project/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/project/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/project/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/project/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rule"},{"location":"generated/project/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/project/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"setup_databricks"},{"location":"generated/query_vs_dataframe/","text":"Query vs DataFrame # HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters and point in time queries. To enable this functionality, we are introducing a new expressive Query abstraction with HSFS that provides these operations and guarantees reproducible creation of training datasets from features in the Feature Store. The new joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d ) # materialize query in the specified file format td . save ( feature_join ) # use materialized training dataset for training, possibly in a different environment td = fs . get_training_dataset ( \u201c rain_dataset \u201d , version = 1 ) # get TFRecordDataset to use in a TensorFlow model dataset = td . tf_data () . tf_record_dataset ( batch_size = 32 , num_epochs = 100 ) # reproduce query for online feature store and drop label for inference jdbc_querystring = td . get_query ( online = True , with_label = False ) Scala # create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . build ()) # materialize query in the specified file format td . save ( featureJoin ) # use materialized training dataset for training , possibly in a different environment val td = fs . getTrainingDataset ( \u201c rain_dataset \u201d , 1 ) # reproduce query for online feature store and drop label for inference val jdbcQuerystring = td . getQuery ( true , false ) If a data scientist wants to modify a new feature that is not available in the Feature Store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the Feature Store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources. The Query Abstraction # Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation. Examples # Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) Scala val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" )) Join # Similarly joins return queries. The simplest join, is one of two feature groups without specifying a join key or type. By default Hopsworks will use the maximal matching subset of the primary key of the two feature groups as joining key, if not specified otherwise. Python # Returns Query feature_join = rain_fg . join ( temperature_fg ) Scala # Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joines feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the name of the features to join on. Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure. Filter # In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) Scala val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ))) Methods # [source] as_of # Query . as_of ( wallclock_time ) [source] filter # Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] join # Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source] json # Query . json () [source] pull_changes # Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source] read # Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source] show # Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source] to_dict # Query . to_dict () [source] to_string # Query . to_string ( online = False ) Properties # [source] left_feature_group_end_time # [source] left_feature_group_start_time #","title":"Query vs. Dataframe"},{"location":"generated/query_vs_dataframe/#query-vs-dataframe","text":"HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters and point in time queries. To enable this functionality, we are introducing a new expressive Query abstraction with HSFS that provides these operations and guarantees reproducible creation of training datasets from features in the Feature Store. The new joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d ) # materialize query in the specified file format td . save ( feature_join ) # use materialized training dataset for training, possibly in a different environment td = fs . get_training_dataset ( \u201c rain_dataset \u201d , version = 1 ) # get TFRecordDataset to use in a TensorFlow model dataset = td . tf_data () . tf_record_dataset ( batch_size = 32 , num_epochs = 100 ) # reproduce query for online feature store and drop label for inference jdbc_querystring = td . get_query ( online = True , with_label = False ) Scala # create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . build ()) # materialize query in the specified file format td . save ( featureJoin ) # use materialized training dataset for training , possibly in a different environment val td = fs . getTrainingDataset ( \u201c rain_dataset \u201d , 1 ) # reproduce query for online feature store and drop label for inference val jdbcQuerystring = td . getQuery ( true , false ) If a data scientist wants to modify a new feature that is not available in the Feature Store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the Feature Store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.","title":"Query vs DataFrame"},{"location":"generated/query_vs_dataframe/#the-query-abstraction","text":"Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation.","title":"The Query Abstraction"},{"location":"generated/query_vs_dataframe/#examples","text":"Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) Scala val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" ))","title":"Examples"},{"location":"generated/query_vs_dataframe/#join","text":"Similarly joins return queries. The simplest join, is one of two feature groups without specifying a join key or type. By default Hopsworks will use the maximal matching subset of the primary key of the two feature groups as joining key, if not specified otherwise. Python # Returns Query feature_join = rain_fg . join ( temperature_fg ) Scala # Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joines feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the name of the features to join on. Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure.","title":"Join"},{"location":"generated/query_vs_dataframe/#filter","text":"In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) Scala val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) Scala val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )))","title":"Filter"},{"location":"generated/query_vs_dataframe/#methods","text":"[source]","title":"Methods"},{"location":"generated/query_vs_dataframe/#as_of","text":"Query . as_of ( wallclock_time ) [source]","title":"as_of"},{"location":"generated/query_vs_dataframe/#filter_1","text":"Query . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature query . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: query . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: query . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/query_vs_dataframe/#join_1","text":"Query . join ( sub_query , on = [], left_on = [], right_on = [], join_type = \"inner\" , prefix = None ) Join Query with another Query. If no join keys are specified, Hopsworks will use the maximal matching subset of the primary keys of the feature groups you are joining. Joins of one level are supported, no neted joins. Arguments sub_query hsfs.constructor.query.Query : Right-hand side query to join. on Optional[List[str]] : List of feature names to join on if they are available in both feature groups. Defaults to [] . left_on Optional[List[str]] : List of feature names to join on from the left feature group of the join. Defaults to [] . right_on Optional[List[str]] : List of feature names to join on from the right feature group of the join. Defaults to [] . join_type Optional[str] : Type of join to perform, can be \"inner\" , \"outer\" , \"left\" or \"right\" . Defaults to \"inner\". prefix Optional[str] : User provided prefix to avoid feature name clash. Prefix is applied to the right feature group of the query. Defaults to None . Returns Query : A new Query object representing the join. [source]","title":"join"},{"location":"generated/query_vs_dataframe/#json","text":"Query . json () [source]","title":"json"},{"location":"generated/query_vs_dataframe/#pull_changes","text":"Query . pull_changes ( wallclock_start_time , wallclock_end_time ) [source]","title":"pull_changes"},{"location":"generated/query_vs_dataframe/#read","text":"Query . read ( online = False , dataframe_type = \"default\" , read_options = {}) Read the specified query into a DataFrame. It is possible to specify the storage (online/offline) to read from and the type of the output DataFrame (Spark, Pandas, Numpy, Python Lists). Arguments online Optional[bool] : Read from online storage. Defaults to False . dataframe_type Optional[str] : DataFrame type to return. Defaults to \"default\" . read_options Optional[dict] : Optional dictionary with read options for Spark. Defaults to {} . Returns DataFrame : DataFrame depending on the chosen type. [source]","title":"read"},{"location":"generated/query_vs_dataframe/#show","text":"Query . show ( n , online = False ) Show the first N rows of the Query. Arguments n int : Number of rows to show. online Optional[bool] : Show from online storage. Defaults to False . [source]","title":"show"},{"location":"generated/query_vs_dataframe/#to_dict","text":"Query . to_dict () [source]","title":"to_dict"},{"location":"generated/query_vs_dataframe/#to_string","text":"Query . to_string ( online = False )","title":"to_string"},{"location":"generated/query_vs_dataframe/#properties","text":"[source]","title":"Properties"},{"location":"generated/query_vs_dataframe/#left_feature_group_end_time","text":"[source]","title":"left_feature_group_end_time"},{"location":"generated/query_vs_dataframe/#left_feature_group_start_time","text":"","title":"left_feature_group_start_time"},{"location":"generated/statistics/","text":"Statistics # HSFS provides functionality to compute statistics for training datasets and feature groups and save these along with their other metadata in the feature store . These statistics are meant to be helpful for Data Scientists to perform explorative data analysis and then recognize suitable features or training datasets for models. Statistics are configured on a training dataset or feature group level using a StatisticsConfig object. This object can be passed at creation time of the dataset or group or it can later on be updated through the API. [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] ) For example, to enable all statistics (descriptive, histograms and correlations) for a training dataset: Python from hsfs.statistics_config import StatisticsConfig td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d , statistics_config = StatisticsConfig ( true , true , true )) Scala val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . statisticsConfig ( new StatisticsConfig ( true , true , true )) . build ()) And similarly for feature groups. Default StatisticsConfig By default all training datasets and feature groups will be configured such that only descriptive statistics are computed. However, you can also enable histograms and correlations or limit the features for which statistics are computed. Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"Statistics"},{"location":"generated/statistics/#statistics","text":"HSFS provides functionality to compute statistics for training datasets and feature groups and save these along with their other metadata in the feature store . These statistics are meant to be helpful for Data Scientists to perform explorative data analysis and then recognize suitable features or training datasets for models. Statistics are configured on a training dataset or feature group level using a StatisticsConfig object. This object can be passed at creation time of the dataset or group or it can later on be updated through the API. [source]","title":"Statistics"},{"location":"generated/statistics/#statisticsconfig","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] ) For example, to enable all statistics (descriptive, histograms and correlations) for a training dataset: Python from hsfs.statistics_config import StatisticsConfig td = fs . create_training_dataset ( \"rain_dataset\" , version = 1 , label = \u201d weekly_rain \u201d , data_format = \u201d tfrecords \u201d , statistics_config = StatisticsConfig ( true , true , true )) Scala val td = ( fs . createTrainingDataset () . name ( \"rain_dataset\" ) . version ( 1 ) . label ( \u201d weekly_rain \u201d ) . dataFormat ( \u201d tfrecords \u201d ) . statisticsConfig ( new StatisticsConfig ( true , true , true )) . build ()) And similarly for feature groups. Default StatisticsConfig By default all training datasets and feature groups will be configured such that only descriptive statistics are computed. However, you can also enable histograms and correlations or limit the features for which statistics are computed.","title":"StatisticsConfig"},{"location":"generated/statistics/#properties","text":"[source]","title":"Properties"},{"location":"generated/statistics/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/statistics/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/statistics/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/statistics/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/storage_connector/","text":"Storage Connector # Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. HopsFS # Properties # [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] spark_options # HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # HopsFSConnector . to_dict () [source] update_from_response_json # HopsFSConnector . update_from_response_json ( json_dict ) JDBC # Properties # [source] arguments # Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source] connection_string # JDBC connection string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] spark_options # JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # JdbcConnector . to_dict () [source] update_from_response_json # JdbcConnector . update_from_response_json ( json_dict ) S3 # Properties # [source] access_key # Access key. [source] bucket # Return the bucket for S3 connectors. [source] description # User provided description of the storage connector. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] session_token # Session token. Methods # [source] prepare_spark # S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # S3Connector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # S3Connector . to_dict () [source] update_from_response_json # S3Connector . update_from_response_json ( json_dict ) Redshift # Properties # [source] arguments # Additional JDBC, REDSHIFT, or Snowflake arguments. [source] auto_create # Database username for redshift cluster. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] description # User provided description of the storage connector. [source] expiration # Cluster temporary credential expiration time. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] table_name # Table name for redshift cluster. Methods # [source] read # RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # RedshiftConnector . to_dict () [source] update_from_response_json # RedshiftConnector . update_from_response_json ( json_dict ) Azure Data Lake Storage # Properties # [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] container_name # Container name of the ADLS storage connector [source] description # User provided description of the storage connector. [source] directory_id # Directory ID of the ADLS storage connector [source] generation # Generation of the ADLS storage connector [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. ADLS) - return the path of the connector [source] service_credential # Service credential of the ADLS storage connector Methods # [source] prepare_spark # AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] spark_options # AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # AdlsConnector . to_dict () [source] update_from_response_json # AdlsConnector . update_from_response_json ( json_dict ) Snowflake # Properties # [source] account # Account of the Snowflake storage connector [source] database # Database of the Snowflake storage connector [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Additional options for the Snowflake storage connector [source] password # Password of the Snowflake storage connector [source] role # Role of the Snowflake storage connector [source] schema # Schema of the Snowflake storage connector [source] table # Table of the Snowflake storage connector [source] token # OAuth token of the Snowflake storage connector [source] url # URL of the Snowflake storage connector [source] user # User of the Snowflake storage connector [source] warehouse # Warehouse of the Snowflake storage connector Methods # [source] read # SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] snowflake_connector_options # SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source] spark_options # SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # SnowflakeConnector . to_dict () [source] update_from_response_json # SnowflakeConnector . update_from_response_json ( json_dict )","title":"Storage Connector"},{"location":"generated/storage_connector/#storage-connector","text":"Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS.","title":"Storage Connector"},{"location":"generated/storage_connector/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/storage_connector/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/storage_connector/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/storage_connector/#hopsfs","text":"","title":"HopsFS"},{"location":"generated/storage_connector/#properties","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#description","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name","text":"Name of the storage connector.","title":"name"},{"location":"generated/storage_connector/#methods","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read","text":"HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/storage_connector/#spark_options","text":"HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict","text":"HopsFSConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json","text":"HopsFSConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#jdbc","text":"","title":"JDBC"},{"location":"generated/storage_connector/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#arguments","text":"Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source]","title":"arguments"},{"location":"generated/storage_connector/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/storage_connector/#description_1","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id_1","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_1","text":"Name of the storage connector.","title":"name"},{"location":"generated/storage_connector/#methods_1","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_1","text":"JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/storage_connector/#spark_options_1","text":"JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_1","text":"JdbcConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_1","text":"JdbcConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#s3","text":"","title":"S3"},{"location":"generated/storage_connector/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/storage_connector/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/storage_connector/#description_2","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/storage_connector/#id_2","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_2","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/storage_connector/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/storage_connector/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/storage_connector/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/storage_connector/#session_token","text":"Session token.","title":"session_token"},{"location":"generated/storage_connector/#methods_2","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#prepare_spark","text":"S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/storage_connector/#read_2","text":"S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/storage_connector/#refetch","text":"S3Connector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_2","text":"S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_2","text":"S3Connector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_2","text":"S3Connector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#redshift","text":"","title":"Redshift"},{"location":"generated/storage_connector/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#arguments_1","text":"Additional JDBC, REDSHIFT, or Snowflake arguments. [source]","title":"arguments"},{"location":"generated/storage_connector/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/storage_connector/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/storage_connector/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/storage_connector/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/storage_connector/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/storage_connector/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/storage_connector/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/storage_connector/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/storage_connector/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/storage_connector/#description_3","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/storage_connector/#iam_role_1","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/storage_connector/#id_3","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_3","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/storage_connector/#methods_3","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_3","text":"RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/storage_connector/#refetch_1","text":"RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/storage_connector/#spark_options_3","text":"RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_3","text":"RedshiftConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_3","text":"RedshiftConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#azure-data-lake-storage","text":"","title":"Azure Data Lake Storage"},{"location":"generated/storage_connector/#properties_4","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/storage_connector/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/storage_connector/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/storage_connector/#description_4","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/storage_connector/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/storage_connector/#id_4","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_4","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#path_1","text":"If the connector refers to a path (e.g. ADLS) - return the path of the connector [source]","title":"path"},{"location":"generated/storage_connector/#service_credential","text":"Service credential of the ADLS storage connector","title":"service_credential"},{"location":"generated/storage_connector/#methods_4","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#prepare_spark_1","text":"AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/storage_connector/#read_4","text":"AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/storage_connector/#spark_options_4","text":"AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_4","text":"AdlsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_4","text":"AdlsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/storage_connector/#snowflake","text":"","title":"Snowflake"},{"location":"generated/storage_connector/#properties_5","text":"[source]","title":"Properties"},{"location":"generated/storage_connector/#account","text":"Account of the Snowflake storage connector [source]","title":"account"},{"location":"generated/storage_connector/#database","text":"Database of the Snowflake storage connector [source]","title":"database"},{"location":"generated/storage_connector/#description_5","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/storage_connector/#id_5","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/storage_connector/#name_5","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/storage_connector/#options","text":"Additional options for the Snowflake storage connector [source]","title":"options"},{"location":"generated/storage_connector/#password","text":"Password of the Snowflake storage connector [source]","title":"password"},{"location":"generated/storage_connector/#role","text":"Role of the Snowflake storage connector [source]","title":"role"},{"location":"generated/storage_connector/#schema","text":"Schema of the Snowflake storage connector [source]","title":"schema"},{"location":"generated/storage_connector/#table","text":"Table of the Snowflake storage connector [source]","title":"table"},{"location":"generated/storage_connector/#token","text":"OAuth token of the Snowflake storage connector [source]","title":"token"},{"location":"generated/storage_connector/#url","text":"URL of the Snowflake storage connector [source]","title":"url"},{"location":"generated/storage_connector/#user","text":"User of the Snowflake storage connector [source]","title":"user"},{"location":"generated/storage_connector/#warehouse","text":"Warehouse of the Snowflake storage connector","title":"warehouse"},{"location":"generated/storage_connector/#methods_5","text":"[source]","title":"Methods"},{"location":"generated/storage_connector/#read_5","text":"SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/storage_connector/#snowflake_connector_options","text":"SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source]","title":"snowflake_connector_options"},{"location":"generated/storage_connector/#spark_options_5","text":"SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/storage_connector/#to_dict_5","text":"SnowflakeConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/storage_connector/#update_from_response_json_5","text":"SnowflakeConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/tags/","text":"Tags # The feature store enables users to attach tags to artifacts, such as feature groups or training datasets. Tags are aditional metadata attached to your artifacts and thus they can be used for an enhanced full text search. Adding tags to an artifact provides users with a more dynamic metadata content that can be used for both storage as well as enhancing artifact discoverability. Note : By default Hopsworks makes all metadata searchable, users can opt out for particular featurestores if they want to keep them private. A tag is a { key : value } association, providing additional information about the data, such as for example geographic origin. This is useful in an organization as it adds more context to your data making it easier to share and discover data and artifacts. Note : Tagging is only available in the enterprise version. Tag Schemas # The first step is to define the schemas of tags that can later be attached. These schemas follow the https://json-schema.org as reference. The schemas define legal jsons and these can be primitives, objects or arrays. The schemas themselves are also defined as jsons. Allowed primitive types are: string boolean integer number (float) A tag of primitive type - string would look like: { \"type\" : \"string\" } and this would allow a json value of: string tag value We can also define arbitrarily complex json schemas, such as: { \"type\" : \"object\", \"properties\" : { \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } } }, \"required\" : [\"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a value that follows this schema would be: { \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"] } Properties section of a tag is a dictionary that defines field names and types. Json schema are pretty lenient, all that the properties section tells us, is that if a field appears, it should be of the appropriate type. If the json object contains the field first_name , this field cannot be of type boolean , it has to be of type string . What we emphasize here, is that the properties section does not impose that fields declared are mandatory, or that the json object cannot contain other fields that were not defined in the schemas. Required section enforces the mandatory fields. In our case above first_name , last_name , age are declared as mandatory, while hobbies is left as an optional field. Additional Properties section enforces the strictness of the schema. If we set this to false the json objects of this schema can only use fields that are declared (mandatoriy or optional) by the schema. No undeclared fields will be allowed. Type object is the default type for schemas, so you can ommit it if you want to keep the schema short. Advanced tag usage # We can use additional properties of schemas as defined by https://json-schema.org to enhance our previous person schema: Add a $schema section to allow us to use more advanced features of the json schemas defined in later drafts. The default schema draft is 4 and we will use 7 here (latest). Add an id field that is of type string but has to follow a particular regex pattern. We will also make this field mandatory. Set some rules on age , for example age should be an Integer between 0 and 150. Add an address field that is itself an object. { \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\" : \"object\", \"properties\" : { \"id\" : { \"type\" : \"string\", \"pattern\" : \"^[A-Z]{2}[0-9]{4}$\" }, \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\", \"minimum\" : 0, \"maximum\" : 150 }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } }, \"address\" : { \"street\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" } } }, \"required\" : [\"id\", \"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a valid value for this new schema would be: { \"id\" : \"AB1234\", \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"], \"address\" : { \"street\" : \"Vasagatan nr. 12\", \"city\" : \"Stockholm\" } } Basic tag usage # Our new Feature Store UI is aimed to ease the general use of tags by users and we thus currently support only basic tags usage there. Basic tag schemas allow only one level depth fields. So types of fields are limited to primitives or array of primitives. Basic schemas also only allow the required and additionalProperties sections. Creating schemas # Schemas are defined at a cluster level, so they are available to all projects. They can only be defined by a user with admin rights. Attach tags using the UI # Tags can be attached using the feature store UI or programmatically using the API. This API will be described in the rest of this notebook. Search with Tags # Once tags are attached, the feature groups are now searchable also by their tags, both keys and values. Advanced tag usage # Our old UI allows for the full capabilities of the json schemas as defined by https://json-schema.org . This includes allowing to define tags of primitive type as well as arbitrarily complex json objects. Creating advanced schemas # If you want to create advanced schemas, you can do this in our old UI by providing the raw json value of the schema. Attach advanced tag values using the UI # Our old UI also provides you with a way to attach tag values that follow these advanced semantics by providing their raw json values. Examples # You can try our tags example in notebooks populated by our feature store tour under notebooks hsfs/tags . You can also check our example on https://examples.hopsworks.ai/featurestore/hsfs/tags/feature_store_tags/ . API # Both feature groups and training datasets contain the following add, get, get_all, delete operations for working with tags. Feature Groups # Attach tags # Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source] add_tag # FeatureGroupBase . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. Get tag by key # [source] get_tag # FeatureGroupBase . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. Get all tags # [source] get_tags # FeatureGroupBase . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. Delete tag # [source] delete_tag # FeatureGroupBase . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. Training Datasets # Attach tags # Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. Get tag by key # [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. Get all tags # [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. Delete tag # [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag.","title":"Feature Store Tags"},{"location":"generated/tags/#tags","text":"The feature store enables users to attach tags to artifacts, such as feature groups or training datasets. Tags are aditional metadata attached to your artifacts and thus they can be used for an enhanced full text search. Adding tags to an artifact provides users with a more dynamic metadata content that can be used for both storage as well as enhancing artifact discoverability. Note : By default Hopsworks makes all metadata searchable, users can opt out for particular featurestores if they want to keep them private. A tag is a { key : value } association, providing additional information about the data, such as for example geographic origin. This is useful in an organization as it adds more context to your data making it easier to share and discover data and artifacts. Note : Tagging is only available in the enterprise version.","title":"Tags"},{"location":"generated/tags/#tag-schemas","text":"The first step is to define the schemas of tags that can later be attached. These schemas follow the https://json-schema.org as reference. The schemas define legal jsons and these can be primitives, objects or arrays. The schemas themselves are also defined as jsons. Allowed primitive types are: string boolean integer number (float) A tag of primitive type - string would look like: { \"type\" : \"string\" } and this would allow a json value of: string tag value We can also define arbitrarily complex json schemas, such as: { \"type\" : \"object\", \"properties\" : { \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } } }, \"required\" : [\"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a value that follows this schema would be: { \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"] } Properties section of a tag is a dictionary that defines field names and types. Json schema are pretty lenient, all that the properties section tells us, is that if a field appears, it should be of the appropriate type. If the json object contains the field first_name , this field cannot be of type boolean , it has to be of type string . What we emphasize here, is that the properties section does not impose that fields declared are mandatory, or that the json object cannot contain other fields that were not defined in the schemas. Required section enforces the mandatory fields. In our case above first_name , last_name , age are declared as mandatory, while hobbies is left as an optional field. Additional Properties section enforces the strictness of the schema. If we set this to false the json objects of this schema can only use fields that are declared (mandatoriy or optional) by the schema. No undeclared fields will be allowed. Type object is the default type for schemas, so you can ommit it if you want to keep the schema short.","title":"Tag Schemas"},{"location":"generated/tags/#advanced-tag-usage","text":"We can use additional properties of schemas as defined by https://json-schema.org to enhance our previous person schema: Add a $schema section to allow us to use more advanced features of the json schemas defined in later drafts. The default schema draft is 4 and we will use 7 here (latest). Add an id field that is of type string but has to follow a particular regex pattern. We will also make this field mandatory. Set some rules on age , for example age should be an Integer between 0 and 150. Add an address field that is itself an object. { \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\" : \"object\", \"properties\" : { \"id\" : { \"type\" : \"string\", \"pattern\" : \"^[A-Z]{2}[0-9]{4}$\" }, \"first_name\" : { \"type\" : \"string\" }, \"last_name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\", \"minimum\" : 0, \"maximum\" : 150 }, \"hobbies\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } }, \"address\" : { \"street\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" } } }, \"required\" : [\"id\", \"first_name\", \"last_name\", \"age\"], \"additionalProperties\": false } and a valid value for this new schema would be: { \"id\" : \"AB1234\", \"first_name\" : \"John\", \"last_name\" : \"Doe\", \"age\" : 27, \"hobbies\" : [\"tennis\", \"reading\"], \"address\" : { \"street\" : \"Vasagatan nr. 12\", \"city\" : \"Stockholm\" } }","title":"Advanced tag usage"},{"location":"generated/tags/#basic-tag-usage","text":"Our new Feature Store UI is aimed to ease the general use of tags by users and we thus currently support only basic tags usage there. Basic tag schemas allow only one level depth fields. So types of fields are limited to primitives or array of primitives. Basic schemas also only allow the required and additionalProperties sections.","title":"Basic tag usage"},{"location":"generated/tags/#creating-schemas","text":"Schemas are defined at a cluster level, so they are available to all projects. They can only be defined by a user with admin rights.","title":"Creating schemas"},{"location":"generated/tags/#attach-tags-using-the-ui","text":"Tags can be attached using the feature store UI or programmatically using the API. This API will be described in the rest of this notebook.","title":"Attach tags using the UI"},{"location":"generated/tags/#search-with-tags","text":"Once tags are attached, the feature groups are now searchable also by their tags, both keys and values.","title":"Search with Tags"},{"location":"generated/tags/#advanced-tag-usage_1","text":"Our old UI allows for the full capabilities of the json schemas as defined by https://json-schema.org . This includes allowing to define tags of primitive type as well as arbitrarily complex json objects.","title":"Advanced tag usage"},{"location":"generated/tags/#creating-advanced-schemas","text":"If you want to create advanced schemas, you can do this in our old UI by providing the raw json value of the schema.","title":"Creating advanced schemas"},{"location":"generated/tags/#attach-advanced-tag-values-using-the-ui","text":"Our old UI also provides you with a way to attach tag values that follow these advanced semantics by providing their raw json values.","title":"Attach advanced tag values using the UI"},{"location":"generated/tags/#examples","text":"You can try our tags example in notebooks populated by our feature store tour under notebooks hsfs/tags . You can also check our example on https://examples.hopsworks.ai/featurestore/hsfs/tags/feature_store_tags/ .","title":"Examples"},{"location":"generated/tags/#api","text":"Both feature groups and training datasets contain the following add, get, get_all, delete operations for working with tags.","title":"API"},{"location":"generated/tags/#feature-groups","text":"","title":"Feature Groups"},{"location":"generated/tags/#attach-tags","text":"Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source]","title":"Attach tags"},{"location":"generated/tags/#add_tag","text":"FeatureGroupBase . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"add_tag"},{"location":"generated/tags/#get-tag-by-key","text":"[source]","title":"Get tag by key"},{"location":"generated/tags/#get_tag","text":"FeatureGroupBase . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag.","title":"get_tag"},{"location":"generated/tags/#get-all-tags","text":"[source]","title":"Get all tags"},{"location":"generated/tags/#get_tags","text":"FeatureGroupBase . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags.","title":"get_tags"},{"location":"generated/tags/#delete-tag","text":"[source]","title":"Delete tag"},{"location":"generated/tags/#delete_tag","text":"FeatureGroupBase . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag.","title":"delete_tag"},{"location":"generated/tags/#training-datasets","text":"","title":"Training Datasets"},{"location":"generated/tags/#attach-tags_1","text":"Note : You can only attach one tag value for a tag name. By calling the add operation on the same tag multiple times, you perform an update operation. If you require attaching multiple values to a tag, like maybe a sequence, consider changing the tag type to an array of the type you just defined. [source]","title":"Attach tags"},{"location":"generated/tags/#add_tag_1","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"add_tag"},{"location":"generated/tags/#get-tag-by-key_1","text":"[source]","title":"Get tag by key"},{"location":"generated/tags/#get_tag_1","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag.","title":"get_tag"},{"location":"generated/tags/#get-all-tags_1","text":"[source]","title":"Get all tags"},{"location":"generated/tags/#get_tags_1","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags.","title":"get_tags"},{"location":"generated/tags/#delete-tag_1","text":"[source]","title":"Delete tag"},{"location":"generated/tags/#delete_tag_1","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag.","title":"delete_tag"},{"location":"generated/training_dataset/","text":"Training Dataset # The training dataset abstraction in Hopsworks Feature Store allows users to group a set of features (potentially from multiple different feature groups) with labels for training a model to do a particular prediction task. The training dataset is a versioned and managed dataset and is stored in HopsFS as tfrecords , parquet , csv , or tsv files. Versioning # Training Dataset can be versioned. Data Scientist should use the version to indicate to the model, as well as to the schema or the feature engineering logic of the features associated to this training dataset. Creation # To create training dataset, the user supplies a Pandas, Numpy or Spark dataframe with features and labels together with metadata. Once the training dataset has been created, the dataset is discoverable in the feature registry and users can use it to train models. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] coalesce # If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source] data_format # File format of the training dataset. [source] description # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] prepared_statement_engine # JDBC connection engine to retrieve connections to online features store from. [source] prepared_statements # The dict object of prepared_statements as values and kes as indices of positions in the query for selecting features from feature groups of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] serving_keys # Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] transformation_functions # Set transformation functions. [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete # TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_serving_vector # TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] init_prepared_statement # TrainingDataset . init_prepared_statement ( external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] tf_data # TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError . TFData engine # [source] tf_record_dataset # TFDataEngine . tf_record_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False , serialized_ndarray_fname = [], ) Reads tfrecord files and returns ParallelMapDataset or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object ParallelMapDataset can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 3 ) td . tf_data ( target_name = \"id\" ) . tf_record_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set True then one hot encode labels, defaults to False . num_classes Optional[int] : If above True then provide number of target classes, defaults to None . process Optional[bool] : If set True api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname Optional[list] : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . ParallelMapDataset . If process is set to False . [source] tf_csv_dataset # TFDataEngine . tf_csv_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False ) Reads csv files and returns CsvDatasetV2 or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object CsvDatasetV2 can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 1 ) td . tf_data ( target_name = \"id\" ) . tf_csv_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set true then one hot encode labels, defaults to False . num_classes Optional[int] : If above true then provide number of target classes, defaults to None . process Optional[bool] : If set true api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . CsvDatasetV2 . If process is set to False .","title":"Training Dataset"},{"location":"generated/training_dataset/#training-dataset","text":"The training dataset abstraction in Hopsworks Feature Store allows users to group a set of features (potentially from multiple different feature groups) with labels for training a model to do a particular prediction task. The training dataset is a versioned and managed dataset and is stored in HopsFS as tfrecords , parquet , csv , or tsv files.","title":"Training Dataset"},{"location":"generated/training_dataset/#versioning","text":"Training Dataset can be versioned. Data Scientist should use the version to indicate to the model, as well as to the schema or the feature engineering logic of the features associated to this training dataset.","title":"Versioning"},{"location":"generated/training_dataset/#creation","text":"To create training dataset, the user supplies a Pandas, Numpy or Spark dataframe with features and labels together with metadata. Once the training dataset has been created, the dataset is discoverable in the feature registry and users can use it to train models. [source]","title":"Creation"},{"location":"generated/training_dataset/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/training_dataset/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/training_dataset/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/training_dataset/#properties","text":"[source]","title":"Properties"},{"location":"generated/training_dataset/#coalesce","text":"If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source]","title":"coalesce"},{"location":"generated/training_dataset/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/training_dataset/#description","text":"[source]","title":"description"},{"location":"generated/training_dataset/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/training_dataset/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/training_dataset/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/training_dataset/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/training_dataset/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/training_dataset/#prepared_statement_engine","text":"JDBC connection engine to retrieve connections to online features store from. [source]","title":"prepared_statement_engine"},{"location":"generated/training_dataset/#prepared_statements","text":"The dict object of prepared_statements as values and kes as indices of positions in the query for selecting features from feature groups of the training dataset. [source]","title":"prepared_statements"},{"location":"generated/training_dataset/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/training_dataset/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/training_dataset/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/training_dataset/#serving_keys","text":"Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source]","title":"serving_keys"},{"location":"generated/training_dataset/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/training_dataset/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/training_dataset/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/training_dataset/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/training_dataset/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/training_dataset/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/training_dataset/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/training_dataset/#methods","text":"[source]","title":"Methods"},{"location":"generated/training_dataset/#add_tag","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/training_dataset/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/training_dataset/#delete","text":"TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/training_dataset/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/training_dataset/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/training_dataset/#get_serving_vector","text":"TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vector"},{"location":"generated/training_dataset/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/training_dataset/#get_tag","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/training_dataset/#get_tags","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/training_dataset/#init_prepared_statement","text":"TrainingDataset . init_prepared_statement ( external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source]","title":"init_prepared_statement"},{"location":"generated/training_dataset/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/training_dataset/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/training_dataset/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/training_dataset/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/training_dataset/#tf_data","text":"TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source]","title":"tf_data"},{"location":"generated/training_dataset/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/training_dataset/#tfdata-engine","text":"[source]","title":"TFData engine"},{"location":"generated/training_dataset/#tf_record_dataset","text":"TFDataEngine . tf_record_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False , serialized_ndarray_fname = [], ) Reads tfrecord files and returns ParallelMapDataset or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object ParallelMapDataset can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 3 ) td . tf_data ( target_name = \"id\" ) . tf_record_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set True then one hot encode labels, defaults to False . num_classes Optional[int] : If above True then provide number of target classes, defaults to None . process Optional[bool] : If set True api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname Optional[list] : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . ParallelMapDataset . If process is set to False . [source]","title":"tf_record_dataset"},{"location":"generated/training_dataset/#tf_csv_dataset","text":"TFDataEngine . tf_csv_dataset ( batch_size = None , num_epochs = None , one_hot_encode_labels = False , num_classes = None , process = False ) Reads csv files and returns CsvDatasetV2 or PrefetchDataset object, depending on process set to False or True , respectively. If process set to False returned object CsvDatasetV2 can be further processed by user. For example applying custom transformations to features, batching, caching etc. process=True will return PrefetchDataset object, that contains tuple of feature vector and label, already batched and ready to input into model training. Example of using tf_record_dataset: connection = hsfs . connection () fs = connection . get_feature_store (); td = fs . get_training_dataset ( \"sample_model\" , 1 ) td . tf_data ( target_name = \"id\" ) . tf_csv_dataset ( batch_size = 1 , num_epochs = 1 , process = True ) Arguments batch_size Optional[int] : Size of batch, defaults to None . num_epochs Optional[int] : Number of epochs to train, defaults to None . one_hot_encode_labels Optional[bool] : If set true then one hot encode labels, defaults to False . num_classes Optional[int] : If above true then provide number of target classes, defaults to None . process Optional[bool] : If set true api will optimise tf data read operation, and return feature vector for model with single input, defaults to False . serialized_ndarray_fname : Names of features that contain serialised multi dimensional arrays, defaults to [] . Returns PrefetchDataset . If process is set to True . CsvDatasetV2 . If process is set to False .","title":"tf_csv_dataset"},{"location":"generated/transformation_functions/","text":"Transformation Functions # HSFS provides functionality to attach transformation functions to training datasets . To be able to attach a transformation function to a training dataset it has to be either part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators. Don't decorate transformation function with Pyspark @udf or @pandas_udf , as well as don't use any Pyspark dependencies. HSFS will decorate transformation function only if it is used inside Pyspark application. Examples # Python Register transformation function plus_one in the Hopsworks feature store. from hsfs_transformers import transformers plus_one_meta = fs . create_transformation_function ( transformation_function = transformers . plus_one , output_type = int , version = 1 ) plus_one_meta . save () To retrieve all transformation functions from the feature store use get_transformation_functions that will return list of TransformatioFunction objects. Specific transformation function can be retrieved by get_transformation_function method where user can provide name and version of the transformation function. If only name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 fs . get_transformation_function ( name = \"plus_one\" ) # get transformation function by name and version. fs . get_transformation_function ( name = \"plus_one\" , version = 2 ) To attach transformation function to training dataset provide transformation functions as dict, where key is feature name and value is online transformation function name. Also training dataset must be created from the Query object. Once attached transformation function will be applied on whenever save , insert and get_serving_vector methods are called on training dataset object. Python Attaching transformation functions to the training dataset plus_one_meta = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) fs . create_training_dataset ( name = \"td_demo\" , description = \"Dataset to train the demo model\" , data_format = \"csv\" , transformation_functions = { \"feature_name\" : plus_one_meta } statistics_config = None , version = 1 ) td . save ( join_query ) Scala support Creating and attaching Transformation functions to training datasets are not supported for hsfs scala client. If training dataset with transformation function was created using python client and later insert or getServingVector methods are called on this training dataset from scala client hsfs will throw an exception. Transfromation Function # [source] TransformationFunction # hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ) Properties # [source] id # Training dataset id. [source] name # [source] output_type # [source] source_code_content # [source] transformation_fn # [source] transformer_code # [source] version # Methods # [source] delete # TransformationFunction . delete () Delete transformation function from backend. [source] save # TransformationFunction . save () Persist transformation function in backend. Creation # [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. Retrieval # [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"Transformation Functions"},{"location":"generated/transformation_functions/#transformation-functions","text":"HSFS provides functionality to attach transformation functions to training datasets . To be able to attach a transformation function to a training dataset it has to be either part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators. Don't decorate transformation function with Pyspark @udf or @pandas_udf , as well as don't use any Pyspark dependencies. HSFS will decorate transformation function only if it is used inside Pyspark application.","title":"Transformation Functions"},{"location":"generated/transformation_functions/#examples","text":"Python Register transformation function plus_one in the Hopsworks feature store. from hsfs_transformers import transformers plus_one_meta = fs . create_transformation_function ( transformation_function = transformers . plus_one , output_type = int , version = 1 ) plus_one_meta . save () To retrieve all transformation functions from the feature store use get_transformation_functions that will return list of TransformatioFunction objects. Specific transformation function can be retrieved by get_transformation_function method where user can provide name and version of the transformation function. If only name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 fs . get_transformation_function ( name = \"plus_one\" ) # get transformation function by name and version. fs . get_transformation_function ( name = \"plus_one\" , version = 2 ) To attach transformation function to training dataset provide transformation functions as dict, where key is feature name and value is online transformation function name. Also training dataset must be created from the Query object. Once attached transformation function will be applied on whenever save , insert and get_serving_vector methods are called on training dataset object. Python Attaching transformation functions to the training dataset plus_one_meta = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) fs . create_training_dataset ( name = \"td_demo\" , description = \"Dataset to train the demo model\" , data_format = \"csv\" , transformation_functions = { \"feature_name\" : plus_one_meta } statistics_config = None , version = 1 ) td . save ( join_query ) Scala support Creating and attaching Transformation functions to training datasets are not supported for hsfs scala client. If training dataset with transformation function was created using python client and later insert or getServingVector methods are called on this training dataset from scala client hsfs will throw an exception.","title":"Examples"},{"location":"generated/transformation_functions/#transfromation-function","text":"[source]","title":"Transfromation Function"},{"location":"generated/transformation_functions/#transformationfunction","text":"hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , output_type = None , id = None , type = None , items = None , count = None , href = None , )","title":"TransformationFunction"},{"location":"generated/transformation_functions/#properties","text":"[source]","title":"Properties"},{"location":"generated/transformation_functions/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/transformation_functions/#name","text":"[source]","title":"name"},{"location":"generated/transformation_functions/#output_type","text":"[source]","title":"output_type"},{"location":"generated/transformation_functions/#source_code_content","text":"[source]","title":"source_code_content"},{"location":"generated/transformation_functions/#transformation_fn","text":"[source]","title":"transformation_fn"},{"location":"generated/transformation_functions/#transformer_code","text":"[source]","title":"transformer_code"},{"location":"generated/transformation_functions/#version","text":"","title":"version"},{"location":"generated/transformation_functions/#methods","text":"[source]","title":"Methods"},{"location":"generated/transformation_functions/#delete","text":"TransformationFunction . delete () Delete transformation function from backend. [source]","title":"delete"},{"location":"generated/transformation_functions/#save","text":"TransformationFunction . save () Persist transformation function in backend.","title":"save"},{"location":"generated/transformation_functions/#creation","text":"[source]","title":"Creation"},{"location":"generated/transformation_functions/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object.","title":"create_transformation_function"},{"location":"generated/transformation_functions/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/transformation_functions/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/transformation_functions/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"get_transformation_functions"},{"location":"generated/api/connection_api/","text":"Connection # [source] Connection # hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] cert_folder # [source] host # [source] hostname_verification # [source] port # [source] project # [source] region_name # [source] secrets_store # [source] trust_store_path # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] setup_databricks # Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"Connection"},{"location":"generated/api/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/api/connection_api/#connection_1","text":"hsfs . connection . Connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) A feature store connection object. The connection is project specific, so you can access the project's own feature store but also any feature store which has been shared with the project you connect to. This class provides convenience classmethods accessible from the hsfs -module: Connection factory For convenience, hsfs provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsfs conn = hsfs . connection () Save API Key as File To get started quickly, without saving the Hopsworks API in a secret storage, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to the Hopsworks Feature Store. You can then connect by simply passing the path to the key file when instantiating a connection: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = 'featurestore.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Clients in external clusters need to connect to the Hopsworks Feature Store using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\" and \"featurestore\" scopes to be able to access a feature store. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . engine Optional[str] : Which engine to use, \"spark\" , \"hive\" or \"training\" . Defaults to None , which initializes the engine to Spark if the environment provides Spark, for example on Hopsworks and Databricks, or falls back on Hive if Spark is not available, e.g. on local Python environments or AWS SageMaker. This option allows you to override this behaviour. \"training\" engine is useful when only feature store metadata is needed, for example training dataset location and label information when Hopsworks training experiment is conducted. region_name str : The name of the AWS region in which the required secrets are stored, defaults to \"default\" . secrets_store str : The secrets storage to be used, either \"secretsmanager\" , \"parameterstore\" or \"local\" , defaults to \"parameterstore\" . hostname_verification bool : Whether or not to verify Hopsworks\u2019 certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . cert_folder str : The directory to store retrieved HopsFS certificates, defaults to \"hops\" . Only required when running without a Spark environment. api_key_file Optional[str] : Path to a file containing the API Key, if provided, secrets_store will be ignored, defaults to None . api_key_value Optional[str] : API Key as string, if provided, secrets_store will be ignored , however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None`. Returns Connection . Feature Store connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/api/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/api/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/api/connection_api/#cert_folder","text":"[source]","title":"cert_folder"},{"location":"generated/api/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/api/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/api/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/api/connection_api/#project","text":"[source]","title":"project"},{"location":"generated/api/connection_api/#region_name","text":"[source]","title":"region_name"},{"location":"generated/api/connection_api/#secrets_store","text":"[source]","title":"secrets_store"},{"location":"generated/api/connection_api/#trust_store_path","text":"","title":"trust_store_path"},{"location":"generated/api/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments such as AWS SageMaker. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/api/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments such as AWS SageMaker. Subsequently you can call connect() again to reopen the connection. Example import hsfs conn = hsfs . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/api/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsfs.connection() . [source]","title":"connection"},{"location":"generated/api/connection_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. [source]","title":"get_feature_store"},{"location":"generated/api/connection_api/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rule"},{"location":"generated/api/connection_api/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/api/connection_api/#setup_databricks","text":"Connection . setup_databricks ( host = None , port = 443 , project = None , engine = None , region_name = \"default\" , secrets_store = \"parameterstore\" , hostname_verification = True , trust_store_path = None , cert_folder = \"hops\" , api_key_file = None , api_key_value = None , ) Set up the HopsFS and Hive connector on a Databricks cluster. This method will setup the HopsFS and Hive connectors to connect from a Databricks cluster to a Hopsworks Feature Store instance. It returns a Connection object and will print instructions on how to finalize the setup of the Databricks cluster. See also the Databricks integration guide .","title":"setup_databricks"},{"location":"generated/api/expectation_api/","text":"Expectation # [source] Expectation # hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store. Properties # [source] description # Description of the expectation. [source] features # List of features this expectation is applied to. [source] name # Name of the expectation, unique per feature store (project). [source] rules # List of rules applied to the features of the expectation. Methods # [source] save # Expectation . save () Persist the expectation metadata object to the feature store. Creation # [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object. Retrieval # [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store.","title":"Expectation"},{"location":"generated/api/expectation_api/#expectation","text":"[source]","title":"Expectation"},{"location":"generated/api/expectation_api/#expectation_1","text":"hsfs . expectation . Expectation ( name , features , rules , description = None , featurestore_id = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing an feature validation expectation in the Feature Store.","title":"Expectation"},{"location":"generated/api/expectation_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/expectation_api/#description","text":"Description of the expectation. [source]","title":"description"},{"location":"generated/api/expectation_api/#features","text":"List of features this expectation is applied to. [source]","title":"features"},{"location":"generated/api/expectation_api/#name","text":"Name of the expectation, unique per feature store (project). [source]","title":"name"},{"location":"generated/api/expectation_api/#rules","text":"List of rules applied to the features of the expectation.","title":"rules"},{"location":"generated/api/expectation_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/expectation_api/#save","text":"Expectation . save () Persist the expectation metadata object to the feature store.","title":"save"},{"location":"generated/api/expectation_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/expectation_api/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object.","title":"create_expectation"},{"location":"generated/api/expectation_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/expectation_api/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source]","title":"get_expectations"},{"location":"generated/api/expectation_api/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store.","title":"get_expectation"},{"location":"generated/api/feature_api/","text":"Feature # [source] Feature # hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas. Properties # [source] default_value # Default value of the feature as string, if the feature was appended to the feature group. [source] hudi_precombine_key # Whether the feature is part of the hudi precombine key of the feature group. [source] name # Name of the feature. [source] online_type # Data type of the feature in the online feature store. [source] partition # Whether the feature is part of the partition key of the feature group. [source] primary # Whether the feature is part of the primary key of the feature group. [source] type # Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store. Methods # [source] from_response_json # Feature . from_response_json ( json_dict ) [source] is_complex # Feature . is_complex () Returns true if the feature has a complex type. [source] json # Feature . json () [source] to_dict # Feature . to_dict ()","title":"Feature"},{"location":"generated/api/feature_api/#feature","text":"[source]","title":"Feature"},{"location":"generated/api/feature_api/#feature_1","text":"hsfs . feature . Feature ( name , type = None , description = None , primary = None , partition = None , hudi_precombine_key = None , online_type = None , default_value = None , feature_group_id = None , feature_group = None , ) Metadata object representing a feature in a feature group in the Feature Store. See Training Dataset Feature for the feature representation of training dataset schemas.","title":"Feature"},{"location":"generated/api/feature_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_api/#default_value","text":"Default value of the feature as string, if the feature was appended to the feature group. [source]","title":"default_value"},{"location":"generated/api/feature_api/#hudi_precombine_key","text":"Whether the feature is part of the hudi precombine key of the feature group. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_api/#name","text":"Name of the feature. [source]","title":"name"},{"location":"generated/api/feature_api/#online_type","text":"Data type of the feature in the online feature store. [source]","title":"online_type"},{"location":"generated/api/feature_api/#partition","text":"Whether the feature is part of the partition key of the feature group. [source]","title":"partition"},{"location":"generated/api/feature_api/#primary","text":"Whether the feature is part of the primary key of the feature group. [source]","title":"primary"},{"location":"generated/api/feature_api/#type","text":"Data type of the feature in the offline feature store. Not a Python type This type property is not to be confused with Python types. The type property represents the actual data type of the feature in the feature store.","title":"type"},{"location":"generated/api/feature_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_api/#from_response_json","text":"Feature . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_api/#is_complex","text":"Feature . is_complex () Returns true if the feature has a complex type. [source]","title":"is_complex"},{"location":"generated/api/feature_api/#json","text":"Feature . json () [source]","title":"json"},{"location":"generated/api/feature_api/#to_dict","text":"Feature . to_dict ()","title":"to_dict"},{"location":"generated/api/feature_group_api/","text":"FeatureGroup # [source] FeatureGroup # hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , validation_type = \"NONE\" , expectations = None , online_topic_name = None , ) Creation # [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object. Retrieval # [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] avro_schema # Avro schema representation of the feature group. [source] created # Timestamp when the feature group was created. [source] creator # Username of the creator. [source] description # Description of the feature group contents. [source] expectations_names # The names of expectations attached to this feature group. [source] feature_store_id # [source] feature_store_name # Name of the feature store in which the feature group is located. [source] features # Schema information. [source] hudi_precombine_key # Feature name that is the hudi precombine key. [source] id # Feature group id. [source] location # [source] name # Name of the feature group. [source] online_enabled # Setting if the feature group is available in online storage. [source] partition_key # List of features building the partition key. [source] primary_key # List of features building the primary key. [source] statistics # Get the latest computed statistics for the feature group. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the feature group. [source] time_travel_format # Setting of the feature group time travel format. [source] validation_type # Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source] version # Version number of the feature group. Methods # [source] add_tag # FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] append_features # FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source] attach_expectation # FeatureGroup . attach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] commit_delete_record # FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source] commit_details # FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source] compute_statistics # FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source] delete # FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source] delete_tag # FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] detach_expectation # FeatureGroup . detach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source] filter # FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source] from_response_json # FeatureGroup . from_response_json ( json_dict ) [source] get_complex_features # FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source] get_expectation # FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source] get_expectations # FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source] get_feature # FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source] get_statistics # FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source] get_tag # FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source] insert # FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns FeatureGroup . Updated feature group metadata object. [source] insert_stream # FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to Fals e. timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source] json # FeatureGroup . json () [source] read # FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source] read_changes # FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source] save # FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source] select # FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source] select_all # FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source] select_except # FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source] show # FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source] to_dict # FeatureGroup . to_dict () [source] update_description # FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source] update_from_response_json # FeatureGroup . update_from_response_json ( json_dict ) [source] update_statistics_config # FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source] validate # FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup","text":"[source]","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#featuregroup_1","text":"hsfs . feature_group . FeatureGroup ( name , version , featurestore_id , description = \"\" , partition_key = None , primary_key = None , hudi_precombine_key = None , featurestore_name = None , created = None , creator = None , id = None , features = None , location = None , online_enabled = False , time_travel_format = None , statistics_config = None , validation_type = \"NONE\" , expectations = None , online_topic_name = None , )","title":"FeatureGroup"},{"location":"generated/api/feature_group_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/feature_group_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object.","title":"create_feature_group"},{"location":"generated/api/feature_group_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_group_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_feature_group"},{"location":"generated/api/feature_group_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_group_api/#avro_schema","text":"Avro schema representation of the feature group. [source]","title":"avro_schema"},{"location":"generated/api/feature_group_api/#created","text":"Timestamp when the feature group was created. [source]","title":"created"},{"location":"generated/api/feature_group_api/#creator","text":"Username of the creator. [source]","title":"creator"},{"location":"generated/api/feature_group_api/#description","text":"Description of the feature group contents. [source]","title":"description"},{"location":"generated/api/feature_group_api/#expectations_names","text":"The names of expectations attached to this feature group. [source]","title":"expectations_names"},{"location":"generated/api/feature_group_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/feature_group_api/#feature_store_name","text":"Name of the feature store in which the feature group is located. [source]","title":"feature_store_name"},{"location":"generated/api/feature_group_api/#features","text":"Schema information. [source]","title":"features"},{"location":"generated/api/feature_group_api/#hudi_precombine_key","text":"Feature name that is the hudi precombine key. [source]","title":"hudi_precombine_key"},{"location":"generated/api/feature_group_api/#id","text":"Feature group id. [source]","title":"id"},{"location":"generated/api/feature_group_api/#location","text":"[source]","title":"location"},{"location":"generated/api/feature_group_api/#name","text":"Name of the feature group. [source]","title":"name"},{"location":"generated/api/feature_group_api/#online_enabled","text":"Setting if the feature group is available in online storage. [source]","title":"online_enabled"},{"location":"generated/api/feature_group_api/#partition_key","text":"List of features building the partition key. [source]","title":"partition_key"},{"location":"generated/api/feature_group_api/#primary_key","text":"List of features building the primary key. [source]","title":"primary_key"},{"location":"generated/api/feature_group_api/#statistics","text":"Get the latest computed statistics for the feature group. [source]","title":"statistics"},{"location":"generated/api/feature_group_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the feature group. [source]","title":"statistics_config"},{"location":"generated/api/feature_group_api/#time_travel_format","text":"Setting of the feature group time travel format. [source]","title":"time_travel_format"},{"location":"generated/api/feature_group_api/#validation_type","text":"Validation type, one of \"STRICT\", \"WARNING\", \"ALL\", \"NONE\". [source]","title":"validation_type"},{"location":"generated/api/feature_group_api/#version","text":"Version number of the feature group.","title":"version"},{"location":"generated/api/feature_group_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_group_api/#add_tag","text":"FeatureGroup . add_tag ( name , value ) Attach a tag to a feature group. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/feature_group_api/#append_features","text":"FeatureGroup . append_features ( features ) Append features to the schema of the feature group. It is only possible to append features to a feature group. Removing features is considered a breaking change. Arguments features : Feature or list. A feature object or list thereof to append to the schema of the feature group. Returns FeatureGroup. The updated feature group object. [source]","title":"append_features"},{"location":"generated/api/feature_group_api/#attach_expectation","text":"FeatureGroup . attach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"attach_expectation"},{"location":"generated/api/feature_group_api/#commit_delete_record","text":"FeatureGroup . commit_delete_record ( delete_df , write_options = {}) Drops records present in the provided DataFrame and commits it as update to this Feature group. This method can only be used on time travel enabled feature groups Arguments delete_df pyspark.sql.DataFrame : dataFrame containing records to be deleted. write_options Optional[Dict[Any, Any]] : User provided write options. Defaults to {} . Raises RestAPIError . [source]","title":"commit_delete_record"},{"location":"generated/api/feature_group_api/#commit_details","text":"FeatureGroup . commit_details ( wallclock_time = None , limit = None ) Retrieves commit timeline for this feature group. This method can only be used on time travel enabled feature groups Arguments wallclock_time Optional[str] : Commit details as of specific point in time. Defaults to None . limit Optional[int] : Number of commits to retrieve. Defaults to None . Returns Dict[str, Dict[str, str]] . Dictionary object of commit metadata timeline, where Key is commit id and value is Dict[str, str] with key value pairs of date committed on, number of rows updated, inserted and deleted. Raises RestAPIError . FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"commit_details"},{"location":"generated/api/feature_group_api/#compute_statistics","text":"FeatureGroup . compute_statistics ( wallclock_time = None ) Recompute the statistics for the feature group and save them to the feature store. Statistics are only computed for data in the offline storage of the feature group. Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". Only valid if feature group is time travel enabled. If specified will recompute statistics on feature group as of specific point in time. If not specified then will compute statistics as of most recent time of this feature group. Defaults to None . Returns Statistics . The statistics metadata object. Raises RestAPIError . Unable to persist the statistics. [source]","title":"compute_statistics"},{"location":"generated/api/feature_group_api/#delete","text":"FeatureGroup . delete () Drop the entire feature group along with its feature data. Potentially dangerous operation This operation drops all metadata associated with this version of the feature group and all the feature data in offline and online storage associated with it. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/feature_group_api/#delete_tag","text":"FeatureGroup . delete_tag ( name ) Delete a tag attached to a feature group. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/feature_group_api/#detach_expectation","text":"FeatureGroup . detach_expectation ( expectation ) Get feature group expectations. Gets all expectations if no expectation name is specified. Arguments name : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"detach_expectation"},{"location":"generated/api/feature_group_api/#filter","text":"FeatureGroup . filter ( f ) Apply filter to the feature group. Selects all features and returns the resulting Query with the applied filter. from hsfs.feature import Feature fg . filter ( Feature ( \"weekly_sales\" ) > 1000 ) If you are planning to join the filtered feature group later on with another feature group, make sure to select the filtered feature explicitly from the respective feature group: fg . filter ( fg . feature1 == 1 ) . show ( 10 ) Composite filters require parenthesis: fg . filter (( fg . feature1 == 1 ) | ( fg . feature2 >= 2 )) Arguments f Union[hsfs.constructor.filter.Filter, hsfs.constructor.filter.Logic] : Filter object. Returns Query . The query object with the applied filter. [source]","title":"filter"},{"location":"generated/api/feature_group_api/#from_response_json","text":"FeatureGroup . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_group_api/#get_complex_features","text":"FeatureGroup . get_complex_features () Returns the names of all features with a complex data type in this feature group. [source]","title":"get_complex_features"},{"location":"generated/api/feature_group_api/#get_expectation","text":"FeatureGroup . get_expectation ( name ) Get attached expectation by name for this feature group. Name is unique across a feature store. Arguments name str : The expectation name. Returns Expectation . The expectation metadata object. [source]","title":"get_expectation"},{"location":"generated/api/feature_group_api/#get_expectations","text":"FeatureGroup . get_expectations () Get all feature group expectations. Arguments name : The expectation name. Returns Expectation . A list of expectation metadata objects. [source]","title":"get_expectations"},{"location":"generated/api/feature_group_api/#get_feature","text":"FeatureGroup . get_feature ( name ) Retrieve a Feature object from the schema of the feature group. There are several ways to access features of a feature group: fg . feature1 fg [ \"feature1\" ] fg . get_feature ( \"feature1\" ) Note Attribute access to features works only for non-reserved names. For example features named id or name will not be accessible via fg.name , instead this will return the name of the feature group itself. Fall back on using the get_feature method. Args: name (str): [description] Returns: [type]: [description] [source]","title":"get_feature"},{"location":"generated/api/feature_group_api/#get_statistics","text":"FeatureGroup . get_statistics ( commit_time = None ) Returns the statistics for this feature group at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Statistics object. Raises RestAPIError . [source]","title":"get_statistics"},{"location":"generated/api/feature_group_api/#get_tag","text":"FeatureGroup . get_tag ( name ) Get the tags of a feature group. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/feature_group_api/#get_tags","text":"FeatureGroup . get_tags () Retrieves all tags attached to a feature group. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/feature_group_api/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object. [source]","title":"get_validations"},{"location":"generated/api/feature_group_api/#insert","text":"FeatureGroup . insert ( features , overwrite = False , operation = \"upsert\" , storage = None , write_options = {} ) Insert data from a dataframe into the feature group. Incrementally insert data to a feature group or overwrite all data contained in the feature group. By default, the data is inserted into the offline storage as well as the online storage if the feature group is online_enabled=True . To insert only into the online storage, set storage=\"online\" , or oppositely storage=\"offline\" . The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. If statistics are enabled, statistics are recomputed for the entire feature group. If feature group's time travel format is HUDI then operation argument can be either insert or upsert . Upsert new feature data with time travel format HUDI : fs = conn . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) upsert_df = ... fg . insert ( upsert_df ) Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : DataFrame, RDD, Ndarray, list. Features to be saved. overwrite Optional[bool] : Drop all data in the feature group before inserting new data. This does not affect metadata, defaults to False. operation Optional[str] : Apache Hudi operation type \"insert\" or \"upsert\" . Defaults to \"upsert\" . storage Optional[str] : Overwrite default behaviour, write to offline storage only with \"offline\" or online only with \"online\" , defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns FeatureGroup . Updated feature group metadata object. [source]","title":"insert"},{"location":"generated/api/feature_group_api/#insert_stream","text":"FeatureGroup . insert_stream ( features , query_name = None , output_mode = \"append\" , await_termination = False , timeout = None , write_options = {}, ) Ingest a Spark Structured Streaming Dataframe to the online feature store. This method creates a long running Spark Streaming Query, you can control the termination of the query through the arguments. It is possible to stop the returned query with the .stop() and check its status with .isActive . To get a list of all active queries, use: sqm = spark . streams # get the list of active streaming queries [ q . name for q in sqm . active ] Arguments features pyspark.sql.DataFrame : Features in Streaming Dataframe to be saved. query_name Optional[str] : It is possible to optionally specify a name for the query to make it easier to recognise in the Spark UI. Defaults to None . output_mode Optional[str] : Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. (1) \"append\" : Only the new rows in the streaming DataFrame/Dataset will be written to the sink. (2) \"complete\" : All the rows in the streaming DataFrame/Dataset will be written to the sink every time there is some update. (3) \"update\" : only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates. If the query doesn\u2019t contain aggregations, it will be equivalent to append mode. Defaults to \"append\" . await_termination Optional[bool] : Waits for the termination of this query, either by query.stop() or by an exception. If the query has terminated with an exception, then the exception will be thrown. If timeout is set, it returns whether the query has terminated or not within the timeout seconds. Defaults to Fals e. timeout Optional[int] : Only relevant in combination with await_termination=True . Defaults to None . write_options Optional[Dict[Any, Any]] : Additional write options for Spark as key-value pairs. Defaults to {} . Returns StreamingQuery : Spark Structured Streaming Query object. [source]","title":"insert_stream"},{"location":"generated/api/feature_group_api/#json","text":"FeatureGroup . json () [source]","title":"json"},{"location":"generated/api/feature_group_api/#read","text":"FeatureGroup . read ( wallclock_time = None , online = False , dataframe_type = \"default\" , read_options = {}) Read the feature group into a dataframe. Reads the feature group by default from the offline storage as Spark DataFrame on Hopsworks and Databricks, and as Pandas dataframe on AWS Sagemaker and pure Python environments. Set online to True to read from the online storage, or change dataframe_type to read as a different format. Read feature group as of latest state: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read () Read feature group as of specific point in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read ( \"2020-10-20 07:34:11\" ) Arguments wallclock_time Optional[str] : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". If Specified will retrieve feature group as of specific point in time. If not specified will return as of most recent time. Defaults to None . online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . dataframe_type Optional[str] : str, optional. Possible values are \"default\" , \"spark\" , \"pandas\" , \"numpy\" or \"python\" , defaults to \"default\" . read_options Optional[dict] : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data. pyspark.DataFrame . A Spark DataFrame. pandas.DataFrame . A Pandas DataFrame. numpy.ndarray . A two-dimensional Numpy array. list . A two-dimensional Python list. Raises RestAPIError . No data is available for feature group with this commit date, If time travel enabled. [source]","title":"read"},{"location":"generated/api/feature_group_api/#read_changes","text":"FeatureGroup . read_changes ( start_wallclock_time , end_wallclock_time , read_options = {}) Reads updates of this feature that occurred between specified points in time. This function only works on feature groups with HUDI time travel format. Reading commits incrementally between specified points in time: fs = connection . get_feature_store (); fg = fs . get_feature_group ( \"example_feature_group\" , 1 ) fg . read_changes ( \"2020-10-20 07:31:38\" , \"2020-10-20 07:34:11\" ) . show () Arguments start_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". end_wallclock_time str : Date string in the format of \"YYYYMMDD\" or \"YYYYMMDDhhmmss\". read_options Optional[dict] : User provided read options. Defaults to {} . Returns DataFrame . The spark dataframe containing the incremental changes of feature data. Raises RestAPIError . No data is available for feature group with this commit date. FeatureStoreException . If the feature group does not have HUDI time travel format [source]","title":"read_changes"},{"location":"generated/api/feature_group_api/#save","text":"FeatureGroup . save ( features , write_options = {}) Persist the metadata and materialize the feature group to the feature store. Calling save creates the metadata for the feature group in the feature store and writes the specified features dataframe as feature group to the online/offline feature store as specified. By default, this writes the feature group to the offline storage, and if online_enabled for the feature group, also to the online feature store. The features dataframe can be a Spark DataFrame or RDD, a Pandas DataFrame, or a two-dimensional Numpy array or a two-dimensional Python nested list. Arguments features Union[pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Query, DataFrame, RDD, Ndarray, list. Features to be saved. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to write data into the feature group. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. key mode instruct the ingestion job on how to deal with corrupted data. Values are PERMISSIVE, DROPMALFORMED or FAILFAST. Default FAILFAST. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to ingest the feature group data. Raises RestAPIError . Unable to create feature group. [source]","title":"save"},{"location":"generated/api/feature_group_api/#select","text":"FeatureGroup . select ( features = []) Select a subset of features of the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to []. Returns Query : A query object with the selected features of the feature group. [source]","title":"select"},{"location":"generated/api/feature_group_api/#select_all","text":"FeatureGroup . select_all () Select all features in the feature group and return a query object. The query can be used to construct joins of feature groups or create a training dataset immediately. Returns Query . A query object with all features of the feature group. [source]","title":"select_all"},{"location":"generated/api/feature_group_api/#select_except","text":"FeatureGroup . select_except ( features = []) Select all features of the feature group except a few and return a query object. The query can be used to construct joins of feature groups or create a training dataset with a subset of features of the feature group. Arguments features List[Union[str, hsfs.feature.Feature]] : list, optional. A list of Feature objects or feature names as strings to be selected, defaults to [], selecting all features. Returns Query : A query object with the selected features of the feature group. [source]","title":"select_except"},{"location":"generated/api/feature_group_api/#show","text":"FeatureGroup . show ( n , online = False ) Show the first n rows of the feature group. Arguments n int : int. Number of rows to show. online Optional[bool] : bool, optional. If True read from online feature store, defaults to False . [source]","title":"show"},{"location":"generated/api/feature_group_api/#to_dict","text":"FeatureGroup . to_dict () [source]","title":"to_dict"},{"location":"generated/api/feature_group_api/#update_description","text":"FeatureGroup . update_description ( description ) Update the description of the feature gorup. Arguments description str : str. New description string. Returns FeatureGroup . The updated feature group object. [source]","title":"update_description"},{"location":"generated/api/feature_group_api/#update_from_response_json","text":"FeatureGroup . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/feature_group_api/#update_statistics_config","text":"FeatureGroup . update_statistics_config () Update the statistics configuration of the feature group. Change the statistics_config object and persist the changes by calling this method. Returns FeatureGroup . The updated metadata object of the feature group. Raises RestAPIError . [source]","title":"update_statistics_config"},{"location":"generated/api/feature_group_api/#validate","text":"FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/api/feature_store_api/","text":"Feature Store # [source] FeatureStore # hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , ) Retrieval # [source] get_feature_store # Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on. Properties # [source] description # Description of the feature store. [source] hive_endpoint # Hive endpoint for the offline feature store. [source] id # Id of the feature store. [source] mysql_server_endpoint # MySQL server endpoint for the online feature store. [source] name # Name of the feature store. [source] offline_featurestore_name # Name of the offline feature store database. [source] online_enabled # Indicator whether online feature store is enabled. [source] online_featurestore_name # Name of the online feature store database. [source] project_id # Id of the project in which the feature store is located. [source] project_name # Name of the project in which the feature store is located. Methods # [source] create_expectation # FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object. [source] create_feature_group # FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object. [source] create_on_demand_feature_group # FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source] from_response_json # FeatureStore . from_response_json ( json_dict ) [source] get_expectation # FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source] get_expectations # FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source] get_feature_group # FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_feature_groups # FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_group # FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_on_demand_feature_groups # FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_training_datasets # FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source] sql # FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Args: query (str): The SQL query to execute. dataframe_type (Optional[str], optional): The type of the returned dataframe. Defaults to \"default\" which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online (Optional[bool], optional): Set to true to execute the query against the online feature store. Defaults to False. read_options (Optional[dict], optional): Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns: DataFrame : DataFrame depending on the chosen type.","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#feature-store","text":"[source]","title":"Feature Store"},{"location":"generated/api/feature_store_api/#featurestore","text":"hsfs . feature_store . FeatureStore ( featurestore_id , featurestore_name , created , hdfs_store_path , project_name , project_id , featurestore_description , inode_id , offline_featurestore_name , hive_endpoint , online_enabled , num_feature_groups = None , num_training_datasets = None , num_storage_connectors = None , online_featurestore_name = None , mysql_server_endpoint = None , online_featurestore_size = None , )","title":"FeatureStore"},{"location":"generated/api/feature_store_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/feature_store_api/#get_feature_store","text":"Connection . get_feature_store ( name = None ) Get a reference to a feature store to perform operations on. Defaulting to the project's default feature store. Shared feature stores can be retrieved by passing the name argument. Arguments name str : The name of the feature store, defaults to None . Returns FeatureStore . A feature store handle object to perform operations on.","title":"get_feature_store"},{"location":"generated/api/feature_store_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/feature_store_api/#description","text":"Description of the feature store. [source]","title":"description"},{"location":"generated/api/feature_store_api/#hive_endpoint","text":"Hive endpoint for the offline feature store. [source]","title":"hive_endpoint"},{"location":"generated/api/feature_store_api/#id","text":"Id of the feature store. [source]","title":"id"},{"location":"generated/api/feature_store_api/#mysql_server_endpoint","text":"MySQL server endpoint for the online feature store. [source]","title":"mysql_server_endpoint"},{"location":"generated/api/feature_store_api/#name","text":"Name of the feature store. [source]","title":"name"},{"location":"generated/api/feature_store_api/#offline_featurestore_name","text":"Name of the offline feature store database. [source]","title":"offline_featurestore_name"},{"location":"generated/api/feature_store_api/#online_enabled","text":"Indicator whether online feature store is enabled. [source]","title":"online_enabled"},{"location":"generated/api/feature_store_api/#online_featurestore_name","text":"Name of the online feature store database. [source]","title":"online_featurestore_name"},{"location":"generated/api/feature_store_api/#project_id","text":"Id of the project in which the feature store is located. [source]","title":"project_id"},{"location":"generated/api/feature_store_api/#project_name","text":"Name of the project in which the feature store is located.","title":"project_name"},{"location":"generated/api/feature_store_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/feature_store_api/#create_expectation","text":"FeatureStore . create_expectation ( name , description = \"\" , features = [], rules = []) Create an expectation metadata object. Lazy This method is lazy and does not persist the expectation in the feature store on its own. To materialize the expectation and save call the save() method of the expectation metadata object. Arguments name str : Name of the training dataset to create. description Optional[str] : A string describing the expectation that can describe its business logic and applications within the feature store. features Optional[List[str]] : The features this expectation is applied on. rules Optional[List[hsfs.rule.Rule]] : The validation rules this expectation will apply to the its features. Returns: Expectation : The expectation metadata object. [source]","title":"create_expectation"},{"location":"generated/api/feature_store_api/#create_feature_group","text":"FeatureStore . create_feature_group ( name , version = None , description = \"\" , online_enabled = False , time_travel_format = \"HUDI\" , partition_key = [], primary_key = [], hudi_precombine_key = None , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a feature group metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To persist the feature group and save feature data along the metadata in the feature store, call the save() method with a DataFrame. Arguments name str : Name of the feature group to create. version Optional[int] : Version of the feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . online_enabled Optional[bool] : Define whether the feature group should be made available also in the online feature store for low latency access, defaults to False . time_travel_format Optional[str] : Format used for time travel, defaults to \"HUDI\" . partition_key Optional[List[str]] : A list of feature names to be used as partition key when writing the feature data to the offline storage, defaults to empty list [] . primary_key Optional[List[str]] : A list of feature names to be used as primary key for the feature group. This primary key can be a composite key of multiple features and will be used as joining key, if not specified otherwise. Defaults to empty list [] , and the first column of the DataFrame will be used as primary key. hudi_precombine_key Optional[str] : A feature name to be used as a precombine key for the \"HUDI\" feature group. Defaults to None . If feature group has time travel format \"HUDI\" and hudi precombine key was not specified then the first primary key of the feature group will be used as hudi precombine key. features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame provided in the save method. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns FeatureGroup . The feature group metadata object. [source]","title":"create_feature_group"},{"location":"generated/api/feature_store_api/#create_on_demand_feature_group","text":"FeatureStore . create_on_demand_feature_group ( name , storage_connector , query = None , data_format = None , path = \"\" , options = {}, version = None , description = \"\" , features = [], statistics_config = None , validation_type = \"NONE\" , expectations = [], ) Create a on-demand feature group metadata object. Lazy This method is lazy and does not persist any metadata in the feature store on its own. To persist the feature group metadata in the feature store, call the save() method. Arguments name str : Name of the on-demand feature group to create. query Optional[str] : A string containing a SQL query valid for the target data source. the query will be used to pull data from the data sources when the feature group is used. data_format Optional[str] : If the on-demand feature groups refers to a directory with data, the data format to use when reading it path Optional[str] : The location within the scope of the storage connector, from where to read the data for the on-demand feature group storage_connector hsfs.StorageConnector : the storage connector to use to establish connectivity with the data source. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will create the feature group with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the on-demand feature group to improve discoverability for Data Scientists, defaults to empty string \"\" . features Optional[List[hsfs.feature.Feature]] : Optionally, define the schema of the on-demand feature group manually as a list of Feature objects. Defaults to empty list [] and will use the schema information of the DataFrame resulting by executing the provided query against the data source. statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this on-demand feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. validation_type Optional[str] : Optionally, set the validation type to one of \"NONE\", \"STRICT\", \"WARNING\", \"ALL\". Determines the mode in which data validation is applied on ingested or already existing feature group data. expectations Optional[List[hsfs.expectation.Expectation]] : Optionally, a list of expectations to be attached to the feature group. The expectations list contains Expectation metadata objects which can be retrieved with the get_expectation() and get_expectations() functions. Returns OnDemandFeatureGroup . The on-demand feature group metadata object. [source]","title":"create_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. [source]","title":"create_training_dataset"},{"location":"generated/api/feature_store_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"create_transformation_function"},{"location":"generated/api/feature_store_api/#from_response_json","text":"FeatureStore . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/feature_store_api/#get_expectation","text":"FeatureStore . get_expectation ( name ) Get an expectation entity from the feature store. Getting an expectation from the Feature Store means getting its metadata handle so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Arguments name str : Name of the training dataset to get. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectation from the feature store. [source]","title":"get_expectation"},{"location":"generated/api/feature_store_api/#get_expectations","text":"FeatureStore . get_expectations () Get all expectation entities from the feature store. Getting expectations from the Feature Store means getting their metadata handles so you can subsequently add features and/or rules and save it which will overwrite the previous instance. Returns Expectation : The expectation metadata object. Raises RestAPIError : If unable to retrieve the expectations from the feature store. [source]","title":"get_expectations"},{"location":"generated/api/feature_store_api/#get_feature_group","text":"FeatureStore . get_feature_group ( name , version = None ) Get a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. version Optional[int] : Version of the feature group to retrieve, defaults to None and will return the version=1 . Returns FeatureGroup : The feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_group"},{"location":"generated/api/feature_store_api/#get_feature_groups","text":"FeatureStore . get_feature_groups ( name ) Get a list of all versions of a feature group entity from the feature store. Getting a feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the feature group to get. Returns FeatureGroup : List of feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_feature_groups"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_group","text":"FeatureStore . get_on_demand_feature_group ( name , version = None ) Get a on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. version Optional[int] : Version of the on-demand feature group to retrieve, defaults to None and will return the version=1 . Returns OnDemandFeatureGroup : The on-demand feature group metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_group"},{"location":"generated/api/feature_store_api/#get_on_demand_feature_groups","text":"FeatureStore . get_on_demand_feature_groups ( name ) Get a list of all versions of an on-demand feature group entity from the feature store. Getting a on-demand feature group from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame or use the Query -API to perform joins between feature groups. Arguments name str : Name of the on-demand feature group to get. Returns OnDemandFeatureGroup : List of on-demand feature group metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_on_demand_feature_groups"},{"location":"generated/api/feature_store_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. [source]","title":"get_online_storage_connector"},{"location":"generated/api/feature_store_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/feature_store_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_dataset"},{"location":"generated/api/feature_store_api/#get_training_datasets","text":"FeatureStore . get_training_datasets ( name ) Get a list of all versions of a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. Returns TrainingDataset : List of training dataset metadata objects. Raises RestAPIError : If unable to retrieve feature group from the feature store. [source]","title":"get_training_datasets"},{"location":"generated/api/feature_store_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/feature_store_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances. [source]","title":"get_transformation_functions"},{"location":"generated/api/feature_store_api/#sql","text":"FeatureStore . sql ( query , dataframe_type = \"default\" , online = False , read_options = {}) Execute SQL command on the offline or online feature store database Args: query (str): The SQL query to execute. dataframe_type (Optional[str], optional): The type of the returned dataframe. Defaults to \"default\" which maps to Spark dataframe for the Spark Engine and Pandas dataframe for the Hive engine. online (Optional[bool], optional): Set to true to execute the query against the online feature store. Defaults to False. read_options (Optional[dict], optional): Additional options to pass to the execution engine. Defaults to {}. If running queries on the online feature store, users can provide an entry {'external': True} , this instructs the library to use the host parameter in the hsfs.connection() to establish the connection to the online feature store. If not set, or set to False, the online feature store storage connector is used which relies on the private ip. Returns: DataFrame : DataFrame depending on the chosen type.","title":"sql"},{"location":"generated/api/job_configuration/","text":"[source] JobConfiguration # hsfs . core . job_configuration . JobConfiguration ( am_memory = 1024 , am_cores = 1 , executor_memory = 2048 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , )","title":"Job configuration"},{"location":"generated/api/job_configuration/#jobconfiguration","text":"hsfs . core . job_configuration . JobConfiguration ( am_memory = 1024 , am_cores = 1 , executor_memory = 2048 , executor_cores = 1 , executor_instances = 1 , dynamic_allocation = True , dynamic_min_executors = 1 , dynamic_max_executors = 2 , )","title":"JobConfiguration"},{"location":"generated/api/rule_api/","text":"Rule # [source] Rule # hsfs . rule . Rule ( name , level , min = None , max = None , value = None , pattern = None , accepted_type = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only. Properties # [source] accepted_type # Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source] legal_values # List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source] level # Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source] max # The upper bound of the value range this feature should fall into. [source] min # The lower bound of the value range this feature should fall into. [source] name # Name of the rule as found in rule definitions. [source] pattern # Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule. [source] value # The upper bound of the value range this feature should fall into.","title":"Rule"},{"location":"generated/api/rule_api/#rule","text":"[source]","title":"Rule"},{"location":"generated/api/rule_api/#rule_1","text":"hsfs . rule . Rule ( name , level , min = None , max = None , value = None , pattern = None , accepted_type = None , legal_values = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. This class is made for hsfs internal use only.","title":"Rule"},{"location":"generated/api/rule_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/rule_api/#accepted_type","text":"Data type accepted for a feature. Applicable only to the HAS_DATATYPE rule. Accepted types are: Null, Fractional, Integral, Boolean, String [source]","title":"accepted_type"},{"location":"generated/api/rule_api/#legal_values","text":"List of legal values a feature should be found int. feature.Applicable only to IS_CONTAINED_IN rule. [source]","title":"legal_values"},{"location":"generated/api/rule_api/#level","text":"Severity level of a rule, one of \"WARNING\" or \"ERROR\". [source]","title":"level"},{"location":"generated/api/rule_api/#max","text":"The upper bound of the value range this feature should fall into. [source]","title":"max"},{"location":"generated/api/rule_api/#min","text":"The lower bound of the value range this feature should fall into. [source]","title":"min"},{"location":"generated/api/rule_api/#name","text":"Name of the rule as found in rule definitions. [source]","title":"name"},{"location":"generated/api/rule_api/#pattern","text":"Pattern to check for a feature's pattern compliance. Applicable only to the HAS_PATTERN rule. [source]","title":"pattern"},{"location":"generated/api/rule_api/#value","text":"The upper bound of the value range this feature should fall into.","title":"value"},{"location":"generated/api/rule_definition_api/","text":"Rule Definition # [source] RuleDefinition # hsfs . ruledefinition . RuleDefinition ( name , predicate , accepted_type , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified. Properties # [source] accepted_type # The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source] description # [source] feature_type # The type of the feature, one of \"Numerical\", \"Categorical\". [source] name # Name of the rule definition. Unique across all features stores. [source] predicate # Predicate of the rule definition, one of \"VALUE\", \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\". Retrieval # [source] get_rules # Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source] get_rule # Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation.","title":"Rule Definiton"},{"location":"generated/api/rule_definition_api/#rule-definition","text":"[source]","title":"Rule Definition"},{"location":"generated/api/rule_definition_api/#ruledefinition","text":"hsfs . ruledefinition . RuleDefinition ( name , predicate , accepted_type , feature_type = None , description = None , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation rule that is used by feature group expectations. The set of rule definitions, for example \"has max\", \"has avg\" is provided by hsfs and cannot be modified.","title":"RuleDefinition"},{"location":"generated/api/rule_definition_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/rule_definition_api/#accepted_type","text":"The type of the feature, one of \"Null\", \"Fractional\", \"Integral\", \"Boolean\", \"String\". [source]","title":"accepted_type"},{"location":"generated/api/rule_definition_api/#description","text":"[source]","title":"description"},{"location":"generated/api/rule_definition_api/#feature_type","text":"The type of the feature, one of \"Numerical\", \"Categorical\". [source]","title":"feature_type"},{"location":"generated/api/rule_definition_api/#name","text":"Name of the rule definition. Unique across all features stores. [source]","title":"name"},{"location":"generated/api/rule_definition_api/#predicate","text":"Predicate of the rule definition, one of \"VALUE\", \"LEGAL_VALUES\", \"ACCEPTED_TYPE\", \"PATTERN\".","title":"predicate"},{"location":"generated/api/rule_definition_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/rule_definition_api/#get_rules","text":"Connection . get_rules () Get a rule with a certain name or all rules available for data validation. [source]","title":"get_rules"},{"location":"generated/api/rule_definition_api/#get_rule","text":"Connection . get_rule ( name ) Get a rule with a certain name or all rules available for data validation.","title":"get_rule"},{"location":"generated/api/statistics_config_api/","text":"StatisticsConfig # [source] StatisticsConfig # hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] ) Properties # [source] columns # Specify a subset of columns to compute statistics for. [source] correlations # Enable correlations as an additional statistic to be computed for each feature pair. [source] enabled # Enable statistics, by default this computes only descriptive statistics. [source] histograms # Enable histograms as an additional statistic to be computed for each feature.","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig","text":"[source]","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#statisticsconfig_1","text":"hsfs . statistics_config . StatisticsConfig ( enabled = True , correlations = False , histograms = False , columns = [] )","title":"StatisticsConfig"},{"location":"generated/api/statistics_config_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/statistics_config_api/#columns","text":"Specify a subset of columns to compute statistics for. [source]","title":"columns"},{"location":"generated/api/statistics_config_api/#correlations","text":"Enable correlations as an additional statistic to be computed for each feature pair. [source]","title":"correlations"},{"location":"generated/api/statistics_config_api/#enabled","text":"Enable statistics, by default this computes only descriptive statistics. [source]","title":"enabled"},{"location":"generated/api/statistics_config_api/#histograms","text":"Enable histograms as an additional statistic to be computed for each feature.","title":"histograms"},{"location":"generated/api/storage_connector_api/","text":"Storage Connector # Retrieval # [source] get_storage_connector # FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source] get_online_storage_connector # FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store. HopsFS # Properties # [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] spark_options # HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # HopsFSConnector . to_dict () [source] update_from_response_json # HopsFSConnector . update_from_response_json ( json_dict ) JDBC # Properties # [source] arguments # Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source] connection_string # JDBC connection string. [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. Methods # [source] read # JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] spark_options # JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # JdbcConnector . to_dict () [source] update_from_response_json # JdbcConnector . update_from_response_json ( json_dict ) S3 # Properties # [source] access_key # Access key. [source] bucket # Return the bucket for S3 connectors. [source] description # User provided description of the storage connector. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. S3) - return the path of the connector [source] secret_key # Secret key. [source] server_encryption_algorithm # Encryption algorithm if server-side S3 bucket encryption is enabled. [source] server_encryption_key # Encryption key if server-side S3 bucket encryption is enabled. [source] session_token # Session token. Methods # [source] prepare_spark # S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] refetch # S3Connector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # S3Connector . to_dict () [source] update_from_response_json # S3Connector . update_from_response_json ( json_dict ) Redshift # Properties # [source] arguments # Additional JDBC, REDSHIFT, or Snowflake arguments. [source] auto_create # Database username for redshift cluster. [source] cluster_identifier # Cluster identifier for redshift cluster. [source] database_driver # Database endpoint for redshift cluster. [source] database_endpoint # Database endpoint for redshift cluster. [source] database_group # Database username for redshift cluster. [source] database_name # Database name for redshift cluster. [source] database_password # Database password for redshift cluster. [source] database_port # Database port for redshift cluster. [source] database_user_name # Database username for redshift cluster. [source] description # User provided description of the storage connector. [source] expiration # Cluster temporary credential expiration time. [source] iam_role # IAM role. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] table_name # Table name for redshift cluster. Methods # [source] read # RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] refetch # RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source] spark_options # RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # RedshiftConnector . to_dict () [source] update_from_response_json # RedshiftConnector . update_from_response_json ( json_dict ) Azure Data Lake Storage # Properties # [source] account_name # Account name of the ADLS storage connector [source] application_id # Application ID of the ADLS storage connector [source] container_name # Container name of the ADLS storage connector [source] description # User provided description of the storage connector. [source] directory_id # Directory ID of the ADLS storage connector [source] generation # Generation of the ADLS storage connector [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] path # If the connector refers to a path (e.g. ADLS) - return the path of the connector [source] service_credential # Service credential of the ADLS storage connector Methods # [source] prepare_spark # AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source] read # AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source] spark_options # AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # AdlsConnector . to_dict () [source] update_from_response_json # AdlsConnector . update_from_response_json ( json_dict ) Snowflake # Properties # [source] account # Account of the Snowflake storage connector [source] database # Database of the Snowflake storage connector [source] description # User provided description of the storage connector. [source] id # Id of the storage connector uniquely identifying it in the Feature store. [source] name # Name of the storage connector. [source] options # Additional options for the Snowflake storage connector [source] password # Password of the Snowflake storage connector [source] role # Role of the Snowflake storage connector [source] schema # Schema of the Snowflake storage connector [source] table # Table of the Snowflake storage connector [source] token # OAuth token of the Snowflake storage connector [source] url # URL of the Snowflake storage connector [source] user # User of the Snowflake storage connector [source] warehouse # Warehouse of the Snowflake storage connector Methods # [source] read # SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source] snowflake_connector_options # SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source] spark_options # SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source] to_dict # SnowflakeConnector . to_dict () [source] update_from_response_json # SnowflakeConnector . update_from_response_json ( json_dict )","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#storage-connector","text":"","title":"Storage Connector"},{"location":"generated/api/storage_connector_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/storage_connector_api/#get_storage_connector","text":"FeatureStore . get_storage_connector ( name ) Get a previously created storage connector from the feature store. Storage connectors encapsulate all information needed for the execution engine to read and write to specific storage. This storage can be S3, a JDBC compliant database or the distributed filesystem HOPSFS. If you want to connect to the online feature store, see the get_online_storage_connector method to get the JDBC connector for the Online Feature Store. Getting a Storage Connector sc = fs . get_storage_connector ( \"demo_fs_meb10000_Training_Datasets\" ) td = fs . create_training_dataset ( ... , storage_connector = sc , ... ) Arguments name str : Name of the storage connector to retrieve. Returns StorageConnector . Storage connector object. [source]","title":"get_storage_connector"},{"location":"generated/api/storage_connector_api/#get_online_storage_connector","text":"FeatureStore . get_online_storage_connector () Get the storage connector for the Online Feature Store of the respective project's feature store. The returned storage connector depends on the project that you are connected to. Returns StorageConnector . JDBC storage connector to the Online Feature Store.","title":"get_online_storage_connector"},{"location":"generated/api/storage_connector_api/#hopsfs","text":"","title":"HopsFS"},{"location":"generated/api/storage_connector_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#description","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read","text":"HopsFSConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#spark_options","text":"HopsFSConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict","text":"HopsFSConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json","text":"HopsFSConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#jdbc","text":"","title":"JDBC"},{"location":"generated/api/storage_connector_api/#properties_1","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments","text":"Additional JDBC arguments. When running hsfs with PySpark/Spark in Hopsworks, the driver is automatically provided in the classpath but you need to set the driver argument to com.mysql.cj.jdbc.Driver when creating the Storage Connector [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#connection_string","text":"JDBC connection string. [source]","title":"connection_string"},{"location":"generated/api/storage_connector_api/#description_1","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_1","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_1","text":"Name of the storage connector.","title":"name"},{"location":"generated/api/storage_connector_api/#methods_1","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_1","text":"JdbcConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#spark_options_1","text":"JdbcConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_1","text":"JdbcConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_1","text":"JdbcConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#s3","text":"","title":"S3"},{"location":"generated/api/storage_connector_api/#properties_2","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#access_key","text":"Access key. [source]","title":"access_key"},{"location":"generated/api/storage_connector_api/#bucket","text":"Return the bucket for S3 connectors. [source]","title":"bucket"},{"location":"generated/api/storage_connector_api/#description_2","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#iam_role","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_2","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_2","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path","text":"If the connector refers to a path (e.g. S3) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#secret_key","text":"Secret key. [source]","title":"secret_key"},{"location":"generated/api/storage_connector_api/#server_encryption_algorithm","text":"Encryption algorithm if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_algorithm"},{"location":"generated/api/storage_connector_api/#server_encryption_key","text":"Encryption key if server-side S3 bucket encryption is enabled. [source]","title":"server_encryption_key"},{"location":"generated/api/storage_connector_api/#session_token","text":"Session token.","title":"session_token"},{"location":"generated/api/storage_connector_api/#methods_2","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark","text":"S3Connector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"s3a://[bucket]/path\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"s3a://[bucket]/path\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_2","text":"S3Connector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch","text":"S3Connector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_2","text":"S3Connector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_2","text":"S3Connector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_2","text":"S3Connector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#redshift","text":"","title":"Redshift"},{"location":"generated/api/storage_connector_api/#properties_3","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#arguments_1","text":"Additional JDBC, REDSHIFT, or Snowflake arguments. [source]","title":"arguments"},{"location":"generated/api/storage_connector_api/#auto_create","text":"Database username for redshift cluster. [source]","title":"auto_create"},{"location":"generated/api/storage_connector_api/#cluster_identifier","text":"Cluster identifier for redshift cluster. [source]","title":"cluster_identifier"},{"location":"generated/api/storage_connector_api/#database_driver","text":"Database endpoint for redshift cluster. [source]","title":"database_driver"},{"location":"generated/api/storage_connector_api/#database_endpoint","text":"Database endpoint for redshift cluster. [source]","title":"database_endpoint"},{"location":"generated/api/storage_connector_api/#database_group","text":"Database username for redshift cluster. [source]","title":"database_group"},{"location":"generated/api/storage_connector_api/#database_name","text":"Database name for redshift cluster. [source]","title":"database_name"},{"location":"generated/api/storage_connector_api/#database_password","text":"Database password for redshift cluster. [source]","title":"database_password"},{"location":"generated/api/storage_connector_api/#database_port","text":"Database port for redshift cluster. [source]","title":"database_port"},{"location":"generated/api/storage_connector_api/#database_user_name","text":"Database username for redshift cluster. [source]","title":"database_user_name"},{"location":"generated/api/storage_connector_api/#description_3","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#expiration","text":"Cluster temporary credential expiration time. [source]","title":"expiration"},{"location":"generated/api/storage_connector_api/#iam_role_1","text":"IAM role. [source]","title":"iam_role"},{"location":"generated/api/storage_connector_api/#id_3","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_3","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#table_name","text":"Table name for redshift cluster.","title":"table_name"},{"location":"generated/api/storage_connector_api/#methods_3","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_3","text":"RedshiftConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#refetch_1","text":"RedshiftConnector . refetch () Refetch storage connector in order to retrieve updated temporary credentials. [source]","title":"refetch"},{"location":"generated/api/storage_connector_api/#spark_options_3","text":"RedshiftConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_3","text":"RedshiftConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_3","text":"RedshiftConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#azure-data-lake-storage","text":"","title":"Azure Data Lake Storage"},{"location":"generated/api/storage_connector_api/#properties_4","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account_name","text":"Account name of the ADLS storage connector [source]","title":"account_name"},{"location":"generated/api/storage_connector_api/#application_id","text":"Application ID of the ADLS storage connector [source]","title":"application_id"},{"location":"generated/api/storage_connector_api/#container_name","text":"Container name of the ADLS storage connector [source]","title":"container_name"},{"location":"generated/api/storage_connector_api/#description_4","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#directory_id","text":"Directory ID of the ADLS storage connector [source]","title":"directory_id"},{"location":"generated/api/storage_connector_api/#generation","text":"Generation of the ADLS storage connector [source]","title":"generation"},{"location":"generated/api/storage_connector_api/#id_4","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_4","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#path_1","text":"If the connector refers to a path (e.g. ADLS) - return the path of the connector [source]","title":"path"},{"location":"generated/api/storage_connector_api/#service_credential","text":"Service credential of the ADLS storage connector","title":"service_credential"},{"location":"generated/api/storage_connector_api/#methods_4","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#prepare_spark_1","text":"AdlsConnector . prepare_spark ( path = None ) Prepare Spark to use this Storage Connector. conn . prepare_spark () spark . read . format ( \"json\" ) . load ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" ) # or spark . read . format ( \"json\" ) . load ( conn . prepare_spark ( \"abfss://[container-name]@[account_name].dfs.core.windows.net/[path]\" )) Arguments path Optional[str] : Path to prepare for reading from cloud storage. Defaults to None . [source]","title":"prepare_spark"},{"location":"generated/api/storage_connector_api/#read_4","text":"AdlsConnector . read ( query = None , data_format = None , options = {}, path = None ) Reads a query or a path into a dataframe using the storage connector. Note, paths are only supported for object stores like S3, HopsFS and ADLS, while queries are meant for JDBC or databases like Redshift and Snowflake. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#spark_options_4","text":"AdlsConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_4","text":"AdlsConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_4","text":"AdlsConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/storage_connector_api/#snowflake","text":"","title":"Snowflake"},{"location":"generated/api/storage_connector_api/#properties_5","text":"[source]","title":"Properties"},{"location":"generated/api/storage_connector_api/#account","text":"Account of the Snowflake storage connector [source]","title":"account"},{"location":"generated/api/storage_connector_api/#database","text":"Database of the Snowflake storage connector [source]","title":"database"},{"location":"generated/api/storage_connector_api/#description_5","text":"User provided description of the storage connector. [source]","title":"description"},{"location":"generated/api/storage_connector_api/#id_5","text":"Id of the storage connector uniquely identifying it in the Feature store. [source]","title":"id"},{"location":"generated/api/storage_connector_api/#name_5","text":"Name of the storage connector. [source]","title":"name"},{"location":"generated/api/storage_connector_api/#options","text":"Additional options for the Snowflake storage connector [source]","title":"options"},{"location":"generated/api/storage_connector_api/#password","text":"Password of the Snowflake storage connector [source]","title":"password"},{"location":"generated/api/storage_connector_api/#role","text":"Role of the Snowflake storage connector [source]","title":"role"},{"location":"generated/api/storage_connector_api/#schema","text":"Schema of the Snowflake storage connector [source]","title":"schema"},{"location":"generated/api/storage_connector_api/#table","text":"Table of the Snowflake storage connector [source]","title":"table"},{"location":"generated/api/storage_connector_api/#token","text":"OAuth token of the Snowflake storage connector [source]","title":"token"},{"location":"generated/api/storage_connector_api/#url","text":"URL of the Snowflake storage connector [source]","title":"url"},{"location":"generated/api/storage_connector_api/#user","text":"User of the Snowflake storage connector [source]","title":"user"},{"location":"generated/api/storage_connector_api/#warehouse","text":"Warehouse of the Snowflake storage connector","title":"warehouse"},{"location":"generated/api/storage_connector_api/#methods_5","text":"[source]","title":"Methods"},{"location":"generated/api/storage_connector_api/#read_5","text":"SnowflakeConnector . read ( query , data_format = None , options = {}, path = None ) Reads a query into a dataframe using the storage connector. [source]","title":"read"},{"location":"generated/api/storage_connector_api/#snowflake_connector_options","text":"SnowflakeConnector . snowflake_connector_options () In order to use the snowflake.connector Python library, this method prepares a Python dictionary with the needed arguments for you to connect to a Snowflake database. import snowflake.connector sc = fs . get_storage_connector ( \"snowflake_conn\" ) ctx = snowflake . connector . connect ( ** sc . snowflake_connector_options ()) [source]","title":"snowflake_connector_options"},{"location":"generated/api/storage_connector_api/#spark_options_5","text":"SnowflakeConnector . spark_options () Return prepared options to be passed to Spark, based on the additional arguments. [source]","title":"spark_options"},{"location":"generated/api/storage_connector_api/#to_dict_5","text":"SnowflakeConnector . to_dict () [source]","title":"to_dict"},{"location":"generated/api/storage_connector_api/#update_from_response_json_5","text":"SnowflakeConnector . update_from_response_json ( json_dict )","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/","text":"Training Dataset # [source] TrainingDataset # hsfs . training_dataset . TrainingDataset ( name , version , data_format , location , featurestore_id , coalesce = False , description = None , storage_connector = None , splits = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , ) Creation # [source] create_training_dataset # FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object. Retrieval # [source] get_training_dataset # FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store. Properties # [source] coalesce # If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source] data_format # File format of the training dataset. [source] description # [source] feature_store_id # [source] id # Training dataset id. [source] label # The label/prediction feature of the training dataset. Can be a composite of multiple features. [source] location # Path to the training dataset location. [source] name # Name of the training dataset. [source] prepared_statement_engine # JDBC connection engine to retrieve connections to online features store from. [source] prepared_statements # The dict object of prepared_statements as values and kes as indices of positions in the query for selecting features from feature groups of the training dataset. [source] query # Query to generate this training dataset from online feature store. [source] schema # Training dataset schema. [source] seed # Seed. [source] serving_keys # Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source] splits # Training dataset splits. train , test or eval and corresponding percentages. [source] statistics # Get the latest computed statistics for the training dataset. [source] statistics_config # Statistics configuration object defining the settings for statistics computation of the training dataset. [source] storage_connector # Storage connector. [source] transformation_functions # Set transformation functions. [source] version # Version number of the training dataset. [source] write_options # User provided options to write training dataset. Methods # [source] add_tag # TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source] compute_statistics # TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source] delete # TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source] delete_tag # TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] from_response_json # TrainingDataset . from_response_json ( json_dict ) [source] get_query # TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source] get_serving_vector # TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source] get_statistics # TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source] get_tag # TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] init_prepared_statement # TrainingDataset . init_prepared_statement ( external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source] insert # TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] json # TrainingDataset . json () [source] read # TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source] save # TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source] show # TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source] tf_data # TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source] to_dict # TrainingDataset . to_dict () [source] update_from_response_json # TrainingDataset . update_from_response_json ( json_dict ) [source] update_statistics_config # TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#training-dataset","text":"[source]","title":"Training Dataset"},{"location":"generated/api/training_dataset_api/#trainingdataset","text":"hsfs . training_dataset . TrainingDataset ( name , version , data_format , location , featurestore_id , coalesce = False , description = None , storage_connector = None , splits = None , seed = None , created = None , creator = None , features = None , statistics_config = None , featurestore_name = None , id = None , inode_id = None , training_dataset_type = None , from_query = None , querydto = None , label = None , transformation_functions = None , )","title":"TrainingDataset"},{"location":"generated/api/training_dataset_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/training_dataset_api/#create_training_dataset","text":"FeatureStore . create_training_dataset ( name , version = None , description = \"\" , data_format = \"tfrecords\" , coalesce = False , storage_connector = None , splits = {}, location = \"\" , seed = None , statistics_config = None , label = [], transformation_functions = {}, ) Create a training dataset metadata object. Lazy This method is lazy and does not persist any metadata or feature data in the feature store on its own. To materialize the training dataset and save feature data along the metadata in the feature store, call the save() method with a DataFrame or Query . Data Formats The feature store currently supports the following data formats for training datasets: tfrecord csv tsv parquet avro orc Currently not supported petastorm, hdf5 and npy file formats. Arguments name str : Name of the training dataset to create. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will create the training dataset with incremented version from the last version in the feature store. description Optional[str] : A string describing the contents of the training dataset to improve discoverability for Data Scientists, defaults to empty string \"\" . data_format Optional[str] : The data format used to save the training dataset, defaults to \"tfrecords\" -format. coalesce Optional[bool] : If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split. Default False. storage_connector Optional[hsfs.StorageConnector] : Storage connector defining the sink location for the training dataset, defaults to None , and materializes training dataset on HopsFS. splits Optional[Dict[str, float]] : A dictionary defining training dataset splits to be created. Keys in the dictionary define the name of the split as str , values represent percentage of samples in the split as float . Currently, only random splits are supported. Defaults to empty dict {} , creating only a single training dataset without splits. location Optional[str] : Path to complement the sink storage connector with, e.g if the storage connector points to an S3 bucket, this path can be used to define a sub-directory inside the bucket to place the training dataset. Defaults to \"\" , saving the training dataset at the root defined by the storage connector. seed Optional[int] : Optionally, define a seed to create the random splits with, in order to guarantee reproducability, defaults to None . statistics_config Optional[Union[hsfs.StatisticsConfig, bool, dict]] : A configuration object, or a dictionary with keys \" enabled \" to generally enable descriptive statistics computation for this feature group, \"correlations \" to turn on feature correlation computation and \"histograms\" to compute feature value frequencies. The values should be booleans indicating the setting. To fully turn off statistics computation pass statistics_config=False . Defaults to None and will compute only descriptive statistics. label Optional[List[str]] : A list of feature names constituting the prediction label/feature of the training dataset. When replaying a Query during model inference, the label features can be omitted from the feature vector retrieval. Defaults to [] , no label. Returns: TrainingDataset : The training dataset metadata object.","title":"create_training_dataset"},{"location":"generated/api/training_dataset_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/training_dataset_api/#get_training_dataset","text":"FeatureStore . get_training_dataset ( name , version = None ) Get a training dataset entity from the feature store. Getting a training dataset from the Feature Store means getting its metadata handle so you can subsequently read the data into a Spark or Pandas DataFrame. Arguments name str : Name of the training dataset to get. version Optional[int] : Version of the training dataset to retrieve, defaults to None and will return the version=1 . Returns TrainingDataset : The training dataset metadata object. Raises RestAPIError : If unable to retrieve feature group from the feature store.","title":"get_training_dataset"},{"location":"generated/api/training_dataset_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/training_dataset_api/#coalesce","text":"If true the training dataset data will be coalesced into a single partition before writing. The resulting training dataset will be a single file per split [source]","title":"coalesce"},{"location":"generated/api/training_dataset_api/#data_format","text":"File format of the training dataset. [source]","title":"data_format"},{"location":"generated/api/training_dataset_api/#description","text":"[source]","title":"description"},{"location":"generated/api/training_dataset_api/#feature_store_id","text":"[source]","title":"feature_store_id"},{"location":"generated/api/training_dataset_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/training_dataset_api/#label","text":"The label/prediction feature of the training dataset. Can be a composite of multiple features. [source]","title":"label"},{"location":"generated/api/training_dataset_api/#location","text":"Path to the training dataset location. [source]","title":"location"},{"location":"generated/api/training_dataset_api/#name","text":"Name of the training dataset. [source]","title":"name"},{"location":"generated/api/training_dataset_api/#prepared_statement_engine","text":"JDBC connection engine to retrieve connections to online features store from. [source]","title":"prepared_statement_engine"},{"location":"generated/api/training_dataset_api/#prepared_statements","text":"The dict object of prepared_statements as values and kes as indices of positions in the query for selecting features from feature groups of the training dataset. [source]","title":"prepared_statements"},{"location":"generated/api/training_dataset_api/#query","text":"Query to generate this training dataset from online feature store. [source]","title":"query"},{"location":"generated/api/training_dataset_api/#schema","text":"Training dataset schema. [source]","title":"schema"},{"location":"generated/api/training_dataset_api/#seed","text":"Seed. [source]","title":"seed"},{"location":"generated/api/training_dataset_api/#serving_keys","text":"Set of primary key names that is used as keys in input dict object for get_serving_vector method. [source]","title":"serving_keys"},{"location":"generated/api/training_dataset_api/#splits","text":"Training dataset splits. train , test or eval and corresponding percentages. [source]","title":"splits"},{"location":"generated/api/training_dataset_api/#statistics","text":"Get the latest computed statistics for the training dataset. [source]","title":"statistics"},{"location":"generated/api/training_dataset_api/#statistics_config","text":"Statistics configuration object defining the settings for statistics computation of the training dataset. [source]","title":"statistics_config"},{"location":"generated/api/training_dataset_api/#storage_connector","text":"Storage connector. [source]","title":"storage_connector"},{"location":"generated/api/training_dataset_api/#transformation_functions","text":"Set transformation functions. [source]","title":"transformation_functions"},{"location":"generated/api/training_dataset_api/#version","text":"Version number of the training dataset. [source]","title":"version"},{"location":"generated/api/training_dataset_api/#write_options","text":"User provided options to write training dataset.","title":"write_options"},{"location":"generated/api/training_dataset_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/training_dataset_api/#add_tag","text":"TrainingDataset . add_tag ( name , value ) Attach a tag to a training dataset. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag. [source]","title":"add_tag"},{"location":"generated/api/training_dataset_api/#compute_statistics","text":"TrainingDataset . compute_statistics () Recompute the statistics for the training dataset and save them to the feature store. [source]","title":"compute_statistics"},{"location":"generated/api/training_dataset_api/#delete","text":"TrainingDataset . delete () Delete training dataset and all associated metadata. Drops only HopsFS data Note that this operation drops only files which were materialized in HopsFS. If you used a Storage Connector for a cloud storage such as S3, the data will not be deleted, but you will not be able to track it anymore from the Feature Store. Potentially dangerous operation This operation drops all metadata associated with this version of the training dataset and and the materialized data in HopsFS. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/api/training_dataset_api/#delete_tag","text":"TrainingDataset . delete_tag ( name ) Delete a tag attached to a training dataset. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/api/training_dataset_api/#from_response_json","text":"TrainingDataset . from_response_json ( json_dict ) [source]","title":"from_response_json"},{"location":"generated/api/training_dataset_api/#get_query","text":"TrainingDataset . get_query ( online = True , with_label = False ) Returns the query used to generate this training dataset Arguments online bool : boolean, optional. Return the query for the online storage, else for offline storage, defaults to True - for online storage. with_label bool : Indicator whether the query should contain features which were marked as prediction label/feature when the training dataset was created, defaults to False . Returns str . Query string for the chosen storage used to generate this training dataset. [source]","title":"get_query"},{"location":"generated/api/training_dataset_api/#get_serving_vector","text":"TrainingDataset . get_serving_vector ( entry , external = False ) Returns assembled serving vector from online feature store. Arguments entry Dict[str, Any] : dictionary of training dataset feature group primary key names as keys and values provided by serving application. external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. Returns list List of feature values related to provided primary keys, ordered according to positions of this features in training dataset query. [source]","title":"get_serving_vector"},{"location":"generated/api/training_dataset_api/#get_statistics","text":"TrainingDataset . get_statistics ( commit_time = None ) Returns the statistics for this training dataset at a specific time. If commit_time is None , the most recent statistics are returned. Arguments commit_time Optional[str] : Commit time in the format YYYYMMDDhhmmss , defaults to None . Returns Statistics . Object with statistics information. [source]","title":"get_statistics"},{"location":"generated/api/training_dataset_api/#get_tag","text":"TrainingDataset . get_tag ( name ) Get the tags of a training dataset. Arguments name : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/api/training_dataset_api/#get_tags","text":"TrainingDataset . get_tags () Returns all tags attached to a training dataset. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/api/training_dataset_api/#init_prepared_statement","text":"TrainingDataset . init_prepared_statement ( external = False ) Initialise and cache parametrized prepared statement to retrieve feature vector from online feature store. Arguments external Optional[bool] : boolean, optional. If set to True, the connection to the online feature store is established using the same host as for the host parameter in the hsfs.connection() method. If set to False, the online feature store storage connector is used which relies on the private IP. [source]","title":"init_prepared_statement"},{"location":"generated/api/training_dataset_api/#insert","text":"TrainingDataset . insert ( features , overwrite , write_options = {}) Insert additional feature data into the training dataset. This method appends data to the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. The schemas must match for this operation. This can also be used to overwrite all data in an existing training dataset. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. overwrite bool : Whether to overwrite the entire data in the training dataset. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the insert call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"insert"},{"location":"generated/api/training_dataset_api/#json","text":"TrainingDataset . json () [source]","title":"json"},{"location":"generated/api/training_dataset_api/#read","text":"TrainingDataset . read ( split = None , read_options = {}) Read the training dataset into a dataframe. It is also possible to read only a specific split. Arguments split : Name of the split to read, defaults to None , reading the entire training dataset. read_options : Additional read options as key/value pairs, defaults to {} . Returns DataFrame : The spark dataframe containing the feature data of the training dataset. [source]","title":"read"},{"location":"generated/api/training_dataset_api/#save","text":"TrainingDataset . save ( features , write_options = {}) Materialize the training dataset to storage. This method materializes the training dataset either from a Feature Store Query , a Spark or Pandas DataFrame , a Spark RDD, two-dimensional Python lists or Numpy ndarrays. Arguments features Union[hsfs.constructor.query.Query, pandas.DataFrame, pyspark.sql.DataFrame, pyspark.RDD, numpy.ndarray, List[list]] : Feature data to be materialized. write_options Optional[Dict[Any, Any]] : Additional write options as key-value pairs, defaults to {} . When using the hive engine, write_options can contain the following entries: key spark and value an object of type hsfs.core.job_configuration.JobConfiguration to configure the Hopsworks Job used to compute the training dataset. key wait_for_job and value True or False to configure whether or not to the save call should return only after the Hopsworks Job has finished. By default it waits. Returns Job : When using the hive engine, it returns the Hopsworks Job that was launched to create the training dataset. Raises RestAPIError : Unable to create training dataset metadata. [source]","title":"save"},{"location":"generated/api/training_dataset_api/#show","text":"TrainingDataset . show ( n , split = None ) Show the first n rows of the training dataset. You can specify a split from which to retrieve the rows. Arguments n int : Number of rows to show. split Optional[str] : Name of the split to show, defaults to None , showing the first rows when taking all splits together. [source]","title":"show"},{"location":"generated/api/training_dataset_api/#tf_data","text":"TrainingDataset . tf_data ( target_name , split = None , feature_names = None , var_len_features = [], is_training = True , cycle_length = 2 , ) Returns an object with utility methods to read training dataset as tf.data.Dataset object and handle it for further processing. Arguments target_name str : Name of the target variable. split Optional[str] : Name of training dataset split. For example, \"train\" , \"test\" or \"val\" , defaults to None , returning the full training dataset. feature_names Optional[list] : Names of training variables, defaults to None . var_len_features Optional[list] : Feature names that have variable length and need to be returned as tf.io.VarLenFeature , defaults to [] . is_training Optional[bool] : Whether it is for training, testing or validation. Defaults to True . cycle_length Optional[int] : Number of files to be read and deserialized in parallel, defaults to 2 . Returns TFDataEngine . An object with utility methods to generate and handle tf.data.Dataset object. [source]","title":"tf_data"},{"location":"generated/api/training_dataset_api/#to_dict","text":"TrainingDataset . to_dict () [source]","title":"to_dict"},{"location":"generated/api/training_dataset_api/#update_from_response_json","text":"TrainingDataset . update_from_response_json ( json_dict ) [source]","title":"update_from_response_json"},{"location":"generated/api/training_dataset_api/#update_statistics_config","text":"TrainingDataset . update_statistics_config () Update the statistics configuration of the training dataset. Change the statistics_config object and persist the changes by calling this method. Returns TrainingDataset . The updated metadata object of the training dataset. Raises RestAPIError .","title":"update_statistics_config"},{"location":"generated/api/transformation_functions_api/","text":"Transfromation Function # [source] TransformationFunction # hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , output_type = None , id = None , type = None , items = None , count = None , href = None , ) Properties # [source] id # Training dataset id. [source] name # [source] output_type # [source] source_code_content # [source] transformation_fn # [source] transformer_code # [source] version # Methods # [source] delete # TransformationFunction . delete () Delete transformation function from backend. [source] save # TransformationFunction . save () Persist transformation function in backend. Creation # [source] create_transformation_function # FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object. Retrieval # [source] get_transformation_function # FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source] get_transformation_functions # FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"Transformation functions"},{"location":"generated/api/transformation_functions_api/#transfromation-function","text":"[source]","title":"Transfromation Function"},{"location":"generated/api/transformation_functions_api/#transformationfunction","text":"hsfs . transformation_function . TransformationFunction ( featurestore_id , transformation_fn = None , version = None , name = None , source_code_content = None , output_type = None , id = None , type = None , items = None , count = None , href = None , )","title":"TransformationFunction"},{"location":"generated/api/transformation_functions_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/transformation_functions_api/#id","text":"Training dataset id. [source]","title":"id"},{"location":"generated/api/transformation_functions_api/#name","text":"[source]","title":"name"},{"location":"generated/api/transformation_functions_api/#output_type","text":"[source]","title":"output_type"},{"location":"generated/api/transformation_functions_api/#source_code_content","text":"[source]","title":"source_code_content"},{"location":"generated/api/transformation_functions_api/#transformation_fn","text":"[source]","title":"transformation_fn"},{"location":"generated/api/transformation_functions_api/#transformer_code","text":"[source]","title":"transformer_code"},{"location":"generated/api/transformation_functions_api/#version","text":"","title":"version"},{"location":"generated/api/transformation_functions_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/api/transformation_functions_api/#delete","text":"TransformationFunction . delete () Delete transformation function from backend. [source]","title":"delete"},{"location":"generated/api/transformation_functions_api/#save","text":"TransformationFunction . save () Persist transformation function in backend.","title":"save"},{"location":"generated/api/transformation_functions_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/api/transformation_functions_api/#create_transformation_function","text":"FeatureStore . create_transformation_function ( transformation_function , output_type , version = None ) Create a transformation function metadata object. Lazy This method is lazy and does not persist the transformation function in the feature store on its own. To materialize the transformation function and save call the save() method of the transformation function metadata object. Arguments transformation_function callable : callable object. output_type Union[str, str, string, bytes, numpy.int8, int8, byte, numpy.int16, int16, short, int, int, numpy.int32, numpy.int64, int64, long, bigint, float, float, numpy.float64, float64, double, datetime.datetime, numpy.datetime64, datetime.date, bool, boolean, bool] : python or numpy output type that will be inferred as pyspark.sql.types type. Returns: TransformationFunction : The TransformationFunction metadata object.","title":"create_transformation_function"},{"location":"generated/api/transformation_functions_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/transformation_functions_api/#get_transformation_function","text":"FeatureStore . get_transformation_function ( name , version = None ) Get transformation function metadata object. Arguments name str : name of transformation function. version Optional[int] : version of transformation function. Optional, if not provided all functions that match to provided name will be retrieved . Returns: TransformationFunction : The TransformationFunction metadata object. [source]","title":"get_transformation_function"},{"location":"generated/api/transformation_functions_api/#get_transformation_functions","text":"FeatureStore . get_transformation_functions () Get all transformation functions metadata objects. Returns: List[TransformationFunction] . List of transformation function instances.","title":"get_transformation_functions"},{"location":"generated/api/validation_api/","text":"Validation # [source] ValidationResult # hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group. Properties # [source] features # Feature of the validation result on which the rule was applied. [source] message # Message describing the outcome of applying the rule against the feature. [source] rule # Feature of the validation result on which the rule was applied. [source] status # [source] value # The computed value of the feature according to the rule. Methods # {{expectation_methods}} Validate a dataframe # [source] validate # FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object. Retrieval # [source] get_validations # FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"Validation"},{"location":"generated/api/validation_api/#validation","text":"[source]","title":"Validation"},{"location":"generated/api/validation_api/#validationresult","text":"hsfs . validation_result . ValidationResult ( status , message , value , features , rule , href = None , expand = None , items = None , count = None , type = None , ) Metadata object representing the validation result of a single rule of an expectation result of a Feature Group.","title":"ValidationResult"},{"location":"generated/api/validation_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/api/validation_api/#features","text":"Feature of the validation result on which the rule was applied. [source]","title":"features"},{"location":"generated/api/validation_api/#message","text":"Message describing the outcome of applying the rule against the feature. [source]","title":"message"},{"location":"generated/api/validation_api/#rule","text":"Feature of the validation result on which the rule was applied. [source]","title":"rule"},{"location":"generated/api/validation_api/#status","text":"[source]","title":"status"},{"location":"generated/api/validation_api/#value","text":"The computed value of the feature according to the rule.","title":"value"},{"location":"generated/api/validation_api/#methods","text":"{{expectation_methods}}","title":"Methods"},{"location":"generated/api/validation_api/#validate-a-dataframe","text":"[source]","title":"Validate a dataframe"},{"location":"generated/api/validation_api/#validate","text":"FeatureGroup . validate ( dataframe = None ) Run validation based on the attached expectations Arguments dataframe Optional[pyspark.sql.DataFrame] : The PySpark dataframe to run the data validation expectations against. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"validate"},{"location":"generated/api/validation_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/api/validation_api/#get_validations","text":"FeatureGroup . get_validations ( validation_time = None , commit_time = None ) Get feature group data validation results based on the attached expectations Arguments validation_time : The data validation time, when the data validation started. commit_time: The commit time of a time travel enabled feature group. Returns FeatureGroupValidation . The feature group validation metadata object.","title":"get_validations"},{"location":"hopsworksai/","text":"Hopsworks.ai # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker, and KubeFlow. Getting started # To get started with deploying a cluster in you Azure or AWS environment, follow these guides: Azure , AWS . If you need more details about hopsworks.ai cluster creation steps you can check the following guides: Azure , AWS . Limiting permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s cloud account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. The following guides explain how you can reduce the permissions you give to hopsworks.ai by doing some of the cluster creation steps manually: Azure , AWS . Integration with managed Kubernetes # When deploying a cluster you can set it up to use managed Kubernetes to run python jobs, Jupyter servers, and ML model serving in a scalable way. This guide provides step-by-step instructions on how to set up Kubernetes and start a cluster using it: AWS . Integration with third-party platforms # Once you have deployed a cluster with hopsworks.ai you can connect to it from third-party platforms such as Databricks , AWS Sagemaker , Azure HDInsight , etc. Go to integrations for more third-party platforms and details. Other # You can find more information on the following topics by clicking on the links: GPU support Ading and removing workers User management","title":"Hopsworks.ai"},{"location":"hopsworksai/#hopsworksai","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker, and KubeFlow.","title":"Hopsworks.ai"},{"location":"hopsworksai/#getting-started","text":"To get started with deploying a cluster in you Azure or AWS environment, follow these guides: Azure , AWS . If you need more details about hopsworks.ai cluster creation steps you can check the following guides: Azure , AWS .","title":"Getting started"},{"location":"hopsworksai/#limiting-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s cloud account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. The following guides explain how you can reduce the permissions you give to hopsworks.ai by doing some of the cluster creation steps manually: Azure , AWS .","title":"Limiting permissions"},{"location":"hopsworksai/#integration-with-managed-kubernetes","text":"When deploying a cluster you can set it up to use managed Kubernetes to run python jobs, Jupyter servers, and ML model serving in a scalable way. This guide provides step-by-step instructions on how to set up Kubernetes and start a cluster using it: AWS .","title":"Integration with managed Kubernetes"},{"location":"hopsworksai/#integration-with-third-party-platforms","text":"Once you have deployed a cluster with hopsworks.ai you can connect to it from third-party platforms such as Databricks , AWS Sagemaker , Azure HDInsight , etc. Go to integrations for more third-party platforms and details.","title":"Integration with third-party platforms"},{"location":"hopsworksai/#other","text":"You can find more information on the following topics by clicking on the links: GPU support Ading and removing workers User management","title":"Other"},{"location":"hopsworksai/adding_removing_workers/","text":"Adding and removing workers # Once you have started a hopsworks cluster you can add and remove workers from the cluster to accommodate your workload. Adding workers # If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting. Removing workers # If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers Hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Adding and removing workers"},{"location":"hopsworksai/adding_removing_workers/#adding-and-removing-workers","text":"Once you have started a hopsworks cluster you can add and remove workers from the cluster to accommodate your workload.","title":"Adding and removing workers"},{"location":"hopsworksai/adding_removing_workers/#adding-workers","text":"If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting.","title":"Adding workers"},{"location":"hopsworksai/adding_removing_workers/#removing-workers","text":"If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers Hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Removing workers"},{"location":"hopsworksai/api_key/","text":"Hopsworks.ai API Key # Hopsworks.ai allows users to generate an API Key that can be used to authenticate and access the Hopsworks.ai REST APIs. Generate an API Key # First, login to your Hopsworks.ai account, then click on the Settings tab as shown below: Click on the Settings tab Click on the API Key tab, and then click on the Generate API Key button: Generate an API Key Copy the generated API Key and store it in a secure location. Warning Make sure to copy your API Key now. You won\u2019t be able to see it again. However, you can always delete it and generate a new one. Copy the generated API Key Use the API Key # To access the Hopsworks.ai REST APIs, you should pass the API key as a header x-api-key when executing requests on Hopsworks.ai as shown below: curl -XGET -H \"x-api-key: <YOUR API KEY>\" https://api.hopsworks.ai/api/clusters Alternatively, you can use your API Key with the Hopsworks.ai terraform provider to manage your Hopsworks clusters using terraform . Delete your API Key # First, login to your Hopsworks.ai account, click on the Settings tab, then click on the API Key tab, and finally click on Delete API Key as shown below: Delete your API Key","title":"Hopsworks.ai API Key"},{"location":"hopsworksai/api_key/#hopsworksai-api-key","text":"Hopsworks.ai allows users to generate an API Key that can be used to authenticate and access the Hopsworks.ai REST APIs.","title":"Hopsworks.ai API Key"},{"location":"hopsworksai/api_key/#generate-an-api-key","text":"First, login to your Hopsworks.ai account, then click on the Settings tab as shown below: Click on the Settings tab Click on the API Key tab, and then click on the Generate API Key button: Generate an API Key Copy the generated API Key and store it in a secure location. Warning Make sure to copy your API Key now. You won\u2019t be able to see it again. However, you can always delete it and generate a new one. Copy the generated API Key","title":"Generate an API Key"},{"location":"hopsworksai/api_key/#use-the-api-key","text":"To access the Hopsworks.ai REST APIs, you should pass the API key as a header x-api-key when executing requests on Hopsworks.ai as shown below: curl -XGET -H \"x-api-key: <YOUR API KEY>\" https://api.hopsworks.ai/api/clusters Alternatively, you can use your API Key with the Hopsworks.ai terraform provider to manage your Hopsworks clusters using terraform .","title":"Use the API Key"},{"location":"hopsworksai/api_key/#delete-your-api-key","text":"First, login to your Hopsworks.ai account, click on the Settings tab, then click on the API Key tab, and finally click on Delete API Key as shown below: Delete your API Key","title":"Delete your API Key"},{"location":"hopsworksai/autoscaling/","text":"Autoscaling # If you run a Hopsworks cluster version 2.2 or above you can enable autoscaling to let hopsworks.ai start and stop workers depending on the demand. Enabling and configuring the autoscaling # Once you have created a cluster you can enable autoscaling by going to the Details tab and clicking on Configure autoscale . You can also set up autoscaling during the cluster creation. For more details about this see the cluster creation documentation ( AWS , AZURE ). Configure autoscale Once you have clicked on Configure autoscale you will access a form allowing you to configure the autoscaling. This form is in two parts. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Configure autoscale details Once you have set your configuration you can review it and enable the autoscaling. Note There are two scenarios if you already have workers in your cluster when enabling the autoscaling: The preexisting workers have the same instance type as the one you set up in the autoscaling. In this case, the autoscaling system will manage these workers and start or stop them automatically. The preexisting workers have a different instance type from the one you set up in the autoscaling. In this case, the autoscaling will not manage these nodes but you will still be able to remove them manually. Configure autoscale review Modifying the autoscaling configuration # You can update the autoscale configuration by going to the Details tab of the cluster and clicking on Configure autoscale . You will then go through the same steps as above. Note that if you change the instance type , nodes that currently exist in the cluster with a different instance type will not be managed by the autoscale system anymore and you will have to remove them manually. Disabling the autoscaling # To disable the autoscaling go to the Details tab, click on Disable autoscale and confirm your action. When you disable autoscaling the nodes that are currently running will keep running. You will need to stop them manually. Disable autoscale","title":"Autoscaling"},{"location":"hopsworksai/autoscaling/#autoscaling","text":"If you run a Hopsworks cluster version 2.2 or above you can enable autoscaling to let hopsworks.ai start and stop workers depending on the demand.","title":"Autoscaling"},{"location":"hopsworksai/autoscaling/#enabling-and-configuring-the-autoscaling","text":"Once you have created a cluster you can enable autoscaling by going to the Details tab and clicking on Configure autoscale . You can also set up autoscaling during the cluster creation. For more details about this see the cluster creation documentation ( AWS , AZURE ). Configure autoscale Once you have clicked on Configure autoscale you will access a form allowing you to configure the autoscaling. This form is in two parts. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Configure autoscale details Once you have set your configuration you can review it and enable the autoscaling. Note There are two scenarios if you already have workers in your cluster when enabling the autoscaling: The preexisting workers have the same instance type as the one you set up in the autoscaling. In this case, the autoscaling system will manage these workers and start or stop them automatically. The preexisting workers have a different instance type from the one you set up in the autoscaling. In this case, the autoscaling will not manage these nodes but you will still be able to remove them manually. Configure autoscale review","title":"Enabling and configuring the autoscaling"},{"location":"hopsworksai/autoscaling/#modifying-the-autoscaling-configuration","text":"You can update the autoscale configuration by going to the Details tab of the cluster and clicking on Configure autoscale . You will then go through the same steps as above. Note that if you change the instance type , nodes that currently exist in the cluster with a different instance type will not be managed by the autoscale system anymore and you will have to remove them manually.","title":"Modifying the autoscaling configuration"},{"location":"hopsworksai/autoscaling/#disabling-the-autoscaling","text":"To disable the autoscaling go to the Details tab, click on Disable autoscale and confirm your action. When you disable autoscaling the nodes that are currently running will keep running. You will need to stop them manually. Disable autoscale","title":"Disabling the autoscaling"},{"location":"hopsworksai/gpu_support/","text":"GPU support # Hopsworks can harness the power of GPUs to speed up machine learning processes. You can take advantage of this feature in Hopsworks.ai by adding GPU equipped workers to your cluster. This can be done in two way: creating a cluster with GPU equipped workers or adding GPU equipped workers to an existing cluster. Creating a cluster with GPU equipped workers # When selecting the workers' instance type during the cluster creation, you can select an instance type equipped with GPUs. The cluster will then be created and Hopsworks will automatically detect the GPU resource. Create cluster with GPUs Adding GPU equipped workers to an existing cluster. # When adding workers to a cluster, you can select an instance type equipped with GPUs. The workers will then be added to the cluster and Hopsworks will automatically detect the new GPU resource. Add GPUs to cluster Using the GPUs # Once workers with GPUs have been added to your cluster you can use them by allocating GPUs to JupyterLab or Jobs. Using GPUs in JupyterLab Using GPUs in jobs For more information about using GPUs in Hopsworks you can consult Hopsworks Experiments documentation .","title":"GPU support"},{"location":"hopsworksai/gpu_support/#gpu-support","text":"Hopsworks can harness the power of GPUs to speed up machine learning processes. You can take advantage of this feature in Hopsworks.ai by adding GPU equipped workers to your cluster. This can be done in two way: creating a cluster with GPU equipped workers or adding GPU equipped workers to an existing cluster.","title":"GPU support"},{"location":"hopsworksai/gpu_support/#creating-a-cluster-with-gpu-equipped-workers","text":"When selecting the workers' instance type during the cluster creation, you can select an instance type equipped with GPUs. The cluster will then be created and Hopsworks will automatically detect the GPU resource. Create cluster with GPUs","title":"Creating a cluster with GPU equipped workers"},{"location":"hopsworksai/gpu_support/#adding-gpu-equipped-workers-to-an-existing-cluster","text":"When adding workers to a cluster, you can select an instance type equipped with GPUs. The workers will then be added to the cluster and Hopsworks will automatically detect the new GPU resource. Add GPUs to cluster","title":"Adding GPU equipped workers to an existing cluster."},{"location":"hopsworksai/gpu_support/#using-the-gpus","text":"Once workers with GPUs have been added to your cluster you can use them by allocating GPUs to JupyterLab or Jobs. Using GPUs in JupyterLab Using GPUs in jobs For more information about using GPUs in Hopsworks you can consult Hopsworks Experiments documentation .","title":"Using the GPUs"},{"location":"hopsworksai/rondb/","text":"Managed RonDB # For applications where Feature Store's performance and scalability is paramount we give users the option to create clusters with Managed RonDB . You don't need to worry about configuration as hopsworks.ai will automatically pick the best options for your setup. Enabling RonDB # To setup a cluster with RonDB, click on the Enable checkbox during cluster creation. If you don't see this option contact us . Enable Managed RonDB Node selection # If you enable Managed RonDB you will see a basic configuration page where you can configure the database nodes. RonDB basic configuration First, you need to select the number of Data nodes . These are the database nodes that will store data. The number of Datanodes is dependent on the number of replicas . For most applications 2 Datanodes will suffice with memory optimized instance type. Local storage is not very important as RonDB is an in-memory database but it has to be big enough for offline storage of recovery data. Next you can configure the number of MySQLd nodes. If you have a specific use-case where you expect high load on MySQL servers, such as a custom streaming application performing SQL queries in short interval, then you can add more. In a general use-case you can go with zero, cluster's Head node already comes with a MySQL server. Advanced # In this section you can change advanced settings of RonDB. Proceed only if you know what you are doing. RonDB advanced configuration Number of replicas # RonDB is a distributed in-memory database. To provide high-availability RonDB replicates data on different node groups. The default number of replicas is 2 which covers most of the cases. If you change this value, you must change the number of Datanodes accordingly. The number of Datanodes configured in the Basic tab should be evenly divisible by the numer of replicas. API nodes # API nodes are specialized nodes which can run user code connecting directly to RonDB datanodes for increased performance. One use-case which might be interesting is to benchmark RonDB. In that case you will need at least one API node and you must select the checkbox which grants access to a benchmark user to specific tables in MySQL. RonDB details # Once the cluster is created you can view some details by clicking on the RonDB tab as shown in the picture below. RonDB cluster details","title":"Managed RonDB"},{"location":"hopsworksai/rondb/#managed-rondb","text":"For applications where Feature Store's performance and scalability is paramount we give users the option to create clusters with Managed RonDB . You don't need to worry about configuration as hopsworks.ai will automatically pick the best options for your setup.","title":"Managed RonDB"},{"location":"hopsworksai/rondb/#enabling-rondb","text":"To setup a cluster with RonDB, click on the Enable checkbox during cluster creation. If you don't see this option contact us . Enable Managed RonDB","title":"Enabling RonDB"},{"location":"hopsworksai/rondb/#node-selection","text":"If you enable Managed RonDB you will see a basic configuration page where you can configure the database nodes. RonDB basic configuration First, you need to select the number of Data nodes . These are the database nodes that will store data. The number of Datanodes is dependent on the number of replicas . For most applications 2 Datanodes will suffice with memory optimized instance type. Local storage is not very important as RonDB is an in-memory database but it has to be big enough for offline storage of recovery data. Next you can configure the number of MySQLd nodes. If you have a specific use-case where you expect high load on MySQL servers, such as a custom streaming application performing SQL queries in short interval, then you can add more. In a general use-case you can go with zero, cluster's Head node already comes with a MySQL server.","title":"Node selection"},{"location":"hopsworksai/rondb/#advanced","text":"In this section you can change advanced settings of RonDB. Proceed only if you know what you are doing. RonDB advanced configuration","title":"Advanced"},{"location":"hopsworksai/rondb/#number-of-replicas","text":"RonDB is a distributed in-memory database. To provide high-availability RonDB replicates data on different node groups. The default number of replicas is 2 which covers most of the cases. If you change this value, you must change the number of Datanodes accordingly. The number of Datanodes configured in the Basic tab should be evenly divisible by the numer of replicas.","title":"Number of replicas"},{"location":"hopsworksai/rondb/#api-nodes","text":"API nodes are specialized nodes which can run user code connecting directly to RonDB datanodes for increased performance. One use-case which might be interesting is to benchmark RonDB. In that case you will need at least one API node and you must select the checkbox which grants access to a benchmark user to specific tables in MySQL.","title":"API nodes"},{"location":"hopsworksai/rondb/#rondb-details","text":"Once the cluster is created you can view some details by clicking on the RonDB tab as shown in the picture below. RonDB cluster details","title":"RonDB details"},{"location":"hopsworksai/terraform/","text":"Hopsworks.ai Terraform Provider # Hopsworks.ai allows users to create and manage their clusters using the Hopsworks.ai terraform provider . In this guide, we first provide brief description on how to get started on AWS and AZURE, then we show how to import an existing cluster to be managed by terraform. Getting Started with AWS # Complete the following steps to start using Hopsworks.ai Terraform Provider on AWS. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AWS CLI and run aws configurre to configure your AWS credentials. Example # In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \">=3.42.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"region\" { type = string default = \"us-east-2\" } provider \"aws\" { region = var.region } provider \"hopsworksai\" { } # Create the required aws resources, an ssh key, an s3 bucket, and an instance profile with the required hopsworks permissions module \"aws\" { source = \"logicalclocks/helpers/hopsworksai//modules/aws\" region = var.region } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.aws.ssh_key_pair_name head { } aws_attributes { region = var.region bucket_name = module.aws.bucket_name instance_profile_arn = module.aws.instance_profile_arn } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AWS resources using the following command terraform destroy Getting Started with AZURE # Complete the following steps to start using Hopsworks.ai Terraform Provider on AZURE. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AZURE CLI and run az login to configure your AZURE credentials. Example # In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \">= 2.60.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"resource_group\" { type = string } provider \"azurerm\" { features {} skip_provider_registration = true } provider \"hopsworksai\" { } data \"azurerm_resource_group\" \"rg\" { name = var.resource_group } # Create the required azure resources, an ssh key, a storage account, and an user assigned managed identity with the required hopsworks permissions module \"azure\" { source = \"logicalclocks/helpers/hopsworksai//modules/azure\" resource_group = var.resource_group } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.azure.ssh_key_pair_name head { } azure_attributes { location = module.azure.location resource_group = module.azure.resource_group storage_account = module.azure.storage_account_name user_assigned_managed_identity = module.azure.user_assigned_identity_name } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AZURE resources using the following command terraform destroy Importing an existing cluster to terraform # In this section, we show how to use terraform import to manage your existing Hopsworks cluster. Step 1 : In your Hopsworks.ai dashboard , choose the cluster you want to import to terraform, then go to the Details tab and copy the Id as shown in the figure below Click on the Details tab and copy the Id Step 2 : In your terminal, create an empty directory and cd to it. mkdir import-demo cd import-demo Step 3 : In this empty directory, create an empty file versions.tf . Open the file and paste the following configurations. Note Notice that you need to change these configurations depending on your cluster, in this example, the Hopsworks cluster reside in region us-east-2 on AWS. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \">=3.42.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } provider \"aws\" { region = us-east-2 } provider \"hopsworksai\" { } Step 4 : Initialize the terraform directory by running the following command terraform init Step 5 : Create another file main.tf . Open the file and paste the following configuration. resource \"hopsworksai_cluster\" \"cluster\" { } Step 6 : Import the cluster state using terraform import , in this step you need the cluster id from Step 1 (33ae7ae0-d03c-11eb-84e2-af555fb63565). terraform import hopsworksai_cluster.cluster 33ae7ae0-d03c-11eb-84e2-af555fb63565 The output should be similar to the following snippet hopsworksai_cluster.cluster: Importing from ID \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" ... hopsworksai_cluster.cluster: Import prepared! Prepared hopsworksai_cluster for import hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Step 7 : At that moment the local terraform state is updated, however, if we try to run terraform plan or terraform apply it will complain about missing configurations. The reason is that our local resource configuration in main.tf is empty, we should populate it using the terraform state commands as shown below: terraform show -no-color > main.tf Step 8 : If you try to run terraform plan again, the command will complain that the read-only attributes are set (Computed attributes) as shown below. The solution is to remove these attributes from the main.tf and retry again until you have no errors. Error: Computed attributes cannot be set on main.tf line 3 , in resource \"hopsworksai_cluster\" \"cluster\" : 3 : activation_state = \"stoppable\" Computed attributes cannot be set, but a value was set for \"activation_state\" . Error: Computed attributes cannot be set on main.tf line 6 , in resource \"hopsworksai_cluster\" \"cluster\" : 6 : cluster_id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Computed attributes cannot be set, but a value was set for \"cluster_id\" . Error: Computed attributes cannot be set on main.tf line 7 , in resource \"hopsworksai_cluster\" \"cluster\" : 7 : creation_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"creation_date\" . Error: Invalid or unknown key on main.tf line 8 , in resource \"hopsworksai_cluster\" \"cluster\" : 8 : id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Error: Computed attributes cannot be set on main.tf line 13 , in resource \"hopsworksai_cluster\" \"cluster\" : 13 : start_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"start_date\" . Error: Computed attributes cannot be set on main.tf line 14 , in resource \"hopsworksai_cluster\" \"cluster\" : 14 : state = \"running\" Computed attributes cannot be set, but a value was set for \"state\" . Error: Computed attributes cannot be set on main.tf line 17 , in resource \"hopsworksai_cluster\" \"cluster\" : 17 : url = \"https://33ae7ae0-d03c-11eb-84e2-af555fb63565.dev-cloud.hopsworks.ai/hopsworks/#!/\" Computed attributes cannot be set, but a value was set for \"url\" . Step 9 : Once you have fixed all the errors, you should get the following output when running terraform plan . With that, you can proceed as normal to manage this cluster locally using terraform. hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] No changes. Infrastructure is up-to-date. This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed. Next Steps # Check the Hopsworks.ai terraform provider documentation for more details about the different resources and data sources supported by the provider and a description of their attributes. Check the Hopsworks.ai terraform AWS examples , each example contains a README file describing how to run it and more details about configuring it. Check the Hopsworks.ai terraform AZURE examples , each example contains a README file describing how to run it and more details about configuring it.","title":"Hopsworks.ai Terraform Provider"},{"location":"hopsworksai/terraform/#hopsworksai-terraform-provider","text":"Hopsworks.ai allows users to create and manage their clusters using the Hopsworks.ai terraform provider . In this guide, we first provide brief description on how to get started on AWS and AZURE, then we show how to import an existing cluster to be managed by terraform.","title":"Hopsworks.ai Terraform Provider"},{"location":"hopsworksai/terraform/#getting-started-with-aws","text":"Complete the following steps to start using Hopsworks.ai Terraform Provider on AWS. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AWS CLI and run aws configurre to configure your AWS credentials.","title":"Getting Started with AWS"},{"location":"hopsworksai/terraform/#example","text":"In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \">=3.42.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"region\" { type = string default = \"us-east-2\" } provider \"aws\" { region = var.region } provider \"hopsworksai\" { } # Create the required aws resources, an ssh key, an s3 bucket, and an instance profile with the required hopsworks permissions module \"aws\" { source = \"logicalclocks/helpers/hopsworksai//modules/aws\" region = var.region } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.aws.ssh_key_pair_name head { } aws_attributes { region = var.region bucket_name = module.aws.bucket_name instance_profile_arn = module.aws.instance_profile_arn } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AWS resources using the following command terraform destroy","title":"Example"},{"location":"hopsworksai/terraform/#getting-started-with-azure","text":"Complete the following steps to start using Hopsworks.ai Terraform Provider on AZURE. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AZURE CLI and run az login to configure your AZURE credentials.","title":"Getting Started with AZURE"},{"location":"hopsworksai/terraform/#example_1","text":"In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \">= 2.60.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"resource_group\" { type = string } provider \"azurerm\" { features {} skip_provider_registration = true } provider \"hopsworksai\" { } data \"azurerm_resource_group\" \"rg\" { name = var.resource_group } # Create the required azure resources, an ssh key, a storage account, and an user assigned managed identity with the required hopsworks permissions module \"azure\" { source = \"logicalclocks/helpers/hopsworksai//modules/azure\" resource_group = var.resource_group } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.azure.ssh_key_pair_name head { } azure_attributes { location = module.azure.location resource_group = module.azure.resource_group storage_account = module.azure.storage_account_name user_assigned_managed_identity = module.azure.user_assigned_identity_name } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AZURE resources using the following command terraform destroy","title":"Example"},{"location":"hopsworksai/terraform/#importing-an-existing-cluster-to-terraform","text":"In this section, we show how to use terraform import to manage your existing Hopsworks cluster. Step 1 : In your Hopsworks.ai dashboard , choose the cluster you want to import to terraform, then go to the Details tab and copy the Id as shown in the figure below Click on the Details tab and copy the Id Step 2 : In your terminal, create an empty directory and cd to it. mkdir import-demo cd import-demo Step 3 : In this empty directory, create an empty file versions.tf . Open the file and paste the following configurations. Note Notice that you need to change these configurations depending on your cluster, in this example, the Hopsworks cluster reside in region us-east-2 on AWS. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \">=3.42.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } provider \"aws\" { region = us-east-2 } provider \"hopsworksai\" { } Step 4 : Initialize the terraform directory by running the following command terraform init Step 5 : Create another file main.tf . Open the file and paste the following configuration. resource \"hopsworksai_cluster\" \"cluster\" { } Step 6 : Import the cluster state using terraform import , in this step you need the cluster id from Step 1 (33ae7ae0-d03c-11eb-84e2-af555fb63565). terraform import hopsworksai_cluster.cluster 33ae7ae0-d03c-11eb-84e2-af555fb63565 The output should be similar to the following snippet hopsworksai_cluster.cluster: Importing from ID \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" ... hopsworksai_cluster.cluster: Import prepared! Prepared hopsworksai_cluster for import hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Step 7 : At that moment the local terraform state is updated, however, if we try to run terraform plan or terraform apply it will complain about missing configurations. The reason is that our local resource configuration in main.tf is empty, we should populate it using the terraform state commands as shown below: terraform show -no-color > main.tf Step 8 : If you try to run terraform plan again, the command will complain that the read-only attributes are set (Computed attributes) as shown below. The solution is to remove these attributes from the main.tf and retry again until you have no errors. Error: Computed attributes cannot be set on main.tf line 3 , in resource \"hopsworksai_cluster\" \"cluster\" : 3 : activation_state = \"stoppable\" Computed attributes cannot be set, but a value was set for \"activation_state\" . Error: Computed attributes cannot be set on main.tf line 6 , in resource \"hopsworksai_cluster\" \"cluster\" : 6 : cluster_id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Computed attributes cannot be set, but a value was set for \"cluster_id\" . Error: Computed attributes cannot be set on main.tf line 7 , in resource \"hopsworksai_cluster\" \"cluster\" : 7 : creation_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"creation_date\" . Error: Invalid or unknown key on main.tf line 8 , in resource \"hopsworksai_cluster\" \"cluster\" : 8 : id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Error: Computed attributes cannot be set on main.tf line 13 , in resource \"hopsworksai_cluster\" \"cluster\" : 13 : start_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"start_date\" . Error: Computed attributes cannot be set on main.tf line 14 , in resource \"hopsworksai_cluster\" \"cluster\" : 14 : state = \"running\" Computed attributes cannot be set, but a value was set for \"state\" . Error: Computed attributes cannot be set on main.tf line 17 , in resource \"hopsworksai_cluster\" \"cluster\" : 17 : url = \"https://33ae7ae0-d03c-11eb-84e2-af555fb63565.dev-cloud.hopsworks.ai/hopsworks/#!/\" Computed attributes cannot be set, but a value was set for \"url\" . Step 9 : Once you have fixed all the errors, you should get the following output when running terraform plan . With that, you can proceed as normal to manage this cluster locally using terraform. hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] No changes. Infrastructure is up-to-date. This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed.","title":"Importing an existing cluster to terraform"},{"location":"hopsworksai/terraform/#next-steps","text":"Check the Hopsworks.ai terraform provider documentation for more details about the different resources and data sources supported by the provider and a description of their attributes. Check the Hopsworks.ai terraform AWS examples , each example contains a README file describing how to run it and more details about configuring it. Check the Hopsworks.ai terraform AZURE examples , each example contains a README file describing how to run it and more details about configuring it.","title":"Next Steps"},{"location":"hopsworksai/user_management/","text":"User management # In Hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with Hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting. Adding members to an organization # Organization membership can be edited by clicking Members on the left of Hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. An invited user must accept the invitation to be part of the organization. An invitation will show up in the invited member's Dashboard. In this example Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard Sharing resources # Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters. Removing members from an organization # To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member","title":"User management"},{"location":"hopsworksai/user_management/#user-management","text":"In Hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with Hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting.","title":"User management"},{"location":"hopsworksai/user_management/#adding-members-to-an-organization","text":"Organization membership can be edited by clicking Members on the left of Hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. An invited user must accept the invitation to be part of the organization. An invitation will show up in the invited member's Dashboard. In this example Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard","title":"Adding members to an organization"},{"location":"hopsworksai/user_management/#sharing-resources","text":"Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters.","title":"Sharing resources"},{"location":"hopsworksai/user_management/#removing-members-from-an-organization","text":"To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member","title":"Removing members from an organization"},{"location":"hopsworksai/aws/cluster_creation/","text":"Getting started with Hopsworks.ai (AWS) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enter the name of the S3 bucket (5) you want the cluster to store its data in, in S3 bucket . Note The S3 bucket you are using must be empty. Create a Hopsworks cluster, general information Step 3 workers configuration # In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand. Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select an SSH key # When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key Step 5 select the Instance Profile # To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket Choose the instance profile Step 6 set the backup retention policy # Note This step is only accessible to enterprise users. To back up the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 7 Managed Containers # Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS and Amazon ECR . Add EKS cluster name Step 8 VPC selection # In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC Step 9 Availability Zone selection # If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone Step 10 Security group selection # If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note For Hopsworks.ai to create the SSL certificates the security group needs to allow inbound traffic on port 80. If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Choose security group Step 11 User management selection # In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 12 Managed RonDB # Hopsworks.ai users have the option to create a cluster with dedicated VMs running RonDB . For details on how to configure RonDB check our guide here . Enable Managed RonDB If you need this feature and don't see this step please contact us . Step 13 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 14 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 15 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/cluster_creation/#getting-started-with-hopsworksai-aws","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"hopsworksai/aws/cluster_creation/#step-2-setting-the-general-information","text":"Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enter the name of the S3 bucket (5) you want the cluster to store its data in, in S3 bucket . Note The S3 bucket you are using must be empty. Create a Hopsworks cluster, general information","title":"Step 2 setting the General information"},{"location":"hopsworksai/aws/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand.","title":"Step 3 workers configuration"},{"location":"hopsworksai/aws/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"hopsworksai/aws/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"hopsworksai/aws/cluster_creation/#step-4-select-an-ssh-key","text":"When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key","title":"Step 4 select an SSH key"},{"location":"hopsworksai/aws/cluster_creation/#step-5-select-the-instance-profile","text":"To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket Choose the instance profile","title":"Step 5 select the Instance Profile"},{"location":"hopsworksai/aws/cluster_creation/#step-6-set-the-backup-retention-policy","text":"Note This step is only accessible to enterprise users. To back up the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 6 set the backup retention policy"},{"location":"hopsworksai/aws/cluster_creation/#step-7-managed-containers","text":"Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS and Amazon ECR . Add EKS cluster name","title":"Step 7 Managed Containers"},{"location":"hopsworksai/aws/cluster_creation/#step-8-vpc-selection","text":"In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC","title":"Step 8 VPC selection"},{"location":"hopsworksai/aws/cluster_creation/#step-9-availability-zone-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone","title":"Step 9 Availability Zone selection"},{"location":"hopsworksai/aws/cluster_creation/#step-10-security-group-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note For Hopsworks.ai to create the SSL certificates the security group needs to allow inbound traffic on port 80. If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Choose security group","title":"Step 10 Security group selection"},{"location":"hopsworksai/aws/cluster_creation/#step-11-user-management-selection","text":"In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 11 User management selection"},{"location":"hopsworksai/aws/cluster_creation/#step-12-managed-rondb","text":"Hopsworks.ai users have the option to create a cluster with dedicated VMs running RonDB . For details on how to configure RonDB check our guide here . Enable Managed RonDB If you need this feature and don't see this step please contact us .","title":"Step 12 Managed RonDB"},{"location":"hopsworksai/aws/cluster_creation/#step-13-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 13 add tags to your instances."},{"location":"hopsworksai/aws/cluster_creation/#step-14-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 14 add an init script to your instances."},{"location":"hopsworksai/aws/cluster_creation/#step-15-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 15 Review and create"},{"location":"hopsworksai/aws/eks_ecr_integration/","text":"Integration with Amazon EKS and Amazon ECR # This guide shows how to create a cluster in Hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster. Step 1: Create an EKS cluster on AWS # If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command. Step 1.1: Installing eksctl, aws, and kubectl # Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl. Step 1.2: Create an EKS cluster using eksctl # You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.17 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .17 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .17 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get the list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976 Step 2: Create an instance profile role on AWS # You need to add permission to the instance profile you use for instances deployed by Hopsworks.ai to give them access to EKS and ECR. Go to the IAM service in the AWS management console , click Roles , search for your role, and click on it. Click on Add inline policy . Go to the JSON tab and replace the existing JSON permissions with the JSON permissions below.. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowPullMainImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/filebeat\" , \"arn:aws:ecr:*:*:repository/base\" ] }, { \"Sid\" : \"AllowPushandPullImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:CreateRepository\" , \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:DeleteRepository\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" } ] } Click on Review policy . Give a name to your policy and click on Create policy . Copy the Role ARN of your profile (not to be confused with the Instance Profile ARNs two lines bellow). Coppy the *Role ARN* Step 3: Allow your role to use your EKS cluster # You need to configure your EKS cluster to accept connections from the role you created above. This is done by using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . Note The kubectl edit command uses the vi editor by default, however, you can override this behavior by setting KUBE_EDITOR to your preferred editor . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign the system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with the Role ARN you copied in the previous step before saving. Warning Make sure to keep the same formatting as in the example below. The configuration format is sensitive to indentation and copy-pasting does not always keep the correct indentation. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save it and exit the editor. The output should be: configmap/aws-auth edited Step 4: Open Hopsworks required ports on your EKS cluster security group # To keep this documentation simple will run Hopsworks in the same virtual network as the EKS cluster. For this purpose, we need to open ports for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. You can also use VPC peering to run hopsworks and EKS in two different VPCs. Make sure to create the peering before starting the hopsworks cluster as it connects to EKS at startup. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Go to the Security Groups section of EC2 in the AWS management console and search for your security group using the id obtained above. Note the VPC ID , you will need it when creating the hopsworks cluster. Then, click on it then go to the Inbound rules tab and click on Edit inbound rules . You should now see the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group. Step 5: Allow Hopsworks.ai to delete ECR repositories on your behalf # For hopsworks.ai to be able to clean up the ECR repo when terminating your hopsworks cluster, you need to add a new inline policy to the Cross-Account role or user connected to Hopsworks.ai , that you set up when connecting your AWS account to hopsworks.ai . Navigate to AWS management console , then click on Roles or Users depending on which connection method you have used in Hopsworks.ai, and then search for your role or user name and click on it. Go to the Permissions tab, click on Add inline policy and go to the JSON tab. Replace the existing JSON permissions with the JSON permissions below. Click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDeletingECRRepositories\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:DeleteRepository\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] } ] } Step 6: Create a Hopsworks cluster with EKS and ECR support # In Hopsworks.ai, select Create cluster . Choose the region of your EKS cluster and fill in the name of your S3 bucket , then click Next: Create Hopsworks cluster Choose your preferred SSH key to use with the cluster, then click Next: Choose SSH key Choose the instance profile role that you have created in Step 2 (click on the refresh button if your instance profile is not in the list), then click Next: Choose instance profile role Choose the backup retention period and click Next: Choose the backup retention policy Choose Enabled to enable the use of Amazon EKS and ECR: Choose Enabled Add your EKS cluster name, then click Next: Add EKS cluster name Choose the VPC of your EKS cluster. It's name should have the form eksctl-YOUR-CLUSTER-NAME-cluster . You can also find it using the VPC ID you noted in Step 4 (click on the refresh button if the VPC is not in the list). Then click Next: Choose VPC Choose any of the subnets in the VPC, then click Next. Note Avoid private subnets if you want to enjoy all the hopsworks features . Choose Subnet Choose the security group that you have updated in Step 4 , then click Next: Note Select the Security Group with the same id as in Step 4 and NOT the ones containing ControlPlaneSecurity or ClusterSharedNode in their name. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"Integration with Amazon EKS and Amazon ECR"},{"location":"hopsworksai/aws/eks_ecr_integration/#integration-with-amazon-eks-and-amazon-ecr","text":"This guide shows how to create a cluster in Hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster.","title":"Integration with Amazon EKS and Amazon ECR"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-1-create-an-eks-cluster-on-aws","text":"If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command.","title":"Step 1: Create an EKS cluster on AWS"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-11-installing-eksctl-aws-and-kubectl","text":"Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl.","title":"Step 1.1: Installing eksctl, aws, and kubectl"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-12-create-an-eks-cluster-using-eksctl","text":"You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.17 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .17 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .17 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get the list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976","title":"Step 1.2: Create an EKS cluster using eksctl"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-2-create-an-instance-profile-role-on-aws","text":"You need to add permission to the instance profile you use for instances deployed by Hopsworks.ai to give them access to EKS and ECR. Go to the IAM service in the AWS management console , click Roles , search for your role, and click on it. Click on Add inline policy . Go to the JSON tab and replace the existing JSON permissions with the JSON permissions below.. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowPullMainImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/filebeat\" , \"arn:aws:ecr:*:*:repository/base\" ] }, { \"Sid\" : \"AllowPushandPullImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:CreateRepository\" , \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:DeleteRepository\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" } ] } Click on Review policy . Give a name to your policy and click on Create policy . Copy the Role ARN of your profile (not to be confused with the Instance Profile ARNs two lines bellow). Coppy the *Role ARN*","title":"Step 2: Create an instance profile role on AWS"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-3-allow-your-role-to-use-your-eks-cluster","text":"You need to configure your EKS cluster to accept connections from the role you created above. This is done by using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . Note The kubectl edit command uses the vi editor by default, however, you can override this behavior by setting KUBE_EDITOR to your preferred editor . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign the system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with the Role ARN you copied in the previous step before saving. Warning Make sure to keep the same formatting as in the example below. The configuration format is sensitive to indentation and copy-pasting does not always keep the correct indentation. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save it and exit the editor. The output should be: configmap/aws-auth edited","title":"Step 3: Allow your role to use your EKS cluster"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-4-open-hopsworks-required-ports-on-your-eks-cluster-security-group","text":"To keep this documentation simple will run Hopsworks in the same virtual network as the EKS cluster. For this purpose, we need to open ports for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. You can also use VPC peering to run hopsworks and EKS in two different VPCs. Make sure to create the peering before starting the hopsworks cluster as it connects to EKS at startup. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Go to the Security Groups section of EC2 in the AWS management console and search for your security group using the id obtained above. Note the VPC ID , you will need it when creating the hopsworks cluster. Then, click on it then go to the Inbound rules tab and click on Edit inbound rules . You should now see the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group.","title":"Step 4: Open Hopsworks required ports on your EKS cluster security group"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-5-allow-hopsworksai-to-delete-ecr-repositories-on-your-behalf","text":"For hopsworks.ai to be able to clean up the ECR repo when terminating your hopsworks cluster, you need to add a new inline policy to the Cross-Account role or user connected to Hopsworks.ai , that you set up when connecting your AWS account to hopsworks.ai . Navigate to AWS management console , then click on Roles or Users depending on which connection method you have used in Hopsworks.ai, and then search for your role or user name and click on it. Go to the Permissions tab, click on Add inline policy and go to the JSON tab. Replace the existing JSON permissions with the JSON permissions below. Click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDeletingECRRepositories\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:DeleteRepository\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] } ] }","title":"Step 5: Allow Hopsworks.ai to delete ECR repositories on your behalf"},{"location":"hopsworksai/aws/eks_ecr_integration/#step-6-create-a-hopsworks-cluster-with-eks-and-ecr-support","text":"In Hopsworks.ai, select Create cluster . Choose the region of your EKS cluster and fill in the name of your S3 bucket , then click Next: Create Hopsworks cluster Choose your preferred SSH key to use with the cluster, then click Next: Choose SSH key Choose the instance profile role that you have created in Step 2 (click on the refresh button if your instance profile is not in the list), then click Next: Choose instance profile role Choose the backup retention period and click Next: Choose the backup retention policy Choose Enabled to enable the use of Amazon EKS and ECR: Choose Enabled Add your EKS cluster name, then click Next: Add EKS cluster name Choose the VPC of your EKS cluster. It's name should have the form eksctl-YOUR-CLUSTER-NAME-cluster . You can also find it using the VPC ID you noted in Step 4 (click on the refresh button if the VPC is not in the list). Then click Next: Choose VPC Choose any of the subnets in the VPC, then click Next. Note Avoid private subnets if you want to enjoy all the hopsworks features . Choose Subnet Choose the security group that you have updated in Step 4 , then click Next: Note Select the Security Group with the same id as in Step 4 and NOT the ones containing ControlPlaneSecurity or ClusterSharedNode in their name. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"Step 6: Create a Hopsworks cluster with EKS and ECR support"},{"location":"hopsworksai/aws/getting_started/","text":"Getting started with Hopsworks.ai (AWS) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's AWS account. Step 1: Connecting your AWS account # Hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This can be either achieved by using AWS cross-account roles or AWS access keys. We strongly recommend the usage of cross-account roles whenever possible due to security reasons. Option 1: Using AWS Cross-Account Roles # To create a cross-account role for Hopsworks.ai, you need our AWS account id and the external id we created for you. You can find this information on the first screen of the cross-account configuration flow. Take note of the account id and external id and go to the Roles section of the IAM service in the AWS Management Console and select Create role . Creating the cross-account role instructions Select Another AWS account as trusted entity and fill in our AWS account id and the external id generated for you: Creating the cross-account role step 1 Go to the last step of the wizard, name the role and create it: Creating the cross-account role step 2 As a next step, you need to create an access policy to give Hopsworks.ai permissions to manage clusters in your organization's AWS account. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . Copy the permission JSON from the instructions: Adding the policy instructions Identify your newly created cross-account role in the Roles section of the IAM service in the AWS Management Console and select Add inline policy : Adding the inline policy step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 Copy the Role ARN from the summary of your cross-account role: Adding the inline policy step 4 Paste the Role ARN into Hopsworks.ai and click on Finish : Saving the cross-account role Option 2: Using AWS Access Keys # You can either create a new IAM user or use an existing IAM user to create access keys for Hopsworks.ai. If you want to create a new IAM user, see Creating an IAM User in Your AWS Account . Warning We recommend using Cross-Account Roles instead of Access Keys whenever possible, see Option 1: Using AWS Cross-Account Roles . Hopsworks.ai requires a set of permissions to be able to launch clusters in your AWS account. The permissions can be granted by attaching an access policy to your IAM user. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . The required permissions are shown in the instructions. Copy them if you want to create a new access policy: Configuring access key instructions Add a new Inline policy to your AWS user: Configuring the access key on AWS step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 In the overview of your IAM user, select Create access key : Configuring the access key on AWS step 2 Copy the Access Key ID and the Secret Access Key : Configuring the access key on AWS step 3 Paste the Access Key ID and the Secret Access Key into Hopsworks.ai and click on Finish : Saving the access key pair Step 2: Creating Instance profile # Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . Hopsworks cluster nodes need access to certain resources such as S3 bucket and CloudWatch. Follow the instructions in this guide to create an IAM instance profile with access to your S3 bucket: Guide When creating the policy, paste the following in the JSON tab. Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }, { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] } Step 3: Creating storage # The Hopsworks clusters deployed by hopsworks.ai store their data in an S3 bucket in your AWS account. To enable this you need to create an S3 bucket and an instance profile to give cluster nodes access to the bucket. Proceed to the S3 Management Console and click on Create bucket : Create an S3 bucket Name your bucket and select the region where your Hopsworks cluster will run. Click on Create bucket at the bottom of the page. Create an S3 bucket Step 4: Create an SSH key # When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair . Step 4.1: Create a new key pair # Proceed to Key pairs in the EC2 console and click on Create key pair Create a key pair Name your key, select the file format you prefer and click on Create key pair . Create a key pair Step 4.2: Import a key pair # Proceed to Key pairs in the EC2 console , click on Action and click on Import key pair Import a key pair Name your key pair, upload your public key and click on Import key pair . Import a key pair Step 5: Deploying a Hopsworks cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enter the name of the S3 bucket (5) you created above in S3 bucket . Note The S3 bucket you are using must be empty. Press Next : Create a Hopsworks cluster, general information Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the Instance Profile that you created above and click on Review and Submit : Choose the instance profile Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster Step 6: Outside Access to the Feature Store # By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store Step 7: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/getting_started/#getting-started-with-hopsworksai-aws","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's AWS account.","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/getting_started/#step-1-connecting-your-aws-account","text":"Hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This can be either achieved by using AWS cross-account roles or AWS access keys. We strongly recommend the usage of cross-account roles whenever possible due to security reasons.","title":"Step 1: Connecting your AWS account"},{"location":"hopsworksai/aws/getting_started/#option-1-using-aws-cross-account-roles","text":"To create a cross-account role for Hopsworks.ai, you need our AWS account id and the external id we created for you. You can find this information on the first screen of the cross-account configuration flow. Take note of the account id and external id and go to the Roles section of the IAM service in the AWS Management Console and select Create role . Creating the cross-account role instructions Select Another AWS account as trusted entity and fill in our AWS account id and the external id generated for you: Creating the cross-account role step 1 Go to the last step of the wizard, name the role and create it: Creating the cross-account role step 2 As a next step, you need to create an access policy to give Hopsworks.ai permissions to manage clusters in your organization's AWS account. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . Copy the permission JSON from the instructions: Adding the policy instructions Identify your newly created cross-account role in the Roles section of the IAM service in the AWS Management Console and select Add inline policy : Adding the inline policy step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 Copy the Role ARN from the summary of your cross-account role: Adding the inline policy step 4 Paste the Role ARN into Hopsworks.ai and click on Finish : Saving the cross-account role","title":"Option 1: Using AWS Cross-Account Roles"},{"location":"hopsworksai/aws/getting_started/#option-2-using-aws-access-keys","text":"You can either create a new IAM user or use an existing IAM user to create access keys for Hopsworks.ai. If you want to create a new IAM user, see Creating an IAM User in Your AWS Account . Warning We recommend using Cross-Account Roles instead of Access Keys whenever possible, see Option 1: Using AWS Cross-Account Roles . Hopsworks.ai requires a set of permissions to be able to launch clusters in your AWS account. The permissions can be granted by attaching an access policy to your IAM user. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . The required permissions are shown in the instructions. Copy them if you want to create a new access policy: Configuring access key instructions Add a new Inline policy to your AWS user: Configuring the access key on AWS step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 In the overview of your IAM user, select Create access key : Configuring the access key on AWS step 2 Copy the Access Key ID and the Secret Access Key : Configuring the access key on AWS step 3 Paste the Access Key ID and the Secret Access Key into Hopsworks.ai and click on Finish : Saving the access key pair","title":"Option 2: Using AWS Access Keys"},{"location":"hopsworksai/aws/getting_started/#step-2-creating-instance-profile","text":"Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . Hopsworks cluster nodes need access to certain resources such as S3 bucket and CloudWatch. Follow the instructions in this guide to create an IAM instance profile with access to your S3 bucket: Guide When creating the policy, paste the following in the JSON tab. Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }, { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Creating Instance profile"},{"location":"hopsworksai/aws/getting_started/#step-3-creating-storage","text":"The Hopsworks clusters deployed by hopsworks.ai store their data in an S3 bucket in your AWS account. To enable this you need to create an S3 bucket and an instance profile to give cluster nodes access to the bucket. Proceed to the S3 Management Console and click on Create bucket : Create an S3 bucket Name your bucket and select the region where your Hopsworks cluster will run. Click on Create bucket at the bottom of the page. Create an S3 bucket","title":"Step 3: Creating storage"},{"location":"hopsworksai/aws/getting_started/#step-4-create-an-ssh-key","text":"When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair .","title":"Step 4: Create an SSH key"},{"location":"hopsworksai/aws/getting_started/#step-41-create-a-new-key-pair","text":"Proceed to Key pairs in the EC2 console and click on Create key pair Create a key pair Name your key, select the file format you prefer and click on Create key pair . Create a key pair","title":"Step 4.1: Create a new key pair"},{"location":"hopsworksai/aws/getting_started/#step-42-import-a-key-pair","text":"Proceed to Key pairs in the EC2 console , click on Action and click on Import key pair Import a key pair Name your key pair, upload your public key and click on Import key pair . Import a key pair","title":"Step 4.2: Import a key pair"},{"location":"hopsworksai/aws/getting_started/#step-5-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enter the name of the S3 bucket (5) you created above in S3 bucket . Note The S3 bucket you are using must be empty. Press Next : Create a Hopsworks cluster, general information Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the Instance Profile that you created above and click on Review and Submit : Choose the instance profile Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 5: Deploying a Hopsworks cluster"},{"location":"hopsworksai/aws/getting_started/#step-6-outside-access-to-the-feature-store","text":"By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store","title":"Step 6: Outside Access to the Feature Store"},{"location":"hopsworksai/aws/getting_started/#step-7-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 7: Next steps"},{"location":"hopsworksai/aws/instance_profile_permissions/","text":"Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }, { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Instance profile permissions"},{"location":"hopsworksai/aws/restrictive_permissions/","text":"Limiting AWS permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing Hopsworks.ai to only access resources in a specific VPC. Limiting the cross-account role permissions # Step 1: Create a VPC # To restrict Hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . The option VPC with a Single Public Subnet from the Launch VPC Wizard should work out of the box. Alternatively, an existing VPC such as the default VPC can be used and Hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC After you have created the VPC either Create a Security Group or use VPC's default. Note The Security Group and/or Network ACLs need to be configured so that at least port 80 is reachable from the internet otherwise you will have to use self signed certificate in your Hopsworks cluster. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Step 2: Create an instance profile # You need to create an instance profile that will identify all instances started by Hopsworks.ai. Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile. Step 3: Set permissions of the cross-account role # During the account setup for Hopsworks.ai, you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in Hopsworks.ai. Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in Hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors. Step 5: Supporting multiple VPCs # The policy can be extended to give Hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values . Backup permissions # The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , } Other removable permissions # The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } } Limiting the instance profile permissions # Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , CloudWatch Logs # Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } Upgrade permissions # Removing upgrade permissions # If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the upgrade permissions statement from the instance profile that you have created here . For this remove the following statement from your instance profile { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } Limiting upgrade permissions # You can use tags to restrict the upgrade permissions to only the resources created for your cluster. For this attach a tag to your cluster during the cluster creation . Then, replace the upgrade permissions with the following policy instead. First you need to replace REGION and ACCOUNT with your region and account where you run your cluster, then replace HEAD_NODE_INSTANCE_ID with your aws instance id of the head node and HEAD_NODE_VOLUME_ID with the volume id attached to the head node, and finally replace the TAG_KEY and TAG_VALUE with your the tag name and value that is used with your cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Sid\" : \"AllowAttachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowAttachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowDetachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowDetachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowModifyInstanceAttributeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:ModifyInstanceAttribute\" , \"Resource\" :[ \"arn:aws:ec2:REGION:ACCOUNT:instance/HEAD_NODE_INSTANCE_ID\" , \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" ] }, { \"Sid\" : \"AllowDescribeVolumesForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DescribeVolumes\" , \"Resource\" : \"*\" } ] }","title":"Limiting AWS permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-aws-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing Hopsworks.ai to only access resources in a specific VPC.","title":"Limiting AWS permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#step-1-create-a-vpc","text":"To restrict Hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . The option VPC with a Single Public Subnet from the Launch VPC Wizard should work out of the box. Alternatively, an existing VPC such as the default VPC can be used and Hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC After you have created the VPC either Create a Security Group or use VPC's default. Note The Security Group and/or Network ACLs need to be configured so that at least port 80 is reachable from the internet otherwise you will have to use self signed certificate in your Hopsworks cluster. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443.","title":"Step 1: Create a VPC"},{"location":"hopsworksai/aws/restrictive_permissions/#step-2-create-an-instance-profile","text":"You need to create an instance profile that will identify all instances started by Hopsworks.ai. Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile.","title":"Step 2: Create an instance profile"},{"location":"hopsworksai/aws/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for Hopsworks.ai, you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in Hopsworks.ai. Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 3: Set permissions of the cross-account role"},{"location":"hopsworksai/aws/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in Hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors.","title":"Step 4: Create your Hopsworks instance"},{"location":"hopsworksai/aws/restrictive_permissions/#step-5-supporting-multiple-vpcs","text":"The policy can be extended to give Hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values .","title":"Step 5: Supporting multiple VPCs"},{"location":"hopsworksai/aws/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , }","title":"Backup permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#other-removable-permissions","text":"The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }","title":"Other removable permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-the-instance-profile-permissions","text":"","title":"Limiting the instance profile permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" ,","title":"Backups"},{"location":"hopsworksai/aws/restrictive_permissions/#cloudwatch-logs","text":"Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }","title":"CloudWatch Logs"},{"location":"hopsworksai/aws/restrictive_permissions/#upgrade-permissions","text":"","title":"Upgrade permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#removing-upgrade-permissions","text":"If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the upgrade permissions statement from the instance profile that you have created here . For this remove the following statement from your instance profile { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" }","title":"Removing upgrade permissions"},{"location":"hopsworksai/aws/restrictive_permissions/#limiting-upgrade-permissions","text":"You can use tags to restrict the upgrade permissions to only the resources created for your cluster. For this attach a tag to your cluster during the cluster creation . Then, replace the upgrade permissions with the following policy instead. First you need to replace REGION and ACCOUNT with your region and account where you run your cluster, then replace HEAD_NODE_INSTANCE_ID with your aws instance id of the head node and HEAD_NODE_VOLUME_ID with the volume id attached to the head node, and finally replace the TAG_KEY and TAG_VALUE with your the tag name and value that is used with your cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Sid\" : \"AllowAttachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowAttachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowDetachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowDetachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowModifyInstanceAttributeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:ModifyInstanceAttribute\" , \"Resource\" :[ \"arn:aws:ec2:REGION:ACCOUNT:instance/HEAD_NODE_INSTANCE_ID\" , \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" ] }, { \"Sid\" : \"AllowDescribeVolumesForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DescribeVolumes\" , \"Resource\" : \"*\" } ] }","title":"Limiting upgrade permissions"},{"location":"hopsworksai/aws/upgrade/","text":"Upgrade existing clusters on Hopsworks.ai (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available Step 1: Stop your cluster # You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available Step 2: Add upgrade permissions to your instance profile # Note You can skip this step if you already have the following permissions in your instance profile: [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ] We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Note You can restrict the upgrade permissions given to your instance profile. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] } Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Misconfigured upgrade permissions # During the upgrade process, Hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Upgrade existing clusters on Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/upgrade/#upgrade-existing-clusters-on-hopsworksai-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available","title":"Upgrade existing clusters on Hopsworks.ai (AWS)"},{"location":"hopsworksai/aws/upgrade/#step-1-stop-your-cluster","text":"You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available","title":"Step 1: Stop your cluster"},{"location":"hopsworksai/aws/upgrade/#step-2-add-upgrade-permissions-to-your-instance-profile","text":"Note You can skip this step if you already have the following permissions in your instance profile: [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ] We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Note You can restrict the upgrade permissions given to your instance profile. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add upgrade permissions to your instance profile"},{"location":"hopsworksai/aws/upgrade/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 3: Run the upgrade process"},{"location":"hopsworksai/aws/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"hopsworksai/aws/upgrade/#error-1-misconfigured-upgrade-permissions","text":"During the upgrade process, Hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running","title":"Error 1: Misconfigured upgrade permissions"},{"location":"hopsworksai/aws/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Error 2: Upgrade process error"},{"location":"hopsworksai/azure/aks_acr_integration/","text":"Integration with Azure AKS and ACR # This guide shows how to create a cluster in hopsworks.ai with integrated support for Azure Kubernetes Service (AKS) and Azure Container Registry (ACR). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. Hopsworks AKS and ACR integration have four requirements: A virtual network with access to AKS pods and the AKS API servers One Azure container registry configured in your account One AKS cluster Permissions to the ACR and AKS attached to a user-managed identity This guide provides an example setup with a private AKS cluster and public ACR. Note A public AKS cluster means the Kubernetes API server is accessible outside the virtual network it is deployed in. Similarly, a public ACR is accessible through the internet. User assigned managed identity (managed identity) # Note A user assigned managed identity (managed identity) can be created at the subscription level or to a specific resource group in a subscription. The managed identity is attached to the virtual machines that run inside your subscription (or resource group). Hence, the permissions only apply to services that run within your subscription (or resource group). The AKS and ACR integration requires some permissions to be attached to the managed identity used by the Hopsworks cluster. If you have already created a user assigned managed identity for the storage continue to Add role assignment to the managed identity using this identity. To set up the managed identity, go to the resource group where you will add the managed identity - this should be the same resource group you will deploy Hopsworks in. Click on the Add button. In the search dialog, enter \"user assigned managed identity\" . Click on Create . Then give a name to the managed identity and make sure that it is in the Region where you will deploy your cluster. Click on Review + create , and click on Create . Add role assignment to the managed identity # Go to the managed identity created above. Click on Azure role assignments in the left column. Click on Add role assignment . For the Scope select Resource group or Subscription depending on your preference. Select the Role AcrPull and click on Save . Repeat the same operation with the following roles: AcrPull AcrPush AcrDelete Azure Kubernetes Service Cluster User Role Warning You will also need to attach storage access permissions to the managed identity, see Creating and configuring a storage Once finished the role assignments should look similar to the picture below. AKS permissions Private AKS cluster and public ACR # This guide will step through setting up a private AKS cluster and a public ACR. Step 1: Create an AKS cluster # Go to Kubernetes services in the azure portal and click Add then Add Kubernetes cluster . Place the Kubernetes cluster in the same resource group and region as the Hopsworks cluster and choose a name for the Kubernetes cluster. AKS general configuration Next, click on the Authentication tab and verify the settings are as follow: Authentication method: System-assigned managed identity Role-based access control (RBAC): Enabled AKS-managed Azure Active Directory: Disabled Note Currently, AKS is only supported through managed identities. Contact the Logical Clocks sales team if you have a self-managed Kubernetes cluster. AKS authencation configuration Next, go to the networking tab and check Azure CNI . The portal will automatically fill in the IP address ranges for the Kubernetes virtual network. Take note of the virtual network name that is created, in this example the virtual network name was hopsworksstagevnet154. Lastly, check the Enable private cluster option. AKS network configuration Next, go to the Integrations tab. Under container registry click Create new. AKS create ACR Choose a name for the registry and select premium for the SKU. Then press OK . ACR configuration Next press Review + create , then click Create . To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, go to the registry you created. Go to the Retention (preview) tab and set Status from disabled to enabled . Set the retention policy for 7 days as in the figure below, then press save . ACR retention policy Step 2: create a virtual network for the Hopsworks cluster # Because the Kubernetes API service is private the Hopsworks cluster must be able to reach it over a private network. There are two options to integrate with a private AKS cluster. The first option ( A ) is to put the Hopsworks cluster in a pre-defined virtual network with a peering setup to the Kubernetes network. The second option ( B ) is to create a subnet inside the Kubernetes virtual network where the Hopsworks cluster will be placed. Option A : Peering setup # To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. Go to virtual networks and press create. Choose a name for the new virtual network and select the same resource group you are planning to use for your Hopsworks cluster. Next, go to the IP Addresses tab. Create an address space that does not overlap with the address space in the Kubernetes network. In the previous example, the automatically created Kubernetes network used the address space 10.0.0.0/8 . Hence, the address space 172.18.0.0/16 can safely be used. Next click Review + Create , then Create . Next, go to the created virtual network and go to the Peerings tab. Then click Add . Virtual network peering Choose a name for the peering link. Check the Traffic to remote virtual network as Allow , and Traffic forwarded from remote virtual network to Block . Virtual network peering configuration For the virtual network select the virtual network which was created by AKS, in our example this was hopsworksstagevnet154 . Then press Add . Virtual network peering configuration continuation The last step is to set up a DNS private link to be able to use DNS resolution for the Kubernetes API servers. Go to resource groups in the Azure portal and find the resource group of the Kubernetes cluster. This will be in the form of MC_ in this example it was MC_hopsworks-stage_hopsworks-aks_northeurope . Open the resource group and click on the DNS zone. Private DNS link setup In the left plane there is a tab called Virtual network links , click on the tab. Next press Add . Private DNS link configuration Choose a name for the private link and select the virtual network you will use for the Hopsworks cluster, then press OK. Private DNS link configuration The setup is now finalized and you can create the Hopsworks cluster. Option B : Subnet in AKS network # With this setup, the Hopsworks cluster will reside in the same virtual network as the AKS cluster. The difference is that a new subnet in the virtual network will be used for the Hopsworks cluster. To set up the subnet, first, go to the virtual network that was created by AKS. In our example, this was hopsworksstagevnet154. Next, go to the subnets tab. AKS subnet setup Press + Subnet . Choose a name for the subnet, for example, \"hopsworks\" and an IP range that does not overlap with the Kubernetes network. Then save. AKS subnet setup Create the Hopsworks cluster # This step assumes you are creating your Hopsworks cluster using hopsworks.ai. The AKS configuration can be set under the Managed containers tab. Set Use Azure AKS and Azure ACR as enabled. Two new fields will pop up. Fill them with the name of the container registry and the AKS you created above. In the previous example, we created an ACR with the name hopsworksaks and an AKS cluster with the name hopsaks-cluster . Hence, the configuration should look similar to the picture below Hopsworks AKS configuration In the virtual network tab, you have to select either the virtual network you created for the peering setup or the Kubernetes virtual network depending on which approach you choose. Under the subnet tab, you have to choose the default subnet if you choose the peering approach or the subnet you created if you choose to create a new subnet inside the AKS virtual network.","title":"Integration with Azure AKS and ACR"},{"location":"hopsworksai/azure/aks_acr_integration/#integration-with-azure-aks-and-acr","text":"This guide shows how to create a cluster in hopsworks.ai with integrated support for Azure Kubernetes Service (AKS) and Azure Container Registry (ACR). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. Hopsworks AKS and ACR integration have four requirements: A virtual network with access to AKS pods and the AKS API servers One Azure container registry configured in your account One AKS cluster Permissions to the ACR and AKS attached to a user-managed identity This guide provides an example setup with a private AKS cluster and public ACR. Note A public AKS cluster means the Kubernetes API server is accessible outside the virtual network it is deployed in. Similarly, a public ACR is accessible through the internet.","title":"Integration with Azure AKS and ACR"},{"location":"hopsworksai/azure/aks_acr_integration/#user-assigned-managed-identity-managed-identity","text":"Note A user assigned managed identity (managed identity) can be created at the subscription level or to a specific resource group in a subscription. The managed identity is attached to the virtual machines that run inside your subscription (or resource group). Hence, the permissions only apply to services that run within your subscription (or resource group). The AKS and ACR integration requires some permissions to be attached to the managed identity used by the Hopsworks cluster. If you have already created a user assigned managed identity for the storage continue to Add role assignment to the managed identity using this identity. To set up the managed identity, go to the resource group where you will add the managed identity - this should be the same resource group you will deploy Hopsworks in. Click on the Add button. In the search dialog, enter \"user assigned managed identity\" . Click on Create . Then give a name to the managed identity and make sure that it is in the Region where you will deploy your cluster. Click on Review + create , and click on Create .","title":"User assigned managed identity (managed identity)"},{"location":"hopsworksai/azure/aks_acr_integration/#add-role-assignment-to-the-managed-identity","text":"Go to the managed identity created above. Click on Azure role assignments in the left column. Click on Add role assignment . For the Scope select Resource group or Subscription depending on your preference. Select the Role AcrPull and click on Save . Repeat the same operation with the following roles: AcrPull AcrPush AcrDelete Azure Kubernetes Service Cluster User Role Warning You will also need to attach storage access permissions to the managed identity, see Creating and configuring a storage Once finished the role assignments should look similar to the picture below. AKS permissions","title":"Add role assignment to the managed identity"},{"location":"hopsworksai/azure/aks_acr_integration/#private-aks-cluster-and-public-acr","text":"This guide will step through setting up a private AKS cluster and a public ACR.","title":"Private AKS cluster and public ACR"},{"location":"hopsworksai/azure/aks_acr_integration/#step-1-create-an-aks-cluster","text":"Go to Kubernetes services in the azure portal and click Add then Add Kubernetes cluster . Place the Kubernetes cluster in the same resource group and region as the Hopsworks cluster and choose a name for the Kubernetes cluster. AKS general configuration Next, click on the Authentication tab and verify the settings are as follow: Authentication method: System-assigned managed identity Role-based access control (RBAC): Enabled AKS-managed Azure Active Directory: Disabled Note Currently, AKS is only supported through managed identities. Contact the Logical Clocks sales team if you have a self-managed Kubernetes cluster. AKS authencation configuration Next, go to the networking tab and check Azure CNI . The portal will automatically fill in the IP address ranges for the Kubernetes virtual network. Take note of the virtual network name that is created, in this example the virtual network name was hopsworksstagevnet154. Lastly, check the Enable private cluster option. AKS network configuration Next, go to the Integrations tab. Under container registry click Create new. AKS create ACR Choose a name for the registry and select premium for the SKU. Then press OK . ACR configuration Next press Review + create , then click Create . To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, go to the registry you created. Go to the Retention (preview) tab and set Status from disabled to enabled . Set the retention policy for 7 days as in the figure below, then press save . ACR retention policy","title":"Step 1: Create an AKS cluster"},{"location":"hopsworksai/azure/aks_acr_integration/#step-2-create-a-virtual-network-for-the-hopsworks-cluster","text":"Because the Kubernetes API service is private the Hopsworks cluster must be able to reach it over a private network. There are two options to integrate with a private AKS cluster. The first option ( A ) is to put the Hopsworks cluster in a pre-defined virtual network with a peering setup to the Kubernetes network. The second option ( B ) is to create a subnet inside the Kubernetes virtual network where the Hopsworks cluster will be placed.","title":"Step 2: create a virtual network for the Hopsworks cluster"},{"location":"hopsworksai/azure/aks_acr_integration/#option-a-peering-setup","text":"To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. Go to virtual networks and press create. Choose a name for the new virtual network and select the same resource group you are planning to use for your Hopsworks cluster. Next, go to the IP Addresses tab. Create an address space that does not overlap with the address space in the Kubernetes network. In the previous example, the automatically created Kubernetes network used the address space 10.0.0.0/8 . Hence, the address space 172.18.0.0/16 can safely be used. Next click Review + Create , then Create . Next, go to the created virtual network and go to the Peerings tab. Then click Add . Virtual network peering Choose a name for the peering link. Check the Traffic to remote virtual network as Allow , and Traffic forwarded from remote virtual network to Block . Virtual network peering configuration For the virtual network select the virtual network which was created by AKS, in our example this was hopsworksstagevnet154 . Then press Add . Virtual network peering configuration continuation The last step is to set up a DNS private link to be able to use DNS resolution for the Kubernetes API servers. Go to resource groups in the Azure portal and find the resource group of the Kubernetes cluster. This will be in the form of MC_ in this example it was MC_hopsworks-stage_hopsworks-aks_northeurope . Open the resource group and click on the DNS zone. Private DNS link setup In the left plane there is a tab called Virtual network links , click on the tab. Next press Add . Private DNS link configuration Choose a name for the private link and select the virtual network you will use for the Hopsworks cluster, then press OK. Private DNS link configuration The setup is now finalized and you can create the Hopsworks cluster.","title":"Option A: Peering setup"},{"location":"hopsworksai/azure/aks_acr_integration/#option-b-subnet-in-aks-network","text":"With this setup, the Hopsworks cluster will reside in the same virtual network as the AKS cluster. The difference is that a new subnet in the virtual network will be used for the Hopsworks cluster. To set up the subnet, first, go to the virtual network that was created by AKS. In our example, this was hopsworksstagevnet154. Next, go to the subnets tab. AKS subnet setup Press + Subnet . Choose a name for the subnet, for example, \"hopsworks\" and an IP range that does not overlap with the Kubernetes network. Then save. AKS subnet setup","title":"Option B: Subnet in AKS network"},{"location":"hopsworksai/azure/aks_acr_integration/#create-the-hopsworks-cluster","text":"This step assumes you are creating your Hopsworks cluster using hopsworks.ai. The AKS configuration can be set under the Managed containers tab. Set Use Azure AKS and Azure ACR as enabled. Two new fields will pop up. Fill them with the name of the container registry and the AKS you created above. In the previous example, we created an ACR with the name hopsworksaks and an AKS cluster with the name hopsaks-cluster . Hence, the configuration should look similar to the picture below Hopsworks AKS configuration In the virtual network tab, you have to select either the virtual network you created for the peering setup or the Kubernetes virtual network depending on which approach you choose. Under the subnet tab, you have to choose the default subnet if you choose the peering approach or the subnet you created if you choose to create a new subnet inside the AKS virtual network.","title":"Create the Hopsworks cluster"},{"location":"hopsworksai/azure/cluster_creation/","text":"Getting started with Hopsworks.ai (Azure) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the Resource Group (1) you want to use. Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by Hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (6) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. General configuration Step 3 workers configuration # In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand. Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select a SSH key # When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key Step 5 select the User assigned managed identity: # In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity Step 6 set the backup retention policy: # Note This step is only accessible to enterprise users. To back up the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 7 Virtual network selection # In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network Step 8 Subnet selection # If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step Hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet Step 9 Network Security group selection # In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let Hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Choose security group Step 10 User management selection # In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 12 Managed RonDB # Hopsworks.ai users have the option to create a cluster with dedicated VMs running RonDB . For details on how to configure RonDB check our guide here . Enable Managed RonDB If you need this feature and don't see this step please contact us . Step 13 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 14 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 15 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/cluster_creation/#getting-started-with-hopsworksai-azure","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"hopsworksai/azure/cluster_creation/#step-2-setting-the-general-information","text":"Select the Resource Group (1) you want to use. Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by Hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (6) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. General configuration","title":"Step 2 setting the General information"},{"location":"hopsworksai/azure/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand.","title":"Step 3 workers configuration"},{"location":"hopsworksai/azure/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"hopsworksai/azure/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"hopsworksai/azure/cluster_creation/#step-4-select-a-ssh-key","text":"When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key","title":"Step 4 select a SSH key"},{"location":"hopsworksai/azure/cluster_creation/#step-5-select-the-user-assigned-managed-identity","text":"In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity","title":"Step 5 select the User assigned managed identity:"},{"location":"hopsworksai/azure/cluster_creation/#step-6-set-the-backup-retention-policy","text":"Note This step is only accessible to enterprise users. To back up the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 6 set the backup retention policy:"},{"location":"hopsworksai/azure/cluster_creation/#step-7-virtual-network-selection","text":"In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network","title":"Step 7 Virtual network selection"},{"location":"hopsworksai/azure/cluster_creation/#step-8-subnet-selection","text":"If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step Hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet","title":"Step 8 Subnet selection"},{"location":"hopsworksai/azure/cluster_creation/#step-9-network-security-group-selection","text":"In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let Hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Choose security group","title":"Step 9 Network Security group selection"},{"location":"hopsworksai/azure/cluster_creation/#step-10-user-management-selection","text":"In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 10 User management selection"},{"location":"hopsworksai/azure/cluster_creation/#step-12-managed-rondb","text":"Hopsworks.ai users have the option to create a cluster with dedicated VMs running RonDB . For details on how to configure RonDB check our guide here . Enable Managed RonDB If you need this feature and don't see this step please contact us .","title":"Step 12 Managed RonDB"},{"location":"hopsworksai/azure/cluster_creation/#step-13-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 13 add tags to your instances."},{"location":"hopsworksai/azure/cluster_creation/#step-14-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 14 add an init script to your instances."},{"location":"hopsworksai/azure/cluster_creation/#step-15-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 15 Review and create"},{"location":"hopsworksai/azure/getting_started/","text":"Getting started with Hopsworks.ai (Azure) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Azure account. Step 1: Connecting your Azure account # Hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for Hopsworks.ai granting access to either a subscription or resource group. Step 1.0: Prerequisite # For Hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. You can verify that they are registered by going to your subscription in the Azure portal and click on Resource providers . If one of the resource providers is not registered select it and click on Register . Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y Step 1.1: Creating a service principal for Hopsworks.ai # On Hopsworks.ai, go to Settings/Cloud Accounts and choose to Configure Azure: Cloud account settings Select Add subscription key : Add subscription keys The Azure account configuration will show you the required steps and permissions. Ensure that you have the Azure CLI installed Install the Azure CLI and are logged in Sign in with Azure CLI . Copy the Azure CLI command from the first step and open a terminal: Connect your Azure Account Paste the command into the terminal and execute it: Add service principal At this point, you might get the following error message. This means that your Azure user does not have sufficient permissions to add the service principal. In this case, please ask your Azure administrator to add it for you or give you the required permissions. Error az ad sp create --id d4abcc44-2c40-40bd-9bba-986df591c28f When using this permission, the backing application of the service principal being created must in the local tenant. Step 1.2: Creating a custom role for Hopsworks.ai # Proceed to the Azure Portal and open either a Subscription or Resource Group that you want to use for Hopsworks.ai. Click on Access control (IAM) Select Add and choose Add custom role . Note Granting access to a Subscription will grant access to all Resource Groups in that Subscription . If you are uncertain if that is what you want, then start with a Resource Group . Add custom role Name the role and proceed to Assignable scopes : Name custom role Ensure the scope is set to the Subscription or Resource Group you want to use. You can change it here if required. Proceed to the JSON tab: Review assignable scope Select Edit and replace the actions part of the JSON with the one from Hopsworks.ai Azure account configuration workflow: Hopsworks.ai permission list Note If the access rights provided by Hopsworks.ai Azure account configuration workflow are too permissive, you can go to Limiting Azure permissions for more details on how to limit the permissions. Press Save , proceed to Review + create and create the role: Update permission JSON Step 1.3: Assigning the custom role to Hopsworks.ai # Back in the Subscription or Resource Group , in Access control (IAM) , select Add and choose Add role assignment : Add role assignment Choose the custom role you just created, select User, group, or service principal to Assign access to and select the hopsworks.ai service principal. Press Save : Configure Hopsworks.ai as role assignment Go back to the Hopsworks.ai Azure account configuration workflow and proceed to the next step. Copy the CLI command shown: Configure subscription and tenant id Paste the CLI command into your terminal and execute it. Note that you might have multiple entries listed here. If so, ensure that you pick the subscription that you want to use. Show subscription and tenant id Copy the value of id and paste it into the Subscription id field on Hopsworks.ai. Go back to the terminal and copy the value of tenantId . Ensure to NOT use the tenantId under managedByTenants . Paste the value into the Tenant ID field on Hopsworks.ai and press Finish . Congratulations, you have successfully connected you Azure account to Hopsworks.ai. Store subscription and tenant id Step 2: Creating and configuring a storage # Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . The Hopsworks clusters deployed by hopsworks.ai store their data in a container in your Azure account. To enable this you need to perform the following operations Create a restrictive role to limit access to the storage account Create a User Assigned Managed Identity Create a storage account and give Hopsworks clusters access to the storage using the restrictive role Step 2.1: Creating a Restrictive Role for Accessing Storage # Similarly to Step 1.2 create a new role named Hopsworks Storage Role . Add the following permissions to the role \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" , ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ] Note Some of these permissions can be removed at the cost of Hopsworks features, see Limiting Azure permissions for more details. Step 2.2: Creating a User Assigned Managed Identity # Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for User Assigned Managed Identity and click on it. Search User Assigned Managed Identity Click on Create . Then, select the Location you want to use and name the identity. Click on Review + create . Finally click on Create . Create a User Assigned Managed Identity Step 2.3: Creating a Storage account # Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for Storage account and click on it. Search Storage Account Identity Click on Create , name your storage account, select the Location you want to use and click on Review + create . Finally click on Create . Create a Storage Account Step 2.4: Give the Managed Identity access to the storage # Proceed to the Storage Account you just created and click on Access Control (IAM) (1). Click on Add (2), then click on Add role assignment (3). In Role select Hopsworks Storage Role (4). In Assign access to select User assigned managed identity (5). Select the identity you created in step 2.1 (6). Click on Save (7). Add role assignment to storage Step 3: Adding a ssh key to your resource group # When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. For this purpose you need to add a ssh key to your resource group. Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for SSH Key and click on it. Click on Create. Then, name your key pair and choose between Generate a new key pair and Upload existing public key . Click on Review + create . Finally click on Create . Add to resource group Step 4: Deploying a Hopsworks cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Note It is possible to add or remove workers once the cluster is running. Select the storage account (6) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above and click on Review and Create : Choose the User assigned managed identity Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster Step 5: Outside Access to the Feature Store # By default, only the Hopsworks REST API (and UI) is accessible by clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to Services tab, selecting a service and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store Step 6: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/getting_started/#getting-started-with-hopsworksai-azure","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Azure account.","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/getting_started/#step-1-connecting-your-azure-account","text":"Hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for Hopsworks.ai granting access to either a subscription or resource group.","title":"Step 1: Connecting your Azure account"},{"location":"hopsworksai/azure/getting_started/#step-10-prerequisite","text":"For Hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. You can verify that they are registered by going to your subscription in the Azure portal and click on Resource providers . If one of the resource providers is not registered select it and click on Register . Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y","title":"Step 1.0: Prerequisite"},{"location":"hopsworksai/azure/getting_started/#step-11-creating-a-service-principal-for-hopsworksai","text":"On Hopsworks.ai, go to Settings/Cloud Accounts and choose to Configure Azure: Cloud account settings Select Add subscription key : Add subscription keys The Azure account configuration will show you the required steps and permissions. Ensure that you have the Azure CLI installed Install the Azure CLI and are logged in Sign in with Azure CLI . Copy the Azure CLI command from the first step and open a terminal: Connect your Azure Account Paste the command into the terminal and execute it: Add service principal At this point, you might get the following error message. This means that your Azure user does not have sufficient permissions to add the service principal. In this case, please ask your Azure administrator to add it for you or give you the required permissions. Error az ad sp create --id d4abcc44-2c40-40bd-9bba-986df591c28f When using this permission, the backing application of the service principal being created must in the local tenant.","title":"Step 1.1: Creating a service principal for Hopsworks.ai"},{"location":"hopsworksai/azure/getting_started/#step-12-creating-a-custom-role-for-hopsworksai","text":"Proceed to the Azure Portal and open either a Subscription or Resource Group that you want to use for Hopsworks.ai. Click on Access control (IAM) Select Add and choose Add custom role . Note Granting access to a Subscription will grant access to all Resource Groups in that Subscription . If you are uncertain if that is what you want, then start with a Resource Group . Add custom role Name the role and proceed to Assignable scopes : Name custom role Ensure the scope is set to the Subscription or Resource Group you want to use. You can change it here if required. Proceed to the JSON tab: Review assignable scope Select Edit and replace the actions part of the JSON with the one from Hopsworks.ai Azure account configuration workflow: Hopsworks.ai permission list Note If the access rights provided by Hopsworks.ai Azure account configuration workflow are too permissive, you can go to Limiting Azure permissions for more details on how to limit the permissions. Press Save , proceed to Review + create and create the role: Update permission JSON","title":"Step 1.2: Creating a custom role for Hopsworks.ai"},{"location":"hopsworksai/azure/getting_started/#step-13-assigning-the-custom-role-to-hopsworksai","text":"Back in the Subscription or Resource Group , in Access control (IAM) , select Add and choose Add role assignment : Add role assignment Choose the custom role you just created, select User, group, or service principal to Assign access to and select the hopsworks.ai service principal. Press Save : Configure Hopsworks.ai as role assignment Go back to the Hopsworks.ai Azure account configuration workflow and proceed to the next step. Copy the CLI command shown: Configure subscription and tenant id Paste the CLI command into your terminal and execute it. Note that you might have multiple entries listed here. If so, ensure that you pick the subscription that you want to use. Show subscription and tenant id Copy the value of id and paste it into the Subscription id field on Hopsworks.ai. Go back to the terminal and copy the value of tenantId . Ensure to NOT use the tenantId under managedByTenants . Paste the value into the Tenant ID field on Hopsworks.ai and press Finish . Congratulations, you have successfully connected you Azure account to Hopsworks.ai. Store subscription and tenant id","title":"Step 1.3: Assigning the custom role to Hopsworks.ai"},{"location":"hopsworksai/azure/getting_started/#step-2-creating-and-configuring-a-storage","text":"Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . The Hopsworks clusters deployed by hopsworks.ai store their data in a container in your Azure account. To enable this you need to perform the following operations Create a restrictive role to limit access to the storage account Create a User Assigned Managed Identity Create a storage account and give Hopsworks clusters access to the storage using the restrictive role","title":"Step 2: Creating and configuring a storage"},{"location":"hopsworksai/azure/getting_started/#step-21-creating-a-restrictive-role-for-accessing-storage","text":"Similarly to Step 1.2 create a new role named Hopsworks Storage Role . Add the following permissions to the role \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" , ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ] Note Some of these permissions can be removed at the cost of Hopsworks features, see Limiting Azure permissions for more details.","title":"Step 2.1: Creating a Restrictive Role for Accessing Storage"},{"location":"hopsworksai/azure/getting_started/#step-22-creating-a-user-assigned-managed-identity","text":"Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for User Assigned Managed Identity and click on it. Search User Assigned Managed Identity Click on Create . Then, select the Location you want to use and name the identity. Click on Review + create . Finally click on Create . Create a User Assigned Managed Identity","title":"Step 2.2: Creating a User Assigned Managed Identity"},{"location":"hopsworksai/azure/getting_started/#step-23-creating-a-storage-account","text":"Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for Storage account and click on it. Search Storage Account Identity Click on Create , name your storage account, select the Location you want to use and click on Review + create . Finally click on Create . Create a Storage Account","title":"Step 2.3: Creating a Storage account"},{"location":"hopsworksai/azure/getting_started/#step-24-give-the-managed-identity-access-to-the-storage","text":"Proceed to the Storage Account you just created and click on Access Control (IAM) (1). Click on Add (2), then click on Add role assignment (3). In Role select Hopsworks Storage Role (4). In Assign access to select User assigned managed identity (5). Select the identity you created in step 2.1 (6). Click on Save (7). Add role assignment to storage","title":"Step 2.4: Give the Managed Identity access to the storage"},{"location":"hopsworksai/azure/getting_started/#step-3-adding-a-ssh-key-to-your-resource-group","text":"When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. For this purpose you need to add a ssh key to your resource group. Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for SSH Key and click on it. Click on Create. Then, name your key pair and choose between Generate a new key pair and Upload existing public key . Click on Review + create . Finally click on Create . Add to resource group","title":"Step 3: Adding a ssh key to your resource group"},{"location":"hopsworksai/azure/getting_started/#step-4-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Note It is possible to add or remove workers once the cluster is running. Select the storage account (6) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above and click on Review and Create : Choose the User assigned managed identity Note If you are an enterprise user you will have one more step before being able to click on Review and Create . In this step, you will be asked to set the backups retention policy. More details about this step here Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster with the username and password provided. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster","title":"Step 4: Deploying a Hopsworks cluster"},{"location":"hopsworksai/azure/getting_started/#step-5-outside-access-to-the-feature-store","text":"By default, only the Hopsworks REST API (and UI) is accessible by clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to Services tab, selecting a service and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store","title":"Step 5: Outside Access to the Feature Store"},{"location":"hopsworksai/azure/getting_started/#step-6-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 6: Next steps"},{"location":"hopsworksai/azure/restrictive_permissions/","text":"Limiting Azure permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege. Limiting the cross-account role permissions # Step 1: Create a virtual network and subnet # To restrict Hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps. Step 2: Create a network security group # To restrict Hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For Hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Step 3: Set permissions of the cross-account role # During the account setup for Hopsworks.ai, you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ] Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in Hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration. Backup permissions # The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ] Other removable permissions # The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in Azure. \"actions\" : [ \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , ] The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ] Limiting the User Assigned Managed Identity permissions # Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this remove the following actions from your user assigned managed identity : \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/write\" ] Upgrades # If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the permissions required for upgrade from the custom role that you have created here . For this remove the following actions from your custom role: \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ]","title":"Limiting Azure permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#limiting-azure-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege.","title":"Limiting Azure permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#step-1-create-a-virtual-network-and-subnet","text":"To restrict Hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps.","title":"Step 1: Create a virtual network and subnet"},{"location":"hopsworksai/azure/restrictive_permissions/#step-2-create-a-network-security-group","text":"To restrict Hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For Hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443.","title":"Step 2: Create a network security group"},{"location":"hopsworksai/azure/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for Hopsworks.ai, you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ]","title":"Step 3: Set permissions of the cross-account role"},{"location":"hopsworksai/azure/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in Hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration.","title":"Step 4: Create your Hopsworks instance"},{"location":"hopsworksai/azure/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Backup permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#other-removable-permissions","text":"The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in Azure. \"actions\" : [ \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , ] The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Other removable permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#limiting-the-user-assigned-managed-identity-permissions","text":"","title":"Limiting the User Assigned Managed Identity permissions"},{"location":"hopsworksai/azure/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this remove the following actions from your user assigned managed identity : \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/write\" ]","title":"Backups"},{"location":"hopsworksai/azure/restrictive_permissions/#upgrades","text":"If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the permissions required for upgrade from the custom role that you have created here . For this remove the following actions from your custom role: \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ]","title":"Upgrades"},{"location":"hopsworksai/azure/upgrade/","text":"Upgrade existing clusters on Hopsworks.ai (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available Step 1: Stop your cluster # You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available Step 2: Add upgrade permissions to your user assigned managed identity # Note You can skip this step if you already have the following permissions in your user assigned managed identity : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ] Make sure that the scope of these permissions is your resource group. We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster Step 2.1: Add custom role for upgrade permissions # Once you get the names of the resource group and user-assigned managed identity, follow the same steps as in getting started to add a custom role . First, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade Step 2.2: Assign the custom role to your user-assigned managed identity # Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect. Step 3: Add disk read permissions to your role connected to Hopsworks.ai # We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Step 4: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing permissions error # If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Upgrade existing clusters on Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/upgrade/#upgrade-existing-clusters-on-hopsworksai-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. First, a notification will appear on the top of your cluster when a new version is available as shown in the figure below: A new Hopsworks version is available","title":"Upgrade existing clusters on Hopsworks.ai (Azure)"},{"location":"hopsworksai/azure/upgrade/#step-1-stop-your-cluster","text":"You need to Stop your cluster to start the upgrade process. Once your cluster is stopped, the Upgrade button will appear as shown below: A new Hopsworks version is available","title":"Step 1: Stop your cluster"},{"location":"hopsworksai/azure/upgrade/#step-2-add-upgrade-permissions-to-your-user-assigned-managed-identity","text":"Note You can skip this step if you already have the following permissions in your user assigned managed identity : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ] Make sure that the scope of these permissions is your resource group. We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster","title":"Step 2: Add upgrade permissions to your user assigned managed identity"},{"location":"hopsworksai/azure/upgrade/#step-21-add-custom-role-for-upgrade-permissions","text":"Once you get the names of the resource group and user-assigned managed identity, follow the same steps as in getting started to add a custom role . First, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade","title":"Step 2.1: Add custom role for upgrade permissions"},{"location":"hopsworksai/azure/upgrade/#step-22-assign-the-custom-role-to-your-user-assigned-managed-identity","text":"Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect.","title":"Step 2.2: Assign the custom role to your user-assigned managed identity"},{"location":"hopsworksai/azure/upgrade/#step-3-add-disk-read-permissions-to-your-role-connected-to-hopsworksai","text":"We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai","title":"Step 3: Add disk read permissions to your role connected to Hopsworks.ai"},{"location":"hopsworksai/azure/upgrade/#step-4-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 4: Run the upgrade process"},{"location":"hopsworksai/azure/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"hopsworksai/azure/upgrade/#error-1-missing-permissions-error","text":"If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process","title":"Error 1: Missing permissions error"},{"location":"hopsworksai/azure/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Rollback succeed","title":"Error 2: Upgrade process error"},{"location":"hopsworksai/sso/oauth/","text":"Configure your hopsworks cluster to use OAuth2 for user management. # Once you have created a Hopsworks cluster you can configure it to use OAuth2 for its access control. We will go through a step-by-step description of the configuration process. To illustrate our explanation We will use Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2. Step 1: Configure your identity provider. # To use OAuth2 in hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application In the Overview section, copy the Application (client) ID field . We will use it in step 2 under the name OAUTH_CLIENT_ID . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in step 2 under the name PROVIDER_URI . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in step 2 under the name OAUTH_CLIENT_SECRET . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your hopsworks cluster. You can find it by going to the hopsworks.ai dashboard in the General tab of your cluster and copying the URI (excluding the /#!/ ). Configure platform: Redirect Step 2: Configure Hopsworks # Log into your Hopsworks cluster and go to the admin page. Hopsworks admin Click on Edit variables Edit variables Enter oauth in the Name entry field (1). Set oauth_enabled to true (2).Set oauth_redirect_uri to the same redirect URI as above (3). Set oauth_logout_redirect_uri to the same redirect URI as above without the callback at the end (4). Set oauth_account_status to 2 (5). Set oauth_group_mapping to ANY_GROUP->HOPS_USER (6). Click on Reload variables (7) and click on Admin Home (8). Note If you let the value 1 for oauth_account_status an administrator will need to enable the user in hopsworks each time a new user tries to login with OAuth. Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when the log into hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of the group to be admins in hopsworks. You do several mappings of groups to roles by comma separating them. Set variables Click on Register OAuth 2.0 Client . Register OAuth 2.0 Client Click on the + next to Register OpenId provider and client (1). Set Client id to be the OAUTH_CLIENT_ID you copied above (2). Set Client secret to be the OAUTH_CLIENT_SECRET you copied above (3). Give a name to your provider in Provider name for example, OAuth (4). Give the name that will be displayed on the login page for your provider in Provider display name for example, OAuth (5). Set Provider URI to the PROVIDER_URI you copied above (6). Click Save (7). Configure OAuth 2.0 Client Users will now see a new button on the login page. The button has the name you set above for Provider display name and will redirect to your identity provider. Login","title":"Configure your hopsworks cluster to use OAuth2 for user management."},{"location":"hopsworksai/sso/oauth/#configure-your-hopsworks-cluster-to-use-oauth2-for-user-management","text":"Once you have created a Hopsworks cluster you can configure it to use OAuth2 for its access control. We will go through a step-by-step description of the configuration process. To illustrate our explanation We will use Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2.","title":"Configure your hopsworks cluster to use OAuth2 for user management."},{"location":"hopsworksai/sso/oauth/#step-1-configure-your-identity-provider","text":"To use OAuth2 in hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application In the Overview section, copy the Application (client) ID field . We will use it in step 2 under the name OAUTH_CLIENT_ID . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in step 2 under the name PROVIDER_URI . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in step 2 under the name OAUTH_CLIENT_SECRET . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your hopsworks cluster. You can find it by going to the hopsworks.ai dashboard in the General tab of your cluster and copying the URI (excluding the /#!/ ). Configure platform: Redirect","title":"Step 1: Configure your identity provider."},{"location":"hopsworksai/sso/oauth/#step-2-configure-hopsworks","text":"Log into your Hopsworks cluster and go to the admin page. Hopsworks admin Click on Edit variables Edit variables Enter oauth in the Name entry field (1). Set oauth_enabled to true (2).Set oauth_redirect_uri to the same redirect URI as above (3). Set oauth_logout_redirect_uri to the same redirect URI as above without the callback at the end (4). Set oauth_account_status to 2 (5). Set oauth_group_mapping to ANY_GROUP->HOPS_USER (6). Click on Reload variables (7) and click on Admin Home (8). Note If you let the value 1 for oauth_account_status an administrator will need to enable the user in hopsworks each time a new user tries to login with OAuth. Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when the log into hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of the group to be admins in hopsworks. You do several mappings of groups to roles by comma separating them. Set variables Click on Register OAuth 2.0 Client . Register OAuth 2.0 Client Click on the + next to Register OpenId provider and client (1). Set Client id to be the OAUTH_CLIENT_ID you copied above (2). Set Client secret to be the OAUTH_CLIENT_SECRET you copied above (3). Give a name to your provider in Provider name for example, OAuth (4). Give the name that will be displayed on the login page for your provider in Provider display name for example, OAuth (5). Set Provider URI to the PROVIDER_URI you copied above (6). Click Save (7). Configure OAuth 2.0 Client Users will now see a new button on the login page. The button has the name you set above for Provider display name and will redirect to your identity provider. Login","title":"Step 2: Configure Hopsworks"},{"location":"hopsworksai/sso/sso/","text":"Hopsworks.ai Single Sign-On # We will see here how to set up Single Sign-On for Hopsworks.ai. Once this is set up users from your organization will be able to directly sign in to Hopsworks.ai using your identity provider and without the need to manually create an account. They will then be able to manage the clusters of your organization and if you set up user management on your clusters an account will automatically be created for them in the clusters. Note See Hopsworks Single Sing-On if you do not want to give users the rights to manage your organization clusters but want to use your identity provider to manage access to your Hopsworks clusters. Configure your identity provider. # We will give here the examples of Azure Active Directory and AWS Single Sign-On but a similar setup can be done with any identity provider supporting SAML. Azure Active Directory # Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on Azure Active Directory . You will need the two copyable entries on this page in the following steps. Azure Active Directory Go to the Azure Portal then proceed to the Active Directory and click on Enterprise applications . Click on New application . New application Click on Create your own application . Give a name to your application, for example, hopsworks_sso . Make sure that Integrate any other application you don't find in the gallery (Non-gallery) is selected and click on Create . Create your own application Click on Single sign-on . Then click on SAML . SAML Click on Edit in the Basic SAML Configuration section. Paste the Identifier (Entity ID) and Reply URL that you copied from the Hopsworks.ai setup page. Click on Save . Configure SAML In the SAML Signing Certificate section copy the App Federation Metadata URL . App Federation Metadata URL Click on Users and groups , in the left column, and add the users and groups you want to have access to hopsworks.ai. Go back to Hopsworks.ai. Click on Next step and keep following the documentation at Configure Hopsworks.ai . Next step AWS Single Sign-On # Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on AWS SSO . You will need the copyable entries on this page in the following steps. AWS SSO Go to AWS Single Sign-On in the AWS Management Console and click on Applications , then click on Add New Application . Add New application Click on Add a custom SAML 2.0 application . Add a custom SAML 2.0 application Give a name to your application, for example, hopsworks_sso . Scroll to the bottom and click on If you don't have a metadata file, you can manually type your metadata values . Application configuration Paste the Application ACS URL and Application SAML audience that you copy from the Hopsworks.ai setup page. Click on Save changes . Application configuration 2 Go to the Attribute mappings tab. On the first line enter the value Subject and select unspecified for the format. then, Click on Add new attribute mapping 3 times. Attribute mapping For each of the created lines enter the following values in the first and second columns and let the format as unspecified. First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress , second: ${user:email} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname , Second: ${user:familyName} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname , Second: ${user:givenName} Click on Save changes . Attribute mapping 2 Return to the Configuration tab and click on Edit configuration . Edit configuration Click on Copy URL on the AWS SSO SAML metadata file line. We will call this URL Metadata URL in the coming steps. Metadata URL Go back to Hopsworks.ai. Click on Next step and keep following the documentation at Configure Hopsworks.ai . Next step Configure Hopsworks.ai. # Give a name to your organization. This name will be used in your login URL so choose something you will remember. Here we will use hopsworks-demo . Paste the Metadata URL you copied above and click Finish . Configure Hopsworks.ai If you go back to the SSO tab of Settings you will get a logging page link. By using this link you will automatically be redirected to your identity provider to login. An account will automatically be created in hopsworks.ai for users of your organization when they log in for the first time. Configure Hopsworks.ai","title":"Hopsworks.ai Single Sign-On"},{"location":"hopsworksai/sso/sso/#hopsworksai-single-sign-on","text":"We will see here how to set up Single Sign-On for Hopsworks.ai. Once this is set up users from your organization will be able to directly sign in to Hopsworks.ai using your identity provider and without the need to manually create an account. They will then be able to manage the clusters of your organization and if you set up user management on your clusters an account will automatically be created for them in the clusters. Note See Hopsworks Single Sing-On if you do not want to give users the rights to manage your organization clusters but want to use your identity provider to manage access to your Hopsworks clusters.","title":"Hopsworks.ai Single Sign-On"},{"location":"hopsworksai/sso/sso/#configure-your-identity-provider","text":"We will give here the examples of Azure Active Directory and AWS Single Sign-On but a similar setup can be done with any identity provider supporting SAML.","title":"Configure your identity provider."},{"location":"hopsworksai/sso/sso/#azure-active-directory","text":"Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on Azure Active Directory . You will need the two copyable entries on this page in the following steps. Azure Active Directory Go to the Azure Portal then proceed to the Active Directory and click on Enterprise applications . Click on New application . New application Click on Create your own application . Give a name to your application, for example, hopsworks_sso . Make sure that Integrate any other application you don't find in the gallery (Non-gallery) is selected and click on Create . Create your own application Click on Single sign-on . Then click on SAML . SAML Click on Edit in the Basic SAML Configuration section. Paste the Identifier (Entity ID) and Reply URL that you copied from the Hopsworks.ai setup page. Click on Save . Configure SAML In the SAML Signing Certificate section copy the App Federation Metadata URL . App Federation Metadata URL Click on Users and groups , in the left column, and add the users and groups you want to have access to hopsworks.ai. Go back to Hopsworks.ai. Click on Next step and keep following the documentation at Configure Hopsworks.ai . Next step","title":"Azure Active Directory"},{"location":"hopsworksai/sso/sso/#aws-single-sign-on","text":"Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on AWS SSO . You will need the copyable entries on this page in the following steps. AWS SSO Go to AWS Single Sign-On in the AWS Management Console and click on Applications , then click on Add New Application . Add New application Click on Add a custom SAML 2.0 application . Add a custom SAML 2.0 application Give a name to your application, for example, hopsworks_sso . Scroll to the bottom and click on If you don't have a metadata file, you can manually type your metadata values . Application configuration Paste the Application ACS URL and Application SAML audience that you copy from the Hopsworks.ai setup page. Click on Save changes . Application configuration 2 Go to the Attribute mappings tab. On the first line enter the value Subject and select unspecified for the format. then, Click on Add new attribute mapping 3 times. Attribute mapping For each of the created lines enter the following values in the first and second columns and let the format as unspecified. First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress , second: ${user:email} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname , Second: ${user:familyName} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname , Second: ${user:givenName} Click on Save changes . Attribute mapping 2 Return to the Configuration tab and click on Edit configuration . Edit configuration Click on Copy URL on the AWS SSO SAML metadata file line. We will call this URL Metadata URL in the coming steps. Metadata URL Go back to Hopsworks.ai. Click on Next step and keep following the documentation at Configure Hopsworks.ai . Next step","title":"AWS Single Sign-On"},{"location":"hopsworksai/sso/sso/#configure-hopsworksai","text":"Give a name to your organization. This name will be used in your login URL so choose something you will remember. Here we will use hopsworks-demo . Paste the Metadata URL you copied above and click Finish . Configure Hopsworks.ai If you go back to the SSO tab of Settings you will get a logging page link. By using this link you will automatically be redirected to your identity provider to login. An account will automatically be created in hopsworks.ai for users of your organization when they log in for the first time. Configure Hopsworks.ai","title":"Configure Hopsworks.ai."},{"location":"integrations/assume_role/","text":"Assuming a role # When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to projects in Hopsworks, for a guide on how to configure this see role mapping . After an administrator configured role mappings in Hopsworks you can see the roles you can assume by going to your project settings. Cloud roles mapped to project. You can then use the Hops Python and Java APIs to assume the roles listed in your project's settings page. When calling the assume role method you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call the assume role method with no argument. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the Default button above the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. In the image above if a Data scientist called the assume role method with no arguments she will assume the role with id 1029 but if a Data owner called the same method she will assume the role with id 1. Use temporary credentials. Python from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () Scala import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. Assume role also sets environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role. To read s3 buckets with TensorFlow you also need to set AWS_REGION environment variable (s3 bucket region). The code below shows how to read training and validation datasets from s3 bucket using TensorFlow. Use temporary credentials with TensorFlow. from hops.credentials_provider import get_role , assume_role import tensorflow as tf import os assume_role ( role_arn = get_role ( 1 )) # s3 bucket region need to be set for TensorFlow os . environ [ \"AWS_REGION\" ] = \"eu-north-1\" train_filenames = [ \"s3://resource/train/train.tfrecords\" ] validation_filenames = [ \"s3://resourcet/validation/validation.tfrecords\" ] train_dataset = tf . data . TFRecordDataset ( train_filenames ) validation_dataset = tf . data . TFRecordDataset ( validation_filenames ) for raw_record in train_dataset . take ( 1 ): example = tf . train . Example () example . ParseFromString ( raw_record . numpy ()) print ( example )","title":"Assume role"},{"location":"integrations/assume_role/#assuming-a-role","text":"When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles are configured in AWS and mapped to projects in Hopsworks, for a guide on how to configure this see role mapping . After an administrator configured role mappings in Hopsworks you can see the roles you can assume by going to your project settings. Cloud roles mapped to project. You can then use the Hops Python and Java APIs to assume the roles listed in your project's settings page. When calling the assume role method you can pass the role ARN string or use the get role method that takes the role id as an argument. If you assign a default role for your project you can call the assume role method with no argument. You can assign (if you are a Data owner in that project) a default role to you project by clicking on the Default button above the role you want to make default. You can set one default per project role. If a default is set for a project role (Data scientist or Data owner) and all members (ALL) the default set for the project role will take precedence over the default set for all members. In the image above if a Data scientist called the assume role method with no arguments she will assume the role with id 1029 but if a Data owner called the same method she will assume the role with id 1. Use temporary credentials. Python from hops.credentials_provider import get_role , assume_role credentials = assume_role ( role_arn = get_role ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ) . show () Scala import io . hops . util . CredentialsProvider val creds = CredentialsProvider . assumeRole ( CredentialsProvider . getRole ( 1 )) spark . read . csv ( \"s3a://resource/test.csv\" ). show () The assume role method sets spark hadoop configurations that will allow spark to read s3 buckets. The code examples above show how to read s3 buckets using Python and Scala. Assume role also sets environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN so that programs running in the container can use the credentials for the newly assumed role. To read s3 buckets with TensorFlow you also need to set AWS_REGION environment variable (s3 bucket region). The code below shows how to read training and validation datasets from s3 bucket using TensorFlow. Use temporary credentials with TensorFlow. from hops.credentials_provider import get_role , assume_role import tensorflow as tf import os assume_role ( role_arn = get_role ( 1 )) # s3 bucket region need to be set for TensorFlow os . environ [ \"AWS_REGION\" ] = \"eu-north-1\" train_filenames = [ \"s3://resource/train/train.tfrecords\" ] validation_filenames = [ \"s3://resourcet/validation/validation.tfrecords\" ] train_dataset = tf . data . TFRecordDataset ( train_filenames ) validation_dataset = tf . data . TFRecordDataset ( validation_filenames ) for raw_record in train_dataset . take ( 1 ): example = tf . train . Example () example . ParseFromString ( raw_record . numpy ()) print ( example )","title":"Assuming a role"},{"location":"integrations/hdinsight/","text":"Configure HDInsight for the Hopsworks Feature Store # To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information. Step 1: Set up a Hopsworks API key # In order for HDInsight clusters to be able to communicate with the Hopsworks Feature Store, the clients running on HDInsight need a Hopsworks API key. In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Make sure you have the key handy for the next steps. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Step 2: Use a script action to install the Feature Store connector # HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks Step 3: Configure HDInsight for Feature Store access # The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store. Step 5: Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Azure HDInsight"},{"location":"integrations/hdinsight/#configure-hdinsight-for-the-hopsworks-feature-store","text":"To enable HDInsight to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a script action and configurations to your HDInsight cluster. Prerequisites A HDInsight cluster with cluster type Spark is required to connect to the Feature Store. You can either use an existing cluster or create a new one. Network Connectivity To be able to connect to the Feature Store, please ensure that your HDInsight cluster and the Hopsworks Feature Store are either in the same Virtual Network or Virtual Network Peering is set up between the different networks. In addition, ensure that the Network Security Group of your Hopsworks instance is configured to allow incoming traffic from your HDInsight cluster on ports 443, 3306, 8020, 30010, 9083 and 9085 (443,3306,8020,30010,9083,9085). See Network security groups for more information.","title":"Configure HDInsight for the Hopsworks Feature Store"},{"location":"integrations/hdinsight/#step-1-set-up-a-hopsworks-api-key","text":"In order for HDInsight clusters to be able to communicate with the Hopsworks Feature Store, the clients running on HDInsight need a Hopsworks API key. In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Make sure you have the key handy for the next steps. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Step 1: Set up a Hopsworks API key"},{"location":"integrations/hdinsight/#step-2-use-a-script-action-to-install-the-feature-store-connector","text":"HDInsight requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the script action shown below. Copy the content into a file, name the file hopsworks.sh and replace MY_INSTANCE, MY_PROJECT, MY_VERSION, MY_API_KEY and MY_CONDA_ENV with your values. Copy the hopsworks.sh file into any storage that is readable by your HDInsight clusters and take note of the URI of that file e.g., https://account.blob.core.windows.net/scripts/hopsworks.sh . The script action needs to be applied head and worker nodes and can be applied during cluster creation or to an existing cluster. Ensure to persist the script action so that it is run on newly created nodes. For more information about how to use script actions, see Customize Azure HDInsight clusters by using script actions . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Feature Store script action: set -e HOST = \"MY_INSTANCE.cloud.hopsworks.ai\" # DNS of your Feature Store instance PROJECT = \"MY_PROJECT\" # Port to reach your Hopsworks instance, defaults to 443 HSFS_VERSION = \"MY_VERSION\" # The major version of HSFS needs to match the major version of Hopsworks API_KEY = \"MY_API_KEY\" # The API key to authenticate with Hopsworks CONDA_ENV = \"MY_CONDA_ENV\" # py35 is the default for HDI 3.6 apt-get --assume-yes install python3-dev apt-get --assume-yes install jq /usr/bin/anaconda/envs/ $CONDA_ENV /bin/pip install hsfs == $HSFS_VERSION PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) mkdir -p /usr/lib/hopsworks chown root:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chown -R root:hadoop /usr/lib/hopsworks","title":"Step 2:  Use a script action to install the Feature Store connector"},{"location":"integrations/hdinsight/#step-3-configure-hdinsight-for-feature-store-access","text":"The Hadoop and Spark installations of the HDInsight cluster need to be configured in order to access the Feature Store. This can be achieved either by using a bootstrap script when creating clusters or using Ambari on existing clusters. Apply the following configurations to your HDInsight cluster. Using Hive and the Feature Store HDInsight clusters cannot use their local Hive when being configured for the Feature Store as the Feature Store relies on custom Hive binaries and its own Metastore which will overwrite the local one. If you rely on Hive for feature engineering then it is advised to write your data to an external data storage such as ADLS from your main HDInsight cluster and in the Feature Store, create an on-demand Feature Group on the storage container in ADLS. Hadoop hadoop-env.sh: export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/* Hadoop core-site.xml: hops.ipc.server.ssl.enabled=true fs.hopsfs.impl=io.hops.hopsfs.client.HopsFileSystem client.rpc.ssl.enabled.protocol=TLSv1.2 hops.ssl.keystore.name=/usr/lib/hopsworks/keyStore.jks hops.rpc.socket.factory.class.default=io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory hops.ssl.keystores.passwd.name=/usr/lib/hopsworks/material_passwd hops.ssl.hostname.verifier=ALLOW_ALL hops.ssl.trustore.name=/usr/lib/hopsworks/trustStore.jks Spark spark-defaults.conf: spark.executor.extraClassPath=/usr/lib/hopsworks/client/* spark.driver.extraClassPath=/usr/lib/hopsworks/client/* spark.sql.hive.metastore.jars=/usr/lib/hopsworks/apache-hive-bin/lib/* Spark hive-site.xml: hive.metastore.uris=thrift://MY_HOPSWORKS_INSTANCE_PRIVATE_IP:9083 Info Replace MY_HOPSWORKS_INSTANCE_PRIVATE_IP with the private IP address of you Hopsworks Feature Store.","title":"Step 3: Configure HDInsight for Feature Store access"},{"location":"integrations/hdinsight/#step-5-connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store, for instance using a Jupyter notebook in HDInsight with a PySpark3 kernel: import hsfs # Put the API key into Key Vault for any production setup: # See, https://azure.microsoft.com/en-us/services/key-vault/ secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Step 5: Connect to the Feature Store"},{"location":"integrations/hdinsight/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/mlstudio_designer/","text":"Azure Machine Learning Designer Integration # Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connect to the Feature Store # To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[hive]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Designer"},{"location":"integrations/mlstudio_designer/#azure-machine-learning-designer-integration","text":"Connecting to the Feature Store from the Azure Machine Learning Designer requires setting up a Feature Store API key for the Designer and installing the HSFS on the Designer. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Designer. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Designer Integration"},{"location":"integrations/mlstudio_designer/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/mlstudio_designer/#connect-to-the-feature-store","text":"To connect to the Feature Store from the Azure Machine Learning Designer, create a new pipeline or open an existing one: Add an Execute Python Script step In the pipeline, add a new Execute Python Script step and replace the Python script from the next step: Add the code to access the Feature Store Updating the script Replace MY_VERSION, MY_API_KEY, MY_INSTANCE, MY_PROJECT and MY_FEATURE_GROUP with the respective values. The major version set for MY_VERSION needs to match the major version of Hopsworks. Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks import os import importlib.util package_name = 'hsfs' version = 'MY_VERSION' spec = importlib . util . find_spec ( package_name ) if spec is None : import os os . system ( f \"pip install %s[hive]==%s\" % ( package_name , version )) # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' def azureml_main ( dataframe1 = None , dataframe2 = None ): import hsfs conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) fs = conn . get_feature_store () # Get the project's default feature store return fs . get_feature_group ( 'MY_FEATURE_GROUP' , version = 1 ) . read (), Select a compute target and save the step. The step is now ready to use: Select a compute target As a next step, you have to connect the previously created Execute Python Script step with the next step in the pipeline. For instance, to export the features to a CSV file, create a Export Data step: Add an Export Data step Configure the Export Data step to write to you data store of choice: Configure the Export Data step Connect the to steps by drawing a line between them: Connect the steps Finally, submit the pipeline and wait for it to finish: Performance on the first execution The Execute Python Script step can be slow when being executed for the first time as the HSFS library needs to be installed on the compute target. Subsequent executions on the same compute target should use the already installed library. Execute the pipeline","title":"Connect to the Feature Store"},{"location":"integrations/mlstudio_designer/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/mlstudio_notebooks/","text":"Azure Machine Learning Notebooks Integration # Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering . Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connect from an Azure Machine Learning Notebook # To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store () Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Notebooks"},{"location":"integrations/mlstudio_notebooks/#azure-machine-learning-notebooks-integration","text":"Connecting to the Feature Store from Azure Machine Learning Notebooks requires setting up a Feature Store API key for Azure Machine Learning Notebooks and installing the HSFS on the notebook. This guide explains step by step how to connect to the Feature Store from Azure Machine Learning Notebooks. Network Connectivity To be able to connect to the Feature Store, please ensure that the Network Security Group of your Hopsworks instance on Azure is configured to allow incoming traffic from your compute target on ports 443, 9083 and 9085 (443,9083,9085). See Network security groups for more information. If your compute target is not in the same VNet as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure Virtual Network Peering .","title":"Azure Machine Learning Notebooks Integration"},{"location":"integrations/mlstudio_notebooks/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/mlstudio_notebooks/#connect-from-an-azure-machine-learning-notebook","text":"To access the Feature Store from Azure Machine Learning, open a Python notebook and proceed with the following steps to install HSFS and connect to the Feature Store: Connecting from an Azure Machine Learning Notebook","title":"Connect from an Azure Machine Learning Notebook"},{"location":"integrations/mlstudio_notebooks/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in Azure Machine Learning and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark is used as execution engine and therefore Hive dependencies are not installed. Hence, if you are using a regular Python Kernel without Spark , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . Check PyPI for available releases. You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"integrations/mlstudio_notebooks/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from the notebook: import hsfs # Put the API key into Key Vault for any production setup: # See, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-secrets-in-runs #from azureml.core import Experiment, Run #run = Run.get_context() #secret_value = run.get_secret(name=\"fs-api-key\") secret_value = 'MY_API_KEY' # Create a connection conn = hsfs . connection ( host = 'MY_INSTANCE.cloud.hopsworks.ai' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'MY_PROJECT' , # Name of your Hopsworks Feature Store project api_key_value = secret_value , # The API key to authenticate with Hopsworks hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine ) # Get the feature store handle for the project's feature store fs = conn . get_feature_store ()","title":"Connect to the Feature Store"},{"location":"integrations/mlstudio_notebooks/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/python/","text":"Python Environments (Local or Kubeflow) # Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard. Create a file called featurestore.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='hive' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services . Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Python"},{"location":"integrations/python/#python-environments-local-or-kubeflow","text":"Connecting to the Feature Store from any Python environment requires setting up a Feature Store API key and installing the library. This guide explains step by step how to connect to the Feature Store from any Python environment such as your local environment or KubeFlow.","title":"Python Environments (Local or Kubeflow)"},{"location":"integrations/python/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard. Create a file called featurestore.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/python/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed in the environment from which you want to connect to the Feature Store. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on a local Python evnironment, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSFS"},{"location":"integrations/python/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from your Python environment: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available. So if you have PySpark installed in your local Python environment, but you have not configured Spark, you will have to set engine='hive' . Please refer to the Spark integration guide to configure your local Spark cluster to be able to connect to the Hopsworks Feature Store. Ports If you have trouble to connect, please ensure that your Feature Store can receive incoming traffic from your Python environment on ports 443, 9083 and 9085 (443,9083,9085). If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services .","title":"Connect to the Feature Store"},{"location":"integrations/python/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/role_mapping/","text":"IAM role mapping # Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Create an instance profile role that contains the different resource roles that we want to allow selected users to be able to assume in the Hopsworks cluster. In the example below, we define 4 different resource roles: test-role, s3-role, dev-s3-role, and redshift - and later we will define which users will be allowed to assume which of these resources roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Create the resource roles and edit the trust relationship and add a policy document that will allow the instance profile to assume the resource roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Finally attach the instance profile to the master node of your Hopsworks AWS instance. Role chaining allows the instance profile to assume any of the 4 resource roles in the policy that was attached in step 1. Typically, we will not want any user in Hopsworks to assume any of the resource roles. You can grant selected users the ability to assume any of the 4 resource roles from the admin page in hopsworks. In particular, we specify in which project(s) a given resource role can be used. Within a given project, we can further restrict who can assume the resource role by mapping the role to the group of users (data owners or data scientists). Resource role mapping. By clicking the 'Resource role mapping' icon in the admin page shown in the image above you can add mappings by entering the project name and which roles in that project can access the resource role. Optionally, you can set a role mapping as default by marking the default checkbox. The default roles can only be changed by a Data owner who can do so in the project settings page. Add resource role to project mapping. Any member of a project can then go to the project settings page to see which roles they can assume. Resource role mapped to project. For instructions on how to use the assume role API see assuming a role .","title":"Resource role to project mapping"},{"location":"integrations/role_mapping/#iam-role-mapping","text":"Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Create an instance profile role that contains the different resource roles that we want to allow selected users to be able to assume in the Hopsworks cluster. In the example below, we define 4 different resource roles: test-role, s3-role, dev-s3-role, and redshift - and later we will define which users will be allowed to assume which of these resources roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Create the resource roles and edit the trust relationship and add a policy document that will allow the instance profile to assume the resource roles. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Finally attach the instance profile to the master node of your Hopsworks AWS instance. Role chaining allows the instance profile to assume any of the 4 resource roles in the policy that was attached in step 1. Typically, we will not want any user in Hopsworks to assume any of the resource roles. You can grant selected users the ability to assume any of the 4 resource roles from the admin page in hopsworks. In particular, we specify in which project(s) a given resource role can be used. Within a given project, we can further restrict who can assume the resource role by mapping the role to the group of users (data owners or data scientists). Resource role mapping. By clicking the 'Resource role mapping' icon in the admin page shown in the image above you can add mappings by entering the project name and which roles in that project can access the resource role. Optionally, you can set a role mapping as default by marking the default checkbox. The default roles can only be changed by a Data owner who can do so in the project settings page. Add resource role to project mapping. Any member of a project can then go to the project settings page to see which roles they can assume. Resource role mapped to project. For instructions on how to use the assume role API see assuming a role .","title":"IAM role mapping"},{"location":"integrations/sagemaker/","text":"AWS SageMaker Integration # Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key on AWS # The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks. Identify your SageMaker role # You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance Store the API key # You have two options to make your API key accessible from SageMaker: Option 1: Using the AWS Systems Manager Parameter Store # Store the API key in the AWS Systems Manager Parameter Store # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store Grant access to the Parameter Store from the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role Option 2: Using the AWS Secrets Manager # Store the API key in the AWS Secrets Manager # In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager Grant access to the SecretsManager to the SageMaker notebook role # In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role Install HSFS # To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances. Connect to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"hive\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS . Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"AWS Sagemaker"},{"location":"integrations/sagemaker/#aws-sagemaker-integration","text":"Connecting to the Feature Store from SageMaker requires setting up a Feature Store API key for SageMaker and installing the HSFS on SageMaker. This guide explains step by step how to connect to the Feature Store from SageMaker.","title":"AWS SageMaker Integration"},{"location":"integrations/sagemaker/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only ably to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generate an API key"},{"location":"integrations/sagemaker/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"integrations/sagemaker/#store-the-api-key-on-aws","text":"The API key now needs to be stored on AWS, so it can be retrieved from within SageMaker notebooks.","title":"Store the API key on AWS"},{"location":"integrations/sagemaker/#identify-your-sagemaker-role","text":"You need to know the IAM role used by your SageMaker instance to set up the API key for it. You can find it in the overview of your SageMaker notebook instance of the AWS Management Console. In this example, the name of the role is AmazonSageMaker-ExecutionRole-20190511T072435 . The role is attached to your SageMaker notebook instance","title":"Identify your SageMaker role"},{"location":"integrations/sagemaker/#store-the-api-key","text":"You have two options to make your API key accessible from SageMaker:","title":"Store the API key"},{"location":"integrations/sagemaker/#option-1-using-the-aws-systems-manager-parameter-store","text":"","title":"Option 1: Using the AWS Systems Manager Parameter Store"},{"location":"integrations/sagemaker/#store-the-api-key-in-the-aws-systems-manager-parameter-store","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Systems Manager choose Parameter Store in the left navigation bar and select Create Parameter . As name, enter /hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select Secure String as type and create the parameter . Store the API key in the AWS Systems Manager Parameter Store","title":"Store the API key in the AWS Systems Manager Parameter Store"},{"location":"integrations/sagemaker/#grant-access-to-the-parameter-store-from-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_SAGEMAKER_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Grant access to the Parameter Store from the SageMaker notebook role","title":"Grant access to the Parameter Store from the SageMaker notebook role"},{"location":"integrations/sagemaker/#option-2-using-the-aws-secrets-manager","text":"","title":"Option 2: Using the AWS Secrets Manager"},{"location":"integrations/sagemaker/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS Management Console, ensure that your active region is the region you use for SageMaker. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store the API key in the AWS Secrets Manager As secret name, enter hopsworks/role/[MY_SAGEMAKER_ROLE] replacing [MY_SAGEMAKER_ROLE] with the AWS role used by the SageMaker instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Store the API key in the AWS Secrets Manager","title":"Store the API key in the AWS Secrets Manager"},{"location":"integrations/sagemaker/#grant-access-to-the-secretsmanager-to-the-sagemaker-notebook-role","text":"In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating SageMaker notebook instances. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Grant access to the SecretsManager to the SageMaker notebook role","title":"Grant access to the SecretsManager to the SageMaker notebook role"},{"location":"integrations/sagemaker/#install-hsfs","text":"To be able to access the Hopsworks Feature Store, the HSFS Python library needs to be installed. One way of achieving this is by opening a Python notebook in SageMaker and installing the HSFS with a magic command and pip: !pip install hsfs[hive]~=[HOPSWORKS_VERSION] Hive Dependencies By default, HSFS assumes Spark/EMR is used as execution engine and therefore Hive dependencies are not installed. Hence, on AWS SageMaker, if you are planning to use a regular Python Kernel without Spark/EMR , make sure to install the \"hive\" extra dependencies ( hsfs[hive] ). Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Note that the library will not be persistent. For information around how to permanently install a library to SageMaker, see Install External Libraries and Kernels in Notebook Instances.","title":"Install HSFS"},{"location":"integrations/sagemaker/#connect-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from SageMaker: import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True , # Disable for self-signed certificates engine = 'hive' # Choose Hive as engine if you haven't set up AWS EMR ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. Most AWS SageMaker Kernels have PySpark installed but are not connected to AWS EMR by default, hence, the engine option of the connection let's you overwrite the default behaviour. By default, HSFS will try to use Spark as engine if PySpark is available, however, if Spark/EMR is not configured, you will have to set the engine manually to \"hive\" . Please refer to the EMR integration guide to setup EMR with the Hopsworks Feature Store. Ports If you have trouble connecting, please ensure that the Security Group of your Hopsworks instance on AWS is configured to allow incoming traffic from your SageMaker instance on ports 443, 9083 and 9085 (443,9083,9085). See VPC Security Groups for more information. If your SageMaker instances are not in the same VPC as your Hopsworks instance and the Hopsworks instance is not accessible from the internet then you will need to configure VPC Peering on AWS .","title":"Connect to the Feature Store"},{"location":"integrations/sagemaker/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/spark/","text":"Spark Integration # Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster. Download the Hopsworks Client Jars # In the Feature Store UI, select the integration tab and then select the Spark tab. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the client libraries for HopsHive and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster Download the certificates # Download the certificates from the same Spark tab in the Feature Store UI. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well. Configure your Spark cluster # Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.trustore.name trustStore.jks spark.sql.hive.metastore.jars [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars should point to the path with the Hive Jars which can be found in the clients.tar.gz . PySpark # To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Generating an API Key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select Api keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The created API-Key should at least have the following scopes: featurestore project job API-Keys can be generated in the User Settings on Hopsworks Info You are only ably to retrieve the API Key once. If you did not manage to copy it to your clipboard, delete it again and create a new one. Connecting to the Feature Store # You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above. Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Spark"},{"location":"integrations/spark/#spark-integration","text":"Connecting to the Feature Store from an external Spark cluster, such as Cloudera, requires configuring it with the Hopsworks client jars and configuration. This guide explains step by step how to connect to the Feature Store from an external Spark cluster.","title":"Spark Integration"},{"location":"integrations/spark/#download-the-hopsworks-client-jars","text":"In the Feature Store UI, select the integration tab and then select the Spark tab. Click on Download client Jars . This will start the download of the client.tar.gz archive. The archive contains two jar files for HopsFS, the client libraries for HopsHive and the Java version of the HSFS library. You should upload these libraries to your Spark cluster and attach them as local resources to your Job. If you are using spark-submit , you should specify the --jar option. For more details see: Spark Dependency Management . The Spark Integration gives access to Jars and configuration for an external Spark cluster","title":"Download the Hopsworks Client Jars"},{"location":"integrations/spark/#download-the-certificates","text":"Download the certificates from the same Spark tab in the Feature Store UI. Hopsworks uses X.509 certificates for authentication and authorization. If you are interested in the Hopsworks security model, you can read more about it in this blog post . The certificates are composed of three different components: the keyStore.jks containing the private key and the certificate for your project user, the trustStore.jks containing the certificates for the Hopsworks certificates authority, and a password to unlock the private key in the keyStore.jks . The password is displayed in a pop-up when downloading the certificate and should be saved in a file named material_passwd . Warning When you copy-paste the password to the material_passwd file, pay attention to not introduce additional empty spaces or new lines. The three files ( keyStore.jks , trustStore.jks and material_passwd ) should be attached as resources to your Spark application as well.","title":"Download the certificates"},{"location":"integrations/spark/#configure-your-spark-cluster","text":"Add the following configuration to the Spark application: spark.hadoop.fs.hopsfs.impl io.hops.hopsfs.client.HopsFileSystem spark.hadoop.hops.ipc.server.ssl.enabled true spark.hadoop.hops.ssl.hostname.verifier ALLOW_ALL spark.hadoop.hops.rpc.socket.factory.class.default io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory spark.hadoop.client.rpc.ssl.enabled.protocol TLSv1.2 spark.hadoop.hops.ssl.keystores.passwd.name material_passwd spark.hadoop.hops.ssl.keystore.name keyStore.jks spark.hadoop.hops.ssl.trustore.name trustStore.jks spark.sql.hive.metastore.jars [Path to the Hopsworks Hive Jars] spark.hadoop.hive.metastore.uris thrift://[metastore_ip]:[metastore_port] spark.sql.hive.metastore.jars should point to the path with the Hive Jars which can be found in the clients.tar.gz .","title":"Configure your Spark cluster"},{"location":"integrations/spark/#pyspark","text":"To use PySpark, install the HSFS Python library which can be found on PyPi . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"PySpark"},{"location":"integrations/spark/#generating-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select Api keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The created API-Key should at least have the following scopes: featurestore project job API-Keys can be generated in the User Settings on Hopsworks Info You are only ably to retrieve the API Key once. If you did not manage to copy it to your clipboard, delete it again and create a new one.","title":"Generating an API Key"},{"location":"integrations/spark/#connecting-to-the-feature-store","text":"You are now ready to connect to the Hopsworks Feature Store from Spark: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'api_key' , # The API key to authenticate with the feature store hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Engine HSFS uses either Apache Spark or Apache Hive as an execution engine to perform queries against the feature store. The engine option of the connection let's you overwrite the default behaviour by setting it to \"hive\" or \"spark\" . By default, HSFS will try to use Spark as engine if PySpark is available, hence, no further action should be required if you setup Spark correctly as described above.","title":"Connecting to the Feature Store"},{"location":"integrations/spark/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/storage-connectors/","text":"Storage Connectors # You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store information in Hopsworks about how to securely connect to external data stores. They can be used in both programs and in Hopsworks to easily and securely connect and ingest data to the Feature Store. External (on-demand) Feature Groups can also be defined with storage connectors, where only the metadata is stored in Hopsworks. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or more advanced multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. Mapping IAM Roles to Projects/Roles in Hopsworks In Hopsworks, you can specify a Cloud Role (IAM role or managed identity) and (1) in which Project and (2) what role within that Project can assume that Cloud Role. For example, you could limit access to a given IAM Role to users who have the 'Data Owner' role in a Project called 'RawFeatures'. That IAM Role could provide read access to a Redshift database/table, providing fine-grained access to Redshift from selected users in selected projects in Hopsworks. ADLS HopsFS JDBC Redshift S3 Snowflake Programmatic Connectors (Spark, Python, Java/Scala, Flink) # It is also possible to use the rich ecosystem of connectors available in programs run on Hopsworks. Just Spark has tens of open-source libraries for connecting to relational databases, key-value stores, file systems, object stores, search databases, and graph databases. In Hopsworks, you can securely save your credentials as secrets, and securely access them with API calls when you need to connect to your external store. Next Steps # For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Overview"},{"location":"integrations/storage-connectors/#storage-connectors","text":"You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store information in Hopsworks about how to securely connect to external data stores. They can be used in both programs and in Hopsworks to easily and securely connect and ingest data to the Feature Store. External (on-demand) Feature Groups can also be defined with storage connectors, where only the metadata is stored in Hopsworks. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or more advanced multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. Mapping IAM Roles to Projects/Roles in Hopsworks In Hopsworks, you can specify a Cloud Role (IAM role or managed identity) and (1) in which Project and (2) what role within that Project can assume that Cloud Role. For example, you could limit access to a given IAM Role to users who have the 'Data Owner' role in a Project called 'RawFeatures'. That IAM Role could provide read access to a Redshift database/table, providing fine-grained access to Redshift from selected users in selected projects in Hopsworks. ADLS HopsFS JDBC Redshift S3 Snowflake","title":"Storage Connectors"},{"location":"integrations/storage-connectors/#programmatic-connectors-spark-python-javascala-flink","text":"It is also possible to use the rich ecosystem of connectors available in programs run on Hopsworks. Just Spark has tens of open-source libraries for connecting to relational databases, key-value stores, file systems, object stores, search databases, and graph databases. In Hopsworks, you can securely save your credentials as secrets, and securely access them with API calls when you need to connect to your external store.","title":"Programmatic Connectors (Spark, Python, Java/Scala, Flink)"},{"location":"integrations/storage-connectors/#next-steps","text":"For more information about how to use the Feature Store, see the Quickstart Guide .","title":"Next Steps"},{"location":"integrations/databricks/api_key/","text":"Hopsworks API key # In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Quickstart API key Argument # API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Store the API key # AWS # Option 1: Using the AWS Systems Manager Parameter Store # Store the API key in the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks cluster that should access the Feature Store. Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store Option 2: Using the AWS Secrets Manager # Store the API key in the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager Azure # On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store. Next Steps # Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Hopsworks API Key"},{"location":"integrations/databricks/api_key/#hopsworks-api-key","text":"In order for the Databricks cluster to be able to communicate with the Hopsworks Feature Store, the clients running on Databricks need to be able to access a Hopsworks API key.","title":"Hopsworks API key"},{"location":"integrations/databricks/api_key/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the job, featurestore and project scopes before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: featurestore project job API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/databricks/api_key/#quickstart-api-key-argument","text":"API key as Argument To get started quickly, without saving the Hopsworks API in a secret storage, you can simply supply it as an argument when instantiating a connection: import hsfs conn = hsfs . connection ( host = 'my_instance' , # DNS of your Feature Store instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Feature Store project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Quickstart API key Argument"},{"location":"integrations/databricks/api_key/#store-the-api-key","text":"","title":"Store the API key"},{"location":"integrations/databricks/api_key/#aws","text":"","title":"AWS"},{"location":"integrations/databricks/api_key/#option-1-using-the-aws-systems-manager-parameter-store","text":"Store the API key in the AWS Systems Manager Parameter Store In the AWS Management Console, ensure that your active region is the region you use for Databricks. Go to the AWS Systems Manager choose Parameter Store and select Create Parameter . As name enter /hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks cluster that should access the Feature Store. Select Secure String as type and create the parameter. Storing the Feature Store API key in the Parameter Store Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Systems Manager as service, expand the Read access level and check GetParameter . Expand Resources and select Add ARN . Enter the region of the Systems Manager as well as the name of the parameter WITHOUT the leading slash e.g. hopsworks/role/[MY_DATABRICKS_ROLE]/type/api-key and click Add . Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Parameter Store","title":"Option 1: Using the AWS Systems Manager Parameter Store"},{"location":"integrations/databricks/api_key/#option-2-using-the-aws-secrets-manager","text":"Store the API key in the AWS Secrets Manager In the AWS management console ensure that your active region is the region you use for Databricks. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Storing a Feature Store API key in the Secrets Manager Step 1 As secret name, enter hopsworks/role/[MY_DATABRICKS_ROLE] replacing [MY_DATABRICKS_ROLE] with the AWS role used by the Databricks instance that should access the Feature Store. Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Storing a Feature Store API key in the Secrets Manager Step 2 Grant access to the secret to the Databricks notebook role In the AWS Management Console, go to IAM , select Roles and then the role that is used when creating Databricks clusters. Select Add inline policy . Choose Secrets Manager as service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configuring the access policy for the Secrets Manager","title":"Option 2: Using the AWS Secrets Manager"},{"location":"integrations/databricks/api_key/#azure","text":"On Azure we currently do not support storing the API key in a secret storage. Instead just store the API key in a file in your Databricks workspace so you can access it when connecting to the Feature Store.","title":"Azure"},{"location":"integrations/databricks/api_key/#next-steps","text":"Continue with the configuration guide to finalize the configuration of the Databricks Cluster to communicate with the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/configuration/","text":"Databricks Integration # Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step. Prerequisites # In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks. Networking # If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the Hopsworks.ai VPC/VNet. Hopsworks API key # In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks . Databricks API key # Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure. Register a new Databricks Instance # Users can register a new Databricks instance by navigating to the Integrations tab of a project Feature Store. Registering a Databricks instance requires adding the instance address and the API key. The instance address should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of. Databricks Cluster # A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 6 is suggested to be able to use the full suite of Hopsworks Feature Store capabilities. Configure a cluster # Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above. Connecting to the Feature Store # At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = \"featurestore.key\" , # For Azure, store the API key locally secrets_store = \"local\" , hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Next Steps # For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Configuration"},{"location":"integrations/databricks/configuration/#databricks-integration","text":"Users can configure their Databricks clusters to write the results of feature engineering pipelines in the Hopsworks Feature Store using HSFS. Configuring a Databricks cluster can be done from the Hopsworks Feature Store UI. This guide explains each step.","title":"Databricks Integration"},{"location":"integrations/databricks/configuration/#prerequisites","text":"In order to be able to configure a Databricks cluster to use the Feature Store of your Hopsworks instance, it is necessary to ensure networking is setup correctly between the instances and that the Databricks cluster has access to the Hopsworks API key to perform requests with HSFS from Databricks to Hopsworks.","title":"Prerequisites"},{"location":"integrations/databricks/configuration/#networking","text":"If you haven't done so already, follow the networking guides for either AWS or Azure for instructions on how to configure networking properly between Databricks' VPC (or Virtual Network on Azure) and the Hopsworks.ai VPC/VNet.","title":"Networking"},{"location":"integrations/databricks/configuration/#hopsworks-api-key","text":"In order for the Feature Store API to be able to communicate with the user's Hopsworks instance, the client library (HSFS) needs to have access to a previously generated API key from Hopsworks. For ways to setup and store the Hopsworks API key, please refer to the API key guide for Databricks .","title":"Hopsworks API key"},{"location":"integrations/databricks/configuration/#databricks-api-key","text":"Hopsworks uses the Databricks REST APIs to communicate with the Databricks instance and configure clusters on behalf of users. To achieve that, the first step is to register an instance and a valid API key in Hopsworks. Users can get a valid Databricks API key by following the Databricks Documentation Cluster access control If users have enabled Databricks Cluster access control , it is important that the users running the cluster configuration (i.e. the user generating the API key) has Can Manage privileges on the cluster they are trying to configure.","title":"Databricks API key"},{"location":"integrations/databricks/configuration/#register-a-new-databricks-instance","text":"Users can register a new Databricks instance by navigating to the Integrations tab of a project Feature Store. Registering a Databricks instance requires adding the instance address and the API key. The instance address should be in the format [UUID].cloud.databricks.com (or adb-[UUID].19.azuredatabricks.net for Databricks on Azure), essentially the same web address used to reach the Databricks instance from the browser. Register a Databricks Instance along with a Databricks API key The API key will be stored in the Hopsworks secret store for the user and will be available only for that user. If multiple users need to configure Databricks clusters, each has to generate an API key and register an instance. The Databricks instance registration does not have a project scope, meaning that once registered, the user can configure clusters for all projects they are part of.","title":"Register a new Databricks Instance"},{"location":"integrations/databricks/configuration/#databricks-cluster","text":"A cluster needs to exist before users can configure it using the Hopsworks UI. The cluster can be in any state prior to the configuration. Runtime limitation Currently Runtime 6 is suggested to be able to use the full suite of Hopsworks Feature Store capabilities.","title":"Databricks Cluster"},{"location":"integrations/databricks/configuration/#configure-a-cluster","text":"Clusters are configured for a project user, which, in Hopsworks terms, means a user operating within the scope of a project. To configure a cluster, click on the Configure button. By default the cluster will be configured for the user making the request. If the user doesn't have Can Manage privilege on the cluster, they can ask a project Data Owner to configure it for them. Hopsworks Data Owners are allowed to configure clusters for other project users, as long as they have the required Databricks privileges. Configure a Databricks Cluster from Hopsworks During the cluster configuration the following steps will be taken: Upload an archive to DBFS containing the necessary Jars for HSFS and HopsFS to be able to read and write from the Hopsworks Feature Store Add an initScript to configure the Jars when the cluster is started Install hsfs python library Configure the necessary Spark properties to authenticate and communicate with the Feature Store When a cluster is configured for a specific project user, all the operations with the Hopsworks Feature Store will be executed as that project user. If another user needs to re-use the same cluster, the cluster can be reconfigured by following the same steps above.","title":"Configure a cluster"},{"location":"integrations/databricks/configuration/#connecting-to-the-feature-store","text":"At the end of the configuration, Hopsworks will start the cluster. Once the cluster is running users can establish a connection to the Hopsworks Feature Store from Databricks: API key on Azure Please note, for Azure it is necessary to store the Hopsworks API key locally on the cluster as a file. As we currently do not support storing the API key on an Azure Secret Management Service as we do for AWS. Consult the API key guide for Azure , for more information. AWS import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project secrets_store = 'secretsmanager' , # Either parameterstore or secretsmanager hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store Azure import hsfs conn = hsfs . connection ( 'my_instance' , # DNS of your Feature Store instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks Feature Store project api_key_file = \"featurestore.key\" , # For Azure, store the API key locally secrets_store = \"local\" , hostname_verification = True # Disable for self-signed certificates ) fs = conn . get_feature_store () # Get the project's default feature store","title":"Connecting to the Feature Store"},{"location":"integrations/databricks/configuration/#next-steps","text":"For more information about how to connect, see the Connection guide. Or continue with the Data Source guide to import your own data to the Feature Store.","title":"Next Steps"},{"location":"integrations/databricks/networking/","text":"Networking # In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster. AWS # Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are working on Hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details Azure # Step 1: Set up VNet peering between Hopsworks and Databricks # VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on Hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings Step 2: Configure the Network Security Group # The Network Security Group of the Feature Store on Azure needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Ensure that ports 443 , 9083 , 9085 , 8020 and 50010 are reachable from the Databricks cluster Network Security Group . Hopsworks.ai If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services . Next Steps # Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/databricks/networking/#networking","text":"In order for Spark to communicate with the Feature Store from Databricks, networking needs to be set up correctly. This includes deploying the Hopsworks Instance to either the same VPC or enable VPC/VNet peering between the VPC/VNet of the Databricks Cluster and the Hopsworks Cluster.","title":"Networking"},{"location":"integrations/databricks/networking/#aws","text":"","title":"AWS"},{"location":"integrations/databricks/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your Databricks cluster or to set up VPC Peering between your Databricks VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the Databricks VPC When you deploy the Feature Store Hopsworks instance, select the Databricks VPC and Availability Zone as the VPC and Availability Zone of your Feature Store cluster. Identify your Databricks VPC by searching for VPCs containing Databricks in their name in your Databricks AWS region in the AWS Management Console: Identify the Databricks VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are working on Hopsworks.ai , you can directly deploy the Hopsworks instance to the Databricks VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store cluster and Databricks. Get your Feature Store VPC ID and CIDR by searching for thr Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"integrations/databricks/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 are reachable from the Databricks Security Group: Hopsworks Feature Store Security Group Connectivity from the Databricks Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and searching for dbe-worker in the source field. Selecting any of the dbe-worker Security Groups will be sufficient: Hopsworks Feature Store Security Group details","title":"Step 2: Configure the Security Group"},{"location":"integrations/databricks/networking/#azure","text":"","title":"Azure"},{"location":"integrations/databricks/networking/#step-1-set-up-vnet-peering-between-hopsworks-and-databricks","text":"VNet peering between the Hopsworks and the Databricks virtual network is required to be able to connect to the Feature Store from Databricks. In the Azure portal, go to Azure Databricks and go to Virtual Network Peering : Azure Databricks Select Add Peering : Add peering Name the peering and select the virtual network used by your Hopsworks cluster. The virtual network is shown in the cluster details on Hopsworks.ai (see the next picture). Ensure to press the copy button on the bottom of the page and save the value somewhere. Press Add and create the peering: Configure peering The virtual network used by your cluster is shown under Details : Check the Hopsworks virtual network The peering connection should now be listed as initiated: Peering connection initiated On the Azure portal, go to Virtual networks and search for the virtual network used by your Hopsworks cluster: Virtual networks Open the network and select Peerings : Select peerings Choose to add a peering connection: Add a peering connection Name the peering connection and select I know my resource ID . Paste the string copied when creating the peering from Databricks Azure. If you haven't copied that string, then manually select the virtual network used by Databricks and press OK to create the peering: Configure peering The peering should now be Updating : Cloud account settings Wait for the peering to show up as Connected . There should now be bi-directional network connectivity between the Feature Store and Databricks: Cloud account settings","title":"Step 1: Set up VNet peering between Hopsworks and Databricks"},{"location":"integrations/databricks/networking/#step-2-configure-the-network-security-group","text":"The Network Security Group of the Feature Store on Azure needs to be configured to allow traffic from your Databricks clusters to be able to connect to the Feature Store. Ensure that ports 443 , 9083 , 9085 , 8020 and 50010 are reachable from the Databricks cluster Network Security Group . Hopsworks.ai If you deployed your Hopsworks Feature Store instance with Hopsworks.ai, it suffices to enable outside access of the Feature Store and Online Feature Store services .","title":"Step 2: Configure the Network Security Group"},{"location":"integrations/databricks/networking/#next-steps","text":"Continue with the Hopsworks API key guide to setup access to a Hopsworks API key from the Databricks Cluster, in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/emr/emr_configuration/","text":"Configure EMR for the Hopsworks Feature Store # To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide. Step 1: Set up a Hopsworks API key # In order for EMR clusters to be able to communicate with the Hopsworks Feature Store, the clients running on EMR need to be able to access a Hopsworks API key. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: project API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Store the API key in the AWS Secrets Manager # In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret Grant access to the secret to the EMR EC2 instance profile # Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager Step 2: Configure your EMR cluster # Add the Hopsworks Feature Store configuration to your EMR cluster # In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"core-site\" , \"Properties\" : { \"fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"hops.ipc.server.ssl.enabled\" : true , \"hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" } }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.sql.hive.metastore.jars\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" } }, { \"Classification\" : \"spark-hive-site\" , \"Properties\" : { \"hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } } ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store Add the Bootstrap Action to your EMR cluster # EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store. Next Steps # If you use Python, then install the HSFS library . The Scala version of the library has already been installed to your EMR cluster. Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"integrations/emr/emr_configuration/#configure-emr-for-the-hopsworks-feature-store","text":"To enable EMR to access the Hopsworks Feature Store, you need to set up a Hopsworks API key, add a bootstrap action and configurations to your EMR cluster. Info Ensure Networking is set up correctly before proceeding with this guide.","title":"Configure EMR for the Hopsworks Feature Store"},{"location":"integrations/emr/emr_configuration/#step-1-set-up-a-hopsworks-api-key","text":"In order for EMR clusters to be able to communicate with the Hopsworks Feature Store, the clients running on EMR need to be able to access a Hopsworks API key.","title":"Step 1: Set up a Hopsworks API key"},{"location":"integrations/emr/emr_configuration/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project scope before creating the key. Copy the key into your clipboard for the next step. Scopes The API key should contain at least the following scopes: project API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/emr/emr_configuration/#store-the-api-key-in-the-aws-secrets-manager","text":"In the AWS management console ensure that your active region is the region you use for EMR. Go to the AWS Secrets Manager and select Store new secret . Select Other type of secrets and add api-key as the key and paste the API key created in the previous step as the value. Click next. Store a Hopsworks API key in the Secrets Manager As a secret name, enter hopsworks/featurestore . Select next twice and finally store the secret. Then click on the secret in the secrets list and take note of the Secret ARN . Name the secret","title":"Store the API key in the AWS Secrets Manager"},{"location":"integrations/emr/emr_configuration/#grant-access-to-the-secret-to-the-emr-ec2-instance-profile","text":"Identify your EMR EC2 instance profile in the EMR cluster summary: Identify your EMR EC2 instance profile In the AWS Management Console, go to IAM , select Roles and then the EC2 instance profile used by your EMR cluster. Select Add inline policy . Choose Secrets Manager as a service, expand the Read access level and check GetSecretValue . Expand Resources and select Add ARN . Paste the ARN of the secret created in the previous step. Click on Review , give the policy a name und click on Create policy . Configure the access policy for the Secrets Manager","title":"Grant access to the secret to the EMR EC2 instance profile"},{"location":"integrations/emr/emr_configuration/#step-2-configure-your-emr-cluster","text":"","title":"Step 2: Configure your EMR cluster"},{"location":"integrations/emr/emr_configuration/#add-the-hopsworks-feature-store-configuration-to-your-emr-cluster","text":"In order for EMR to be able to talk to the Feature Store, you need to update the Hadoop and Spark configurations. Copy the configuration below and replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. [ { \"Classification\" : \"hadoop-env\" , \"Properties\" : { }, \"Configurations\" : [ { \"Classification\" : \"export\" , \"Properties\" : { \"HADOOP_CLASSPATH\" : \"$HADOOP_CLASSPATH:/usr/lib/hopsworks/client/*\" }, \"Configurations\" : [ ] } ] }, { \"Classification\" : \"core-site\" , \"Properties\" : { \"fs.hopsfs.impl\" : \"io.hops.hopsfs.client.HopsFileSystem\" , \"hops.ipc.server.ssl.enabled\" : true , \"hops.ssl.hostname.verifier\" : \"ALLOW_ALL\" , \"hops.rpc.socket.factory.class.default\" : \"io.hops.hadoop.shaded.org.apache.hadoop.net.HopsSSLSocketFactory\" , \"client.rpc.ssl.enabled.protocol\" : \"TLSv1.2\" , \"hops.ssl.keystores.passwd.name\" : \"/usr/lib/hopsworks/material_passwd\" , \"hops.ssl.keystore.name\" : \"/usr/lib/hopsworks/keyStore.jks\" , \"hops.ssl.trustore.name\" : \"/usr/lib/hopsworks/trustStore.jks\" } }, { \"Classification\" : \"spark-defaults\" , \"Properties\" : { \"spark.sql.hive.metastore.jars\" : \"/usr/lib/hopsworks/apache-hive-bin/lib/*\" , \"spark.executor.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" , \"spark.driver.extraClassPath\" : \"/usr/lib/hopsworks/client/*\" } }, { \"Classification\" : \"spark-hive-site\" , \"Properties\" : { \"hive.metastore.uris\" : \"thrift://ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal:9083\" } } ] When you create your EMR cluster, add the configuration: Note Don't forget to replace ip-XXX-XX-XX-XXX.XX-XXXX-X.compute.internal with the private DNS name of your Hopsworks master node. Configure EMR to access the Feature Store","title":"Add the Hopsworks Feature Store configuration to your EMR cluster"},{"location":"integrations/emr/emr_configuration/#add-the-bootstrap-action-to-your-emr-cluster","text":"EMR requires Hopsworks connectors to be able to communicate with the Hopsworks Feature Store. These connectors can be installed with the bootstrap action shown below. Copy the content into a file and name the file hopsworks.sh . Copy that file into any S3 bucket that is readable by your EMR clusters and take note of the S3 URI of that file e.g., s3://my-emr-init/hopsworks.sh . #!/bin/bash set -e if [ \" $# \" -ne 3 ] ; then echo \"Usage hopsworks.sh HOPSWORKS_API_KEY_SECRET, HOPSWORKS_HOST, PROJECT_NAME\" exit 1 fi SECRET_NAME = $1 HOST = $2 PROJECT = $3 API_KEY = $( aws secretsmanager get-secret-value --secret-id $SECRET_NAME | jq -r .SecretString | jq -r '.[\"api-key\"]' ) PROJECT_ID = $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/getProjectInfo/ $PROJECT | jq -r .projectId ) sudo yum -y install python3-devel.x86_64 || true sudo mkdir /usr/lib/hopsworks sudo chown hadoop:hadoop /usr/lib/hopsworks cd /usr/lib/hopsworks curl -o client.tar.gz -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /client tar -xvf client.tar.gz tar -xzf client/apache-hive-*-bin.tar.gz mv apache-hive-*-bin apache-hive-bin rm client.tar.gz rm client/apache-hive-*-bin.tar.gz curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .kStore | base64 -d > keyStore.jks curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .tStore | base64 -d > trustStore.jks echo -n $( curl -H \"Authorization: ApiKey ${ API_KEY } \" https:// $HOST /hopsworks-api/api/project/ $PROJECT_ID /credentials | jq -r .password ) > material_passwd chmod -R o-rwx /usr/lib/hopsworks Add the bootstrap actions when configuring your EMR cluster. Provide 3 arguments to the bootstrap action: The name of the API key secret e.g., hopsworks/featurestore , the public DNS name of your Hopsworks cluster, such as ad005770-33b5-11eb-b5a7-bfabd757769f.cloud.hopsworks.ai , and the name of your Hopsworks project, e.g. demo_fs_meb10179 . Set the bootstrap action for EMR Your EMR cluster will now be able to access your Hopsworks Feature Store.","title":"Add the Bootstrap Action to your EMR cluster"},{"location":"integrations/emr/emr_configuration/#next-steps","text":"If you use Python, then install the HSFS library . The Scala version of the library has already been installed to your EMR cluster. Use the Connection API to connect to the Hopsworks Feature Store. For more information about how to use the Feature Store, see the Quickstart Guide . Matching Hopsworks version The major version of HSFS needs to match the major version of Hopsworks . You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Next Steps"},{"location":"integrations/emr/networking/","text":"Networking # In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store. Step 1: Ensure network connectivity # The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are on Hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC Step 2: Configure the Security Group # The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups Next Steps # Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/emr/networking/#networking","text":"In order for Spark to communicate with the Hopsworks Feature Store from EMR, networking needs to be set up correctly. This includes deploying the Hopsworks Feature Store to either the same VPC or enable VPC peering between the VPC of the EMR cluster and the Hopsworks Feature Store.","title":"Networking"},{"location":"integrations/emr/networking/#step-1-ensure-network-connectivity","text":"The DataFrame API needs to be able to connect directly to the IP on which the Feature Store is listening. This means that if you deploy the Feature Store on AWS you will either need to deploy the Feature Store in the same VPC as your EMR cluster or to set up VPC Peering between your EMR VPC and the Feature Store VPC. Option 1: Deploy the Feature Store in the EMR VPC When deploying the Hopsworks Feature Store, select the EMR VPC and Availability Zone as the VPC and Availability Zone of your Feature Store. Identify your EMR VPC in the Summary of your EMR cluster: Identify the EMR VPC Identify the EMR VPC Hopsworks installer If you are performing an installation using the Hopsworks installer script , ensure that the virtual machines you install Hopsworks on are deployed in the EMR VPC. Hopsworks.ai If you are on Hopsworks.ai , you can directly deploy Hopsworks to the EMR VPC, by simply selecting it at the VPC selection step during cluster creation . Option 2: Set up VPC peering Follow the guide VPC Peering to set up VPC peering between the Feature Store and EMR. Get your Feature Store VPC ID and CIDR by searching for the Feature Store VPC in the AWS Management Console: Hopsworks.ai On Hopsworks.ai , the VPC is shown in the cluster details. Identify the Feature Store VPC","title":"Step 1: Ensure network connectivity"},{"location":"integrations/emr/networking/#step-2-configure-the-security-group","text":"The Feature Store Security Group needs to be configured to allow traffic from your EMR clusters to be able to connect to the Feature Store. Hopsworks.ai If you deployed your Hopsworks Feature Store with Hopsworks.ai, you only need to enable outside access of the Feature Store services . Open your feature store instance under EC2 in the AWS Management Console and ensure that ports 443 , 3306 , 9083 , 9085 , 8020 and 30010 (443,3306,8020,30010,9083,9085) are reachable from the EMR Security Group: Hopsworks Feature Store Security Group Connectivity from the EMR Security Group can be allowed by opening the Security Group, adding a port to the Inbound rules and setting the EMR master and core security group as source: Hopsworks Feature Store Security Group details You can find your EMR security groups in the EMR cluster summary: EMR Security Groups","title":"Step 2: Configure the Security Group"},{"location":"integrations/emr/networking/#next-steps","text":"Continue with the Configure EMR for the Hopsworks Feature Store , in order to be able to use the Hopsworks Feature Store.","title":"Next Steps"},{"location":"integrations/storage-connectors/adls/","text":"Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting persmissions to a service principal. Requirements # Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you. Configure an ADLS storage connector in the Hopsworks UI. Azure Create a ADLS Resource # When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. Common Problems: If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" buton to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container. References: How to create a service principal on Azure","title":"ADLS"},{"location":"integrations/storage-connectors/adls/#requirements","text":"Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you. Configure an ADLS storage connector in the Hopsworks UI.","title":"Requirements"},{"location":"integrations/storage-connectors/adls/#azure-create-a-adls-resource","text":"When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. Common Problems: If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" buton to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container. References: How to create a service principal on Azure","title":"Azure Create a ADLS Resource"},{"location":"integrations/storage-connectors/hopsfs/","text":"HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset. You can define a storage connector to a directory in the same project in HopsFS by selecting the directory.","title":"HopsFS"},{"location":"integrations/storage-connectors/jdbc/","text":"Most databases can be connected to using our generic JDBC Storage Connector (such as MySQL, Postgres, Oracle, DB2, and MongoDB). Typically, you will need to add username and password in your JDBC URL or as key/value parameters. Consult the documentation of your target database to determine the correct JDBC URL and parameters. You can define a storage connector to a JDBC enabled source using a JDBC connection string and optional parameters supplied at runtime.","title":"JDBC"},{"location":"integrations/storage-connectors/redshift/","text":"Amazon Redshift is a popular managed data warehouse on AWS. Configure the Redshift storage connector in the Hopsworks UI. In the UI for the Redshift connector, you should enter the following: Cluster identifier: The name of the cluster Database driver: You can use the default JDBC Redshift Driver com.amazon.redshift.jdbc42.Driver (More on this later) Database endpoint: The endpoint for the database. Should be in the format of [UUID].eu-west-1.redshift.amazonaws.com Database name: The name of the database to query Database port: The port of the cluster. Defaults to 5349 There are two options available for authenticating with the Redshift cluster. The first option is to configure a username and a password. The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user. With regards to the database driver, the library to interact with Redshift is not included in Hopsworks - you need to upload the driver yourself. First, you need to download the library . Select the driver version without the AWS SDK. You then upload the driver files to the \u201cResources\u201d dataset in your project, see the screenshot below. Upload the Redshift driver to Hopsworks. Then, you add the file to your notebook or job before launching it, as shown in the screenshots below. When you start a Jupyter notebook, you need to add the driver so it can be accessed in programs.","title":"Redshift"},{"location":"integrations/storage-connectors/s3/","text":"You can define an Amazon S3 storage connector using the bucket name and the IAM role used to access the bucket. The bucket may be encrypted. If you have enabled an IAM role for the cluster or Multiple (federated) IAM roles, you can select the IAM role that should be used to access the S3 bucket. Configure the S3 storage connector in the Hopsworks UI.","title":"S3"},{"location":"integrations/storage-connectors/snowflake/","text":"Snowflake is a popular managed data warehouse on AWS, Azure, and GCP. In the UI for the Snowflake connector, you should enter the following: The following options are required to create a snowflake connector: url: the hostname for your account in the following format: .snowflakecomputing.com. user: login name for the Snowflake user. password: password of the Snowflake user. (required if token is not set) token: OAuth token that can be used to access snowflake. (required if password is not set) database: the database to use for the session after connecting. schema: the schema to use for the session after connecting. The remaining options are not required, but are optional: warehouse: the default virtual warehouse to use for the session after connecting. role: the default security role to use for the session after connecting. table: the table to which data is written to or read from. Additional snowflake options can be added as a list of key-value pair in sfOptions There are two options available for authentication. The first option is to configure a username and a password. The second option is to use an OAuth token. See Configure Snowflake OAuth for instruction on how to configure OAuth support for snowflake, and Using External OAuth on how you can use External OAuth to authenticate to Snowflake. With regards to the database driver, the library to interact with Snowflake is not included in Hopsworks - you need to upload the driver yourself. First, you need to download the jdbc driver and to use snowflake as the data source in spark the snowflake spark connector . Upload the JDBC driver and Snowflake Spark connector to Hopsworks. Then, you add the file to your notebook or job before launching it, as shown in the screenshots below. When you start a Jupyter notebook for Snowflake, you need to add both the JDBC driver and the Snowflake Spark Connector.","title":"Snowflake"}]}